{
  "owner": "string",
  "processed_files": {
    "PDF": [
      {
        "file": "Programming_Persistent_Memory_medium_457.pdf",
        "output": {
          "metadata": {
            "file_name": "Programming_Persistent_Memory_medium_457.pdf",
            "file_type": "pdf",
            "file_size": "4.79 MB",
            "page_count": 457
          },
          "pages": [
            {
              "page_number": 1,
              "text": "Programming \nPersistent \nMemory\nA Comprehensive Guide for Developers\n—\nSteve Scargall",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page1_img1.jpeg",
                "output\\images\\Programming_Persistent_Memory_medium_457_page1_img2.jpeg"
              ],
              "img_summary_files": [],
              "img_vision_files": [
                "output\\images\\img_vision\\Programming_Persistent_Memory_medium_457_page1_img1_vision.json",
                "output\\images\\img_vision\\Programming_Persistent_Memory_medium_457_page1_img2_vision.json"
              ],
              "summary": "Summary not available due to an error.",
              "time_taken": "13.49 sec"
            },
            {
              "page_number": 2,
              "text": "Programming Persistent \nMemory\nA Comprehensive Guide for \nDevelopers\nSteve Scargall",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "14.04 sec"
            },
            {
              "page_number": 3,
              "text": "Programming Persistent Memory: A Comprehensive Guide for Developers\nISBN-13 (pbk): 978-1-4842-4931-4\t\n\t\n\t\nISBN-13 (electronic): 978-1-4842-4932-1\nhttps://doi.org/10.1007/978-1-4842-4932-1\nCopyright © 2020 by Intel\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is \nconcerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction \non microfilms or in any other physical way, and transmission or information storage and retrieval, electronic \nadaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.\nOpen Access  This book is licensed under the terms of the Creative Commons Attribution 4.0 \nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, \nsharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this book are included in the book’s Creative Commons license, unless \nindicated otherwise in a credit line to the material. If material is not included in the book’s Creative Commons \nlicense and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every \noccurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion \nand to the benefit of the trademark owner, with no intention of infringement of the trademark. \nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified \nas such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\nWhile the advice and information in this book are believed to be true and accurate at the date of publication, neither \nthe authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may \nbe made. The publisher makes no warranty, express or implied, with respect to the material contained herein.\nManaging Director, Apress Media LLC: Welmoed Spahr\nAcquisitions Editor: Susan McDermott\nDevelopment Editor: Laura Berendson\nCoordinating Editor: Jessica Vakili\nDistributed to the book trade worldwide by Springer Science+Business Media New York, 233 Spring Street,  \n6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail orders-ny@springer-sbm.com, \nor visit www.springeronline.com. Apress Media, LLC is a California LLC and the sole member (owner) is Springer \nScience + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware corporation.\nFor information on translations, please e-mail rights@apress.com, or visit http://www.apress.com/\nrights-permissions.\nApress titles may be purchased in bulk for academic, corporate, or promotional use. eBook versions and licenses \nare also available for most titles. For more information, reference our Print and eBook Bulk Sales web page at \nhttp://www.apress.com/bulk-sales.\nAny source code or other supplementary material referenced by the author in this book is available to readers on \nGitHub via the book's product page, located at www.apress.com/978-1-4842-4931-4. For more detailed \ninformation, please visit http://www.apress.com/source-code.\nPrinted on acid-free paper\nSteve Scargall\nSanta Clara, CA, USA",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "14.66 sec"
            },
            {
              "page_number": 4,
              "text": "iii\nChapter 1: Introduction to Persistent Memory Programming\b.................................. 1\nA High-Level Example Program\b.................................................................................................... 2\nWhat’s Different?\b..................................................................................................................... 5\nThe Performance Difference\b................................................................................................... 6\nProgram Complexity\b................................................................................................................ 7\nHow Does libpmemkv Work?\b................................................................................................... 8\nWhat’s Next?\b................................................................................................................................. 9\nSummary\b...................................................................................................................................... 9\nChapter 2: Persistent Memory Architecture\b........................................................... 11\nPersistent Memory Characteristics\b............................................................................................. 12\nPlatform Support for Persistent Memory\b.................................................................................... 13\nCache Hierarchy\b.......................................................................................................................... 14\nPower-Fail Protected Domains\b.................................................................................................... 16\nThe Need for Flushing, Ordering, and Fencing\b............................................................................ 19\nData Visibility\b.............................................................................................................................. 23\nIntel Machine Instructions for Persistent Memory\b...................................................................... 24\nDetecting Platform Capabilities\b.................................................................................................. 25\nTable of Contents\nAbout the Author\b................................................................................................... xiii\nAbout the Technical Reviewer\b.................................................................................xv\nAbout the Contributors\b..........................................................................................xvii\nAcknowledgments\b..................................................................................................xxi\nPreface\b.................................................................................................................xxiii",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "15.17 sec"
            },
            {
              "page_number": 5,
              "text": "iv\nApplication Startup and Recovery\b............................................................................................... 27\nWhat’s Next?\b............................................................................................................................... 29\nSummary\b.................................................................................................................................... 29\nChapter 3: Operating System Support for Persistent Memory\b............................... 31\nOperating System Support for Memory and Storage\b.................................................................. 31\nPersistent Memory As Block Storage\b.......................................................................................... 33\nPersistent Memory-Aware File Systems\b..................................................................................... 34\nMemory-Mapped Files\b................................................................................................................ 35\nPersistent Memory Direct Access (DAX)\b..................................................................................... 43\nSummary\b.................................................................................................................................... 53\nChapter 4: Fundamental Concepts of Persistent Memory Programming\b............... 55\nWhat’s Different?\b........................................................................................................................ 55\nAtomic Updates\b........................................................................................................................... 56\nTransactions\b................................................................................................................................ 57\nAtomicity\b............................................................................................................................... 57\nConsistency\b........................................................................................................................... 58\nIsolation\b................................................................................................................................. 58\nDurability\b............................................................................................................................... 58\nFlushing Is Not Transactional\b...................................................................................................... 59\nStart-Time Responsibilities\b......................................................................................................... 59\nTuning for Hardware Configurations\b........................................................................................... 60\nSummary\b.................................................................................................................................... 60\nChapter 5: Introducing the Persistent Memory Development Kit\b........................... 63\nBackground\b................................................................................................................................. 63\nChoosing the Right Semantics\b.................................................................................................... 64\nVolatile Libraries\b......................................................................................................................... 65\nlibmemkind\b............................................................................................................................ 65\nlibvmemcache\b....................................................................................................................... 66\nlibvmem\b................................................................................................................................. 67\nTable of Contents",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "15.68 sec"
            },
            {
              "page_number": 6,
              "text": "v\nPersistent Libraries\b..................................................................................................................... 67\nlibpmem\b................................................................................................................................ 67\nlibpmemobj\b............................................................................................................................ 68\nlibpmemobj-cpp\b.................................................................................................................... 68\nlibpmemkv\b............................................................................................................................. 69\nlibpmemlog\b............................................................................................................................ 69\nlibpmemblk\b............................................................................................................................ 69\nTools and Command Utilities\b....................................................................................................... 70\npmempool\b.............................................................................................................................. 70\npmemcheck\b........................................................................................................................... 70\npmreorder\b.............................................................................................................................. 71\nSummary\b.................................................................................................................................... 71\nChapter 6: libpmem: Low-Level Persistent Memory Support\b................................. 73\nUsing the Library\b......................................................................................................................... 74\nMapping a File\b............................................................................................................................ 75\nCopying to Persistent Memory\b.................................................................................................... 76\nSeparating the Flush Steps\b......................................................................................................... 77\nSummary\b.................................................................................................................................... 79\nChapter 7: libpmemobj: A Native Transactional Object Store\b................................. 81\nWhat is libpmemobj?\b.................................................................................................................. 81\nWhy not malloc( )?\b....................................................................................................................... 82\nGrouping Operations\b................................................................................................................... 83\nMemory Pools\b............................................................................................................................. 83\nCreating Memory Pools\b......................................................................................................... 83\nPool Object Pointer (POP) and the Root Object\b...................................................................... 87\nOpening and Reading from Memory Pools\b............................................................................ 88\nMemory Poolsets\b........................................................................................................................ 90\nConcatenated Poolsets\b.......................................................................................................... 90\nReplica Poolsets\b.................................................................................................................... 91\nManaging Memory Pools and Poolsets\b....................................................................................... 92\nTable of Contents",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "16.19 sec"
            },
            {
              "page_number": 7,
              "text": "vi\nTyped Object Identifiers (TOIDs)\b.................................................................................................. 92\nAllocating Memory\b...................................................................................................................... 93\nPersisting Data\b............................................................................................................................ 94\nAtomic Operations\b................................................................................................................. 94\nReserve/Publish API\b.............................................................................................................. 97\nTransactional API\b................................................................................................................. 100\nOptional Flags\b...................................................................................................................... 104\nPersisting Data Summary\b.................................................................................................... 104\nGuarantees of libpmemobj’s APIs\b............................................................................................. 105\nManaging Library Behavior\b....................................................................................................... 106\nDebugging and Error Handling\b.................................................................................................. 106\nSummary\b.................................................................................................................................. 108\nChapter 8: libpmemobj-cpp: The Adaptable Language - C++ and  \nPersistent Memory\b............................................................................................... 111\nIntroduction\b............................................................................................................................... 111\nMetaprogramming to the Rescue\b............................................................................................. 112\nPersistent Pointers\b.............................................................................................................. 112\nTransactions\b........................................................................................................................ 113\nSnapshotting\b....................................................................................................................... 115\nAllocating\b............................................................................................................................. 116\nC++ Standard limitations\b.......................................................................................................... 118\nAn Object’s Lifetime\b............................................................................................................ 119\nTrivial Types\b......................................................................................................................... 120\nObject Layout\b....................................................................................................................... 122\nPointers\b............................................................................................................................... 123\nLimitations Summary\b.......................................................................................................... 125\nPersistence Simplified\b.............................................................................................................. 126\nThe Ecosystem\b.......................................................................................................................... 133\nPersistent Containers\b.......................................................................................................... 134\nExamples of Persistent Containers\b...................................................................................... 135\nSummary\b.................................................................................................................................. 138\nTable of Contents",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "17.01 sec"
            },
            {
              "page_number": 8,
              "text": "vii\nChapter 9: pmemkv: A Persistent In-­Memory Key-Value Store\b............................ 141\npmemkv Architecture\b................................................................................................................ 143\nA Phonebook Example\b.............................................................................................................. 147\nBringing Persistent Memory Closer to the Cloud\b...................................................................... 151\nSummary\b.................................................................................................................................. 152\nChapter 10: Volatile Use of Persistent Memory\b.................................................... 155\nIntroduction\b............................................................................................................................... 155\nBackground\b............................................................................................................................... 156\nMemory Allocation\b............................................................................................................... 156\nHow it Works\b....................................................................................................................... 156\nSupported “Kinds” of Memory\b............................................................................................. 157\nThe memkind API\b...................................................................................................................... 159\nKind Management API\b......................................................................................................... 159\nHeap Management API\b........................................................................................................ 164\nKind Configuration Management\b......................................................................................... 167\nAdditional memkind Code Examples\b................................................................................... 168\nC++ Allocator for PMEM Kind\b................................................................................................... 168\npmem::allocator methods\b.................................................................................................... 169\nNested Containers\b............................................................................................................... 169\nC++ Examples\b.......................................................................................................................... 170\nUsing the pmem::allocator\b.................................................................................................. 170\nCreating a Vector of Strings\b................................................................................................. 171\nExpanding Volatile Memory Using Persistent Memory\b.............................................................. 173\nlibvmemcache: An Efficient Volatile Key-Value Cache for Large-Capacity  \nPersistent Memory\b.................................................................................................................... 177\nlibvmemcache Overview\b..................................................................................................... 178\nlibvmemcache Design\b......................................................................................................... 180\nUsing libvmemcache\b........................................................................................................... 183\nSummary\b.................................................................................................................................. 186\nTable of Contents",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "17.53 sec"
            },
            {
              "page_number": 9,
              "text": "viii\nChapter 11: Designing Data Structures for Persistent Memory\b........................... 187\nContiguous Data Structures and Fragmentation\b....................................................................... 187\nInternal and External Fragmentation\b................................................................................... 188\nAtomicity and Consistency\b.................................................................................................. 189\nSelective Persistence\b.......................................................................................................... 193\nExample Data Structures\b..................................................................................................... 193\nSummary\b.................................................................................................................................. 206\nChapter 12: Debugging Persistent Memory Applications\b..................................... 207\npmemcheck for Valgrind\b........................................................................................................... 208\nStack Overflow Example\b...................................................................................................... 208\nMemory Leak Example\b........................................................................................................ 209\nIntel Inspector – Persistence Inspector\b.................................................................................... 210\nStack Overflow Example\b...................................................................................................... 211\nMemory Leak Example\b........................................................................................................ 212\nCommon Persistent Memory Programming Problems\b.............................................................. 214\nNonpersistent Stores\b........................................................................................................... 214\nStores Not Added into a Transaction\b.................................................................................... 228\nMemory Added to Two Different Transactions\b..................................................................... 233\nMemory Overwrites\b............................................................................................................. 240\nUnnecessary Flushes\b.......................................................................................................... 242\nOut-of-Order Writes\b............................................................................................................. 247\nSummary\b.................................................................................................................................. 259\nChapter 13: Enabling Persistence Using a Real-World Application\b...................... 261\nThe Database Example\b............................................................................................................. 262\nDifferent Persistent Memory Enablement Approaches\b............................................................. 262\nDeveloping a Persistent Memory-Aware MariaDB* Storage Engine\b......................................... 263\nUnderstanding the Storage Layer\b........................................................................................ 264\nCreating a Storage Engine Class\b......................................................................................... 265\nSummary\b.................................................................................................................................. 276\nTable of Contents",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "18.04 sec"
            },
            {
              "page_number": 10,
              "text": "ix\nChapter 14: Concurrency and Persistent Memory\b................................................ 277\nTransactions and Multithreading\b............................................................................................... 278\nMutexes on Persistent Memory\b................................................................................................ 282\nAtomic Operations and Persistent Memory\b.............................................................................. 285\nLock-Free Algorithms and Persistent Memory\b.................................................................... 285\nConcurrent Data Structures for Persistent Memory\b.................................................................. 286\nConcurrent Ordered Map\b..................................................................................................... 287\nConcurrent Hash Map\b.......................................................................................................... 291\nSummary\b.................................................................................................................................. 293\nChapter 15: Profiling and Performance\b................................................................ 295\nIntroduction\b............................................................................................................................... 295\nPerformance Analysis Concepts\b............................................................................................... 295\nCompute-Bound vs. Memory-Bound\b................................................................................... 295\nMemory Latency vs. Memory Capacity\b............................................................................... 296\nRead vs. Write Performance\b................................................................................................ 296\nMemory Access Patterns\b..................................................................................................... 296\nI/O Storage Bound Workloads\b.............................................................................................. 297\nDetermining the Suitability of Workloads for Persistent Memory\b............................................. 297\nVolatile Use Cases\b............................................................................................................... 298\nUse Cases Requiring Persistence\b........................................................................................ 301\nPerformance Analysis of Workloads Using Persistent Memory\b................................................ 302\nCharacterizing the Workload\b............................................................................................... 303\nMemory Bandwidth and Latency\b......................................................................................... 303\nPersistent Memory Read-Write Ratio\b.................................................................................. 305\nWorking Set Size and Memory Footprint\b............................................................................. 305\nNon-Uniform Memory Architecture (NUMA) Behavior\b......................................................... 305\nOptimizing the Software for Persistent Memory\b................................................................. 307\nSummary\b.................................................................................................................................. 311\nTable of Contents",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "19.06 sec"
            },
            {
              "page_number": 11,
              "text": "x\nChapter 16: PMDK Internals: Important Algorithms and Data Structures\b............ 313\nA Pool of Persistent Memory: High-Level Architecture Overview\b............................................. 313\nThe Uncertainty of Memory Mapping: Persistent Memory Object Identifier\b............................. 315\nPersistent Thread Local Storage: Using Lanes\b.......................................................................... 318\nEnsuring Power-Fail Atomicity: Redo and Undo Logging\b.......................................................... 320\nTransaction Redo Logging\b................................................................................................... 320\nTransaction Undo Logging\b................................................................................................... 321\nlibpmemobj Unified Logging\b................................................................................................ 322\nPersistent Allocations: The Interface of a Transactional Persistent Allocator\b............................ 323\nPersistent Memory Heap Management: Allocator Design for Persistent Memory\b.................... 324\nACID Transactions: Efficient Low-Level Persistent Transactions\b............................................... 328\nLazy Reinitialization of Variables: Storing the Volatile State on Persistent Memory\b................. 330\nSummary\b.................................................................................................................................. 331\nChapter 17: Reliability, Availability, and Serviceability (RAS)\b.............................. 333\nDealing with Uncorrectable Errors\b............................................................................................ 333\nConsumed Uncorrectable Error Handling\b............................................................................ 334\nUnconsumed Uncorrectable Error Handling\b........................................................................ 336\nClearing Uncorrectable Errors\b............................................................................................. 339\nDevice Health\b............................................................................................................................ 339\nACPI-Defined Health Functions (_NCH, _NBS)\b..................................................................... 342\nVendor-Specific Device Health (_DSMs)\b.............................................................................. 342\nACPI NFIT Health Event Notification\b.................................................................................... 343\nUnsafe/Dirty Shutdown\b............................................................................................................. 343\nApplication Utilization of Data Loss Count (DLC)\b................................................................. 344\nSummary\b.................................................................................................................................. 346\nChapter 18: Remote Persistent Memory\b............................................................... 347\nRDMA Networking Protocols\b..................................................................................................... 348\nGoals of the Initial Remote Persistent Memory Architecture\b.................................................... 351\nGuaranteeing Remote Persistence\b............................................................................................ 351\nGeneral-Purpose Remote Replication Method\b..................................................................... 353\nTable of Contents",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "20.80 sec"
            },
            {
              "page_number": 12,
              "text": "xi\nAppliance Remote Replication Method\b................................................................................ 355\nGeneral Software Architecture\b.................................................................................................. 357\nlibrpmem Architecture and Its Use in Replication\b..................................................................... 358\nConfiguring Remote Replication Using Poolsets\b................................................................. 362\nPerformance Considerations\b............................................................................................... 362\nRemote Replication Error Handling\b..................................................................................... 364\nSay Hello to the Replicated World\b....................................................................................... 364\nSummary\b.................................................................................................................................. 370\nChapter 19: Advanced Topics\b............................................................................... 373\nNonuniform Memory Access (NUMA)\b........................................................................................ 373\nNUMACTL Linux Utility\b......................................................................................................... 374\nNDCTL Linux Utility\b.............................................................................................................. 376\nIntel Memory Latency Checker Utility\b.................................................................................. 378\nNUMASTAT Utility\b................................................................................................................. 380\nIntel VTune Profiler – Platform Profiler\b................................................................................ 381\nIPMCTL Utility\b...................................................................................................................... 381\nBIOS Tuning Options\b............................................................................................................ 382\nAutomatic NUMA Balancing\b................................................................................................. 382\nUsing Volume Managers with Persistent Memory\b.................................................................... 383\nThe mmap( ) MAP_SYNC Flag\b................................................................................................... 385\nSummary\b.................................................................................................................................. 386\nAppendix A: How to Install NDCTL and DAXCTL on Linux\b..................................... 389\nPrerequisites\b............................................................................................................................. 389\nInstalling NDCTL and DAXCTL Using the Linux Distribution Package Repository\b...................... 390\nSearching for Packages Within a Package Repository\b........................................................ 391\n\u0007Installing NDCTL and DAXCTL from the Package Repository\b.............................................. 392\nTable of Contents",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "21.30 sec"
            },
            {
              "page_number": 13,
              "text": "xii\nAppendix B: How to Install the Persistent Memory Development Kit (PMDK)\b...... 395\nPMDK Prerequisites\b.................................................................................................................. 395\nInstalling PMDK Using the Linux Distribution Package Repository\b........................................... 395\nPackage Naming Convention\b............................................................................................... 396\nSearching for Packages Within a Package Repository\b........................................................ 396\nInstalling PMDK Libraries from the Package Repository\b..................................................... 398\nInstalling PMDK on Microsoft Windows\b.................................................................................... 402\nAppendix C: How to Install IPMCTL on Linux and Windows\b.................................. 403\nIPMCTL Linux Prerequisites\b...................................................................................................... 404\n\u0007libsafec\b................................................................................................................................ 404\nIPMCTL Linux Packages\b............................................................................................................ 404\nIPMCTL for Microsoft Windows\b................................................................................................. 404\nUsing ipmctl\b.............................................................................................................................. 405\nAppendix D: Java for Persistent Memory\b............................................................. 411\nVolatile Use of Persistent Memory\b............................................................................................ 411\n\u0007Heap Allocation on Alternative Memory Devices\b................................................................. 412\nPersistent Collections for Java (PCJ)\b........................................................................................ 416\n\u0007Using PCJ in Java Applications\b................................................................................................. 417\nLow-Level Persistent Library (LLPL)\b......................................................................................... 418\nUsing LLPL in Java Applications\b................................................................................................ 419\n\u0007Summary\b.................................................................................................................................. 419\nAppendix E: The Future of Remote Persistent Memory Replication\b..................... 421\nGlossary\b................................................................................................................ 425\nIndex\b..................................................................................................................... 429\nTable of Contents",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "21.92 sec"
            },
            {
              "page_number": 14,
              "text": "xiii\nAbout the Author\nSteve Scargall is a persistent memory software and cloud architect at Intel \nCorporation. As a technology evangelist, he supports the enabling and development \neffort to integrate persistent memory technology into software stacks, applications, \nand hardware architectures. This includes working with independent software \nvendors (ISVs) on both proprietary and open source development, original equipment \nmanufacturers (OEMs), and cloud service providers (CSPs).\nSteve holds a Bachelor of Science in computer science and cybernetics from the \nUniversity of Reading, UK, where he studied neural networks, AI, and robotics. He \nhas over 19 years’ experience providing performance analysis on x86 architecture and \nSPARC for Solaris Kernel, ZFS, and UFS file system. He performed DTrace debugging in \nenterprise and cloud environments during his tenures at Sun Microsystems and Oracle.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "22.44 sec"
            },
            {
              "page_number": 15,
              "text": "xv\nAbout the Technical Reviewer\nAndy Rudoff is a principal engineer at Intel Corporation, focusing on non-volatile \nmemory programming. He is a contributor to the SNIA NVM Programming Technical \nWork Group. His more than 30 years’ industry experience includes design and \ndevelopment work in operating systems, file systems, networking, and fault management \nat companies large and small, including Sun Microsystems and VMware. Andy has \ntaught various operating systems classes over the years and is a coauthor of the popular \nUNIX Network Programming textbook.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "22.93 sec"
            },
            {
              "page_number": 16,
              "text": "xvii\nPiotr Balcer is a software engineer at Intel Corporation with many years’ experience \nworking on storage-related technologies. He holds a Bachelor of Science in engineering \nfrom the Gdańsk University of Technology, Poland, where he studied system software \nengineering. Piotr has been working on the software ecosystem for next-generation \npersistent memory since 2014.\nEduardo Berrocal joined Intel Corporation as a cloud software engineer in 2017 after \nreceiving his PhD in computer science from the Illinois Institute of Technology. His \ndoctoral research focused on data analytics and fault tolerance for high-performance \ncomputing. Past experience includes working as an intern at Bell Labs (Nokia), a research \naid at Argonne National Laboratory, a scientific programmer and web developer at the \nUniversity of Chicago, and an intern in the CESVIMA laboratory in Spain.\nAdam Borowski is a software engineer at Intel Corporation, hailing from the \nUniversity of Warsaw, Poland. He is a Debian developer and has made many open \nsource contributions over the past two decades. Adam is currently working on \npersistent memory stacks, both on upstream code and integrating it with downstream \ndistributions.\nIgor Chorazewicz is a software engineer at Intel Corporation. His main focus is on \npersistent memory data structures and enabling C++ applications for persistent \nmemory. Igor holds a Bachelor of Science in engineering from the Gdańsk University of \nTechnology, Poland.\nAdam Czapski is a technical writer at Intel Corporation. He writes technical \ndocumentation in the Data Center Group and is currently working in the persistent \nmemory department. Adam holds a Bachelor of Arts in English philology and a master’s \ndegree in natural language processing from the Gdańsk University of Technology, Poland.\nSteve Dohrmann is a software engineer at Intel Corporation. He has worked on a variety \nof projects over the past 20 years, including media frameworks, mobile agent software, \nsecure collaboration software, and parallel programming language implementation. He \nis currently working on enabling the use of persistent memory in Java*.\nAbout the Contributors",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "23.40 sec"
            },
            {
              "page_number": 17,
              "text": "xviii\nChet Douglas is a principal software engineer at Intel Corporation and focuses on cloud \nsoftware architecture along with operating system and OEM enabling of non-volatile \nmemory technologies. He has over 14 years’ experience working on various enterprise \nand client programs and 28 years of total storage experience. Chet has worked in all \naspects of storage, including storage controller hardware design, SCSI disk/tape/CD \nwriter firmware architecture, storage management software architecture, Microsoft \nWindows* and Linux kernel-mode drivers, enterprise hardware RAID, and client/\nworkstation software RAID. He holds seven storage-related hardware and software \npatents and has a dual Bachelor of Science in electrical engineering and computer \nengineering from Clarkson University, New York.\nKen Gibson is the director of persistent memory software architecture within Intel \nCorporation’s Data Center Group. Since 2012, Ken and his team have been working with \nIntel’s server and software partners to create the open persistent memory programming \nmodel.\nTomasz Gromadzki is a software architect in Intel Corporation’s Non-Volatile Memory \nSolutions Group. His focus is on remote persistent memory access, which includes \nproper integration of persistent memory with other (networking) technologies as well as \noptimal persistent memory replication procedures and algorithms.\nKishor Kharbas is a software engineer on the Java runtime engineering team at Intel \nCorporation. For the past eight years, he has been working to optimize Oracle’s OpenJDK \non Intel platforms. This involves Java garbage collection and compiler back-end \noptimization.\nJackson Marusarz is a senior technical consulting engineer (TCE) in Intel Corporation's \nCompute Performance and Developer Products Division. As the lead TCE for Intel \nVTune Profiler, his main focus is on software performance analysis and tuning for both \nserial and multithreaded applications. Jackson’s time is split between determining how \nto analyze and tune software and creating tools that help others do the same.\nJan Michalski is a software engineer in Intel Corporation’s Non-­Volatile Memory \nSolutions Group. His focus is on remote persistent memory access, which includes \nproper integration of persistent memory with other technologies, as well as looking for \noptimal persistent memory replication procedures and algorithms. He holds a master's \ndegree in computer engineering from the Gdańsk University of Technology, Poland, \nwhere he studied system software engineering.\nAbout the Contributors",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "23.89 sec"
            },
            {
              "page_number": 18,
              "text": "xix\nNicholas Moulin is a cloud software architect at Intel Corporation. Since joining Intel \nin 2012, he has focused on enabling and developing persistent memory software for \noperating systems and platform firmware and managing persistent memory hardware. \nNicholas is currently working with industry partners to define and improve RAS features \nrelevant to the persistent memory programming model.\nSzymon Romik is a software engineer at Intel Corporation and is currently focused on \npersistent memory programming. He previously worked as a lead software engineer \non 5G technologies at Ericsson. Szymon holds a master’s degree in mathematics from \nJagiellonian University, Poland.\nJakub Schmiegel is a software architect in Intel Corporation’s Non-Volatile Memory \nSolutions Group where he has been focused on enabling existing applications to \npersistent memory and analyzing their performance for more than four years. Jakub \nholds a master’s degree in computer science from the Gdańsk University of Technology, \nPoland.\nKevin Shalkowsky is a Telly Award–winning creative director, graphic designer, and \nanimator with more than a decade of experience. While his contributions are in \ntechnology today, Kevin has spent time in broadcast journalism and selling numerous \nproducts through 30-minute late-night infomercials. He resides in Oregon with his wife \nand son. From time to time, you can find Kevin lost in the woods, lost in a parking lot, \nor lost in his design process – but this somehow got him to where he is today, and he \nwouldn’t have it any other way.\nVineet Singh is a memory and storage tools software engineer at Intel Corporation. \nHe develops techniques to help developers adapt to the latest memory technologies. \nVineet holds a PhD in philosophy from the University of California and has a Bachelor \nof Technology degree from the Indian Institute of Information Technology, Design, and \nManufacturing in Jabalpur.\nPawel Skowron is a software engineering manager at Intel Corporation with 20 years' \nexperience in the software industry. Pawel has worked in various roles related to the \nwhole-software development life cycle. His software engineering background lies in the \nareas of embedded systems, database systems, and applications. For the past few years, \nPawel has led the development and validation of the Persistent Memory Development \nKit (https://github.com/pmem/pmdk).\nAbout the Contributors",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "25.51 sec"
            },
            {
              "page_number": 19,
              "text": "xx\nUsha Upadhyayula has been with Intel Corporation for 20 years serving in many \ndifferent roles. Usha holds a master’s degree in computer science from the University \nof South Carolina, and she spent the first few years at Intel developing user-level \napplications in C and C++. She later moved to customer-­enabling roles for Intel media \nprocessors and support for Intel RAID software. Usha is currently part of the Data Center \nGroup where she is focused on enabling cloud service providers to fully utilize and \naccelerate the adoption of Intel persistent memory products.\nSergey Vinogradov is a senior software development engineer at Intel Corporation \nwhere he spent more than seven years working on performance profiling tools and \nthreading runtime libraries. During the past four years, Sergey has been working on C++ \nprogramming models and performance profiling methodologies for persistent memory.  \nAbout the Contributors",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "28.17 sec"
            },
            {
              "page_number": 20,
              "text": "xxi\nAcknowledgments\nFirst and foremost, I would like to thank Ken Gibson for masterminding this book idea \nand for gifting me the pleasure of writing and managing it. Your support, guidance, and \ncontributions have been instrumental in delivering a high-quality product.\nIf the Vulcan mind-meld or The Matrix Headjack were possible, I could have cloned \nAndy Rudoff’s mind and allowed him to work on his daily activities. Instead, Andy’s \ninfinite knowledge of persistent memory had to be tapped through good old verbal \ncommunication and e-mail. I sincerely thank you for devoting so much time to me and \nthis project. The results read for themselves.\nDebbie Graham was instrumental in helping me manage this colossal project. Her \ndedication and support helped drive the project to an on-time completion.\nTo my friends and colleagues at Intel who contributed content, supported \ndiscussions, helped with decision-making, and reviewed drafts during the book-writing \nprocess. These are the real heroes. Without your heavily invested time and support, this \nbook would have taken considerably longer to complete. It is a much better product as a \nresult of the collaborative effort. A huge thanks to all of you.\nI'd like to express my sincerest gratitude and appreciation to the people at Apress, \nwithout whom this book could not have been published. From the initial contact and \noutline discussions through the entire publishing process to this final polished product, \nthe Apress team delivered continuous support and assistance. Many thanks to Susan, \nJessica, and Rita. It was a real pleasure working with you.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "32.78 sec"
            },
            {
              "page_number": 21,
              "text": "xxiii\nPreface\n\u0007About This Book\nPersistent memory is often referred to as non-volatile memory (NVM) or storage \nclass memory (SCM). In this book, we purposefully use persistent memory as an all-­\nencompassing term to represent all the current and future memory technologies that \nfall under this umbrella. This book introduces the persistent memory technology and \nprovides answers to key questions. For software developers, those questions include: \nWhat is persistent memory? How do I use it? What APIs and libraries are available? \nWhat benefits can it provide for my application? What new programming methods do I \nneed to learn? How do I design applications to use persistent memory? Where can I find \ninformation, documentation, and help?\nSystem and cloud architects will be provided with answers to questions such as: \nWhat is persistent memory? How does it work? How is it different than DRAM or SSD/\nNVMe storage devices? What are the hardware and operating system requirements? \nWhat applications need or could benefit from persistent memory? Can my existing \napplications use persistent memory without being modified?\nPersistent memory is not a plug-and-play technology for software applications. \nAlthough it may look and feel like traditional DRAM memory, applications need to be \nmodified to fully utilize the persistence feature of persistent memory. That is not to say \nthat applications cannot run unmodified on systems with persistent memory installed, \nthey can, but they will not see the full potential of what persistent memory offers without \ncode modification.\nThankfully, server and operating system vendors collaborated very early in the \ndesign phase and already have products available on the market. Linux and Microsoft \nWindows already provide native support for persistent memory technologies. Many \npopular virtualization technologies also support persistent memory.\nFor ISVs and the developer community at large, the journey is just beginning. Some \nsoftware has already been modified and is available on the market. However, it will \ntake time for the enterprise and cloud computing industries to adopt and make the \nhardware available to the general marketplace. ISVs and software developers need time \nto understand what changes to existing applications are required and implement them.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "39.75 sec"
            },
            {
              "page_number": 22,
              "text": "xxiv\nTo make the required development work easier, Intel developed and open sourced \nthe Persistent Memory Development Kit (PMDK) available from https://pmem.io/\npmdk/. We introduce the PMDK in more detail in Chapter 5 and walk through most of \nthe available libraries in subsequent chapters. Each chapter provides an in-depth guide \nso developers can understand what library or libraries to use. PMDK is a set of open \nsource libraries and tools based on the Storage Networking Industry Association (SNIA) \nNVM programming model designed and implemented by over 50 industry partners. The \nlatest NVM programming model document can be found at https://www.snia.org/\ntech_activities/standards/curr_standards/npm. The model describes how software \ncan utilize persistent memory features and enables designers to develop APIs that take \nadvantage of NVM features and performance.\nAvailable for both Linux and Windows, PMDK facilitates persistent memory \nprogramming adoption with higher-level language support. C and C++ support is fully \nvalidated. Support for other languages such as Java and Python is work in progress \nat the time this book was written. Other languages are expected to also adopt the \nprogramming model and provide native persistent memory APIs for developers. The \nPMDK development team welcomes and encourages new contributions to core code, \nnew language bindings, or new storage engines for the persistent memory key-value \nstore called pmemkv.\nThis book assumes no prior knowledge of persistent memory hardware devices \nor software development. The book layout allows you to freely navigate the content in \nthe order you want. It is not required to read all chapters in order, though we do build \nupon concepts and knowledge described in previous chapters. In such cases, we make \nbackward and forward references to relevant chapters and sections so you can learn or \nrefresh your memory.\n\u0007Book Structure\nThis book has 19 chapters, each one focusing on a different topic. The book has three \nmain sections. Chapters 1-4 provide an introduction to persistent memory architecture, \nhardware, and operating system support. Chapters 5-16 allow developers to understand \nthe PMDK libraries and how to use them in applications. Finally, Chapters 17-19 provide \ninformation on advanced topics such as RAS and replication of data using RDMA.\nPreface",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "44.46 sec"
            },
            {
              "page_number": 23,
              "text": "xxv\n•\t\nChapter 1. Introduction to Persistent Memory – Introduces persistent \nmemory and dips our toes in the water with a simple persistent key-­\nvalue store example using libpmemkv.\n•\t\nChapter 2. Persistent Memory Architecture – Describes the persistent \nmemory architecture and focuses on the hardware requirements \ndevelopers should know.\n•\t\nChapter 3. Operating System Support for Persistent Memory – \nProvides information relating to operating system changes, new \nfeatures, and how persistent memory is seen by the OS.\n•\t\nChapter 4. Fundamental Concepts of Persistent Memory \nProgramming – Builds on the first three chapters and describes the \nfundamental concepts of persistent memory programming.\n•\t\nChapter 5. Introducing the Persistent Memory Development Kit \n(PMDK) – Introduces the Persistent Memory Development Kit \n(PMDK), a suite of libraries to assist software developers.\n•\t\nChapter 6. libpmem: Low-Level Persistent Memory Support – \nDescribes and shows how to use libpmem from the PMDK, a low-­level \nlibrary providing persistent memory support.\n•\t\nChapter 7. libpmemobj: A Native Transactional Object Store – \nProvides information and examples using libpmemobj, a C native \nobject store library from the PMDK.\n•\t\nChapter 8. libpmemobj-cpp: The Adaptable Language - C++ and \nPersistent Memory – Demonstrates the C++ libpmemobj-cpp object \nstore from the PMDK, built using C++ headers on top of libpmemobj.\n•\t\nChapter 9. pmemkv: A Persistent In-Memory Key-Value Store – \nExpands upon the introduction to libpmemkv from Chapter 1 with a \nmore in-depth discussion using examples.\n•\t\nChapter 10. Volatile Use of Persistent Memory – This chapter is \nfor those who want to take advantage of persistent memory but \ndo not require data to be stored persistently. libmemkind is a user-\nextensible heap manager built on top of jemalloc which enables \ncontrol of memory characteristics and a partitioning of the heap \nPreface",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "49.06 sec"
            },
            {
              "page_number": 24,
              "text": "xxvi\nbetween different kinds of memory, including persistent memory. \nlibvmemcache is an embeddable and lightweight in-memory caching \nsolution. It is designed to fully take advantage of large-capacity \nmemory, such as persistent memory with DAX, through memory \nmapping in an efficient and scalable way.\n•\t\nChapter 11. Designing Data Structures for Persistent Memory – \nProvides a wealth of information for designing data structures for \npersistent memory.\n•\t\nChapter 12. Debugging Persistent Memory Applications – Introduces \ntools and walks through several examples for how software developers \ncan debug persistent memory–enabled applications.\n•\t\nChapter 13. Enabling Persistence using a Real-World Application – \nDiscusses how a real-world application was modified to enable \npersistent memory features.\n•\t\nChapter 14. Concurrency and Persistent Memory – Describes how \nconcurrency in applications should be implemented for use with \npersistent memory.\n•\t\nChapter 15. Profiling and Performance – Teaches performance \nconcepts and demonstrates how to use the Intel VTune suite of tools \nto profile systems and applications before and after code changes are \nmade.\n•\t\nChapter 16. PMDK Internals: Important Algorithms and Data \nStructures – Takes us on a deep dive of the PMDK design, architecture, \nalgorithms, and memory allocator implementation.\n•\t\nChapter 17. Reliability, Availability, and Serviceability (RAS) – \nDescribes the implementation of reliability, availability, and \nserviceability (RAS) with the hardware and operating system layers.\n•\t\nChapter 18. Remote Persistent Memory – Discusses how applications \ncan scale out across multiple systems using local and remote persistent \nmemory.\n•\t\nChapter 19. Advanced Topics – Describes things such as NUMA, using \nsoftware volume managers, and the mmap() MAP_SYNC flag.\nPreface",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "54.70 sec"
            },
            {
              "page_number": 25,
              "text": "xxvii\nThe Appendixes have separate procedures for installing the PMDK and utilities \nrequired for managing persistent memory. We also included an update for Java and the \nfuture of the RDMA protocols. All of this content is considered temporal, so we did not \nwant to include it in the main body of the book.\n\u0007Intended Audience\nThis book has been written for experienced application developers in mind. We \nintend the content to be useful to a wider readership such as system administrators \nand architects, students, lecturers, and academic research fellows to name but a few. \nSystem designers, kernel developers, and anyone with a vested or passing interest in this \nemerging technology will find something useful within this book.\nEvery reader will learn what persistent memory is, how it works, and how operating \nsystems and applications can utilize it. Provisioning and managing persistent memory \nare vendor specific, so we include some resources in the Appendix sections to avoid \novercomplicating the main chapter content.\nApplication developers will learn, by example, how to integrate persistent memory \nin to existing or new applications. We use examples extensively throughout this book \nusing a variety of libraries available within the Persistent Memory Development Kit \n(PMDK). Example code is provided in a variety of programming languages such as C, \nC++, JavaScript, and others. We want developers to feel comfortable using these libraries \nin their own projects. The book provides extensive links to resources where you can find \nhelp and information.\nSystem administrators and architects of Cloud, high-performance computing, \nand enterprise environments can use most of the content of this book to \nunderstand persistent memory features and benefits to support applications and \ndevelopers. Imagine being able to deploy more virtual machines per physical server or \nprovide applications with this new memory/storage tier such that they can keep more \ndata closer to the CPU or restart in a fraction of the time they could before while keeping \na warm cache of data. \nStudents, lecturers, and academic research fellows will also benefit from many \nchapters within this book. Computer science classes can learn about the hardware, \noperating system features, and programming techniques. Lecturers are free use the \ncontent in student classes or to form the basis of research projects such as new persistent \nmemory file systems, algorithms, or caching implementations.\nPreface",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "64.22 sec"
            },
            {
              "page_number": 26,
              "text": "xxviii\nWe introduce tools that profile the server and applications to better understand CPU, \nmemory, and disk IO access patterns. Using this knowledge, we show how applications \ncan be modified to take full advantage of persistence using the Persistent Memory \nDevelopment Kit (PMDK). \n\u0007A Future Reference\nThe book content has been written to provide value for many years. Industry \nspecification such as ACPI, UEFI, and the SNIA non-volatile programming model will, \nunless otherwise stated by the specification, remain backward compatible as new \nversions are released. If new form factors are introduced, the approach to programming \nremains the same. We do not limit ourselves to one specific persistent memory vendor \nor implementation. In places where it is necessary to describe vendor-specific features \nor implementations, we specifically call this out as it may change between vendors or \nbetween product generations. We encourage you to read the vendor documentation for \nthe persistent memory product to learn more.\nDevelopers using the Persistent Memory Development Kit (PMDK) will retain a stable \nAPI interface. PMDK will deliver new features and performance improvements with each \nmajor release. It will evolve with new persistent memory products, CPU instructions, \nplatform designs, industry specifications, and operating system feature support.\n\u0007Source Code Examples\nConcepts and source code samples within this book adhere to the vendor neutral \nSNIA non-volatile memory programming model. SNIA which is the Storage \nNetworking Industry Association is a non-profit global organization dedicated to \ndeveloping standards and education programs to advance storage and information \ntechnology. The model was designed, developed, and is maintained by the SNIA NVM \nTechnical Working Group (TWG) which includes many leading operating system, \nhardware, and server vendors. You can join this group or find information at https://\nwww.snia.org/forums/sssi/nvmp.\nPreface",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "65.86 sec"
            },
            {
              "page_number": 27,
              "text": "xxix\nThe code examples provided with this book have been tested and validated using \nIntel Optane DC persistent memory. Since the PMDK is vendor neutral, they will also \nwork on NVDIMM-N devices. PMDK will support any future persistent memory product \nthat enters the market.\nThe code examples used throughout this book are current at the time of \npublication. All code examples have been validated and tested to ensure they compile \nand execute without error. For brevity, some of the examples in this book use assert() \nstatements to indicate unexpected errors. Any production code would likely replace \nthese with the appropriate error handling actions which would include friendlier \nerror messages and appropriate error recovery actions. Additionally, some of the code \nexamples use different mount points to represent persistent memory aware file systems, \nfor example “/daxfs”, “/pmemfs”, and “/mnt/pmemfs”. This demonstrates persistent \nmemory file systems can be mounted and named appropriately for the application, just \nlike regular block-based file systems. Source code is from the repository that accompanies \nthis book – https://github.com/Apress/programming-persistent-memory.\nSince this is a rapidly evolving technology, the software and APIs references \nthroughout this book may change over time. While every effort is made to be backward \ncompatible, sometimes software must evolve and invalidate previous versions. For this \nreason, it is therefore expected that some of the code samples may not compile on newer \nhardware or operating systems and may need to be changed accordingly. \n\u0007Book Conventions\nThis book uses several conventions to draw your attention to specific pieces of \ninformation. The convention used depends on the type of information displayed.\n\u0007Computer Commands\nCommands, programming library, and API function references may be presented in line \nwith the paragraph text using a monospaced font. For example:\nTo illustrate how persistent memory is used, let’s start with a sample program \ndemonstrating the key-value store provided by a library called libpmemkv.\nPreface",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "70.67 sec"
            },
            {
              "page_number": 28,
              "text": "xxx\n\u0007Computer Terminal Output\nComputer terminal output is usually taken directly from a computer terminal presented \nin a monospaced font such as the following example demonstrating cloning the \nPersistent Memory Development Kit (PMDK) from the GitHub project:\n$ git clone https://github.com/pmem/pmdk\nCloning into 'pmdk'...\nremote: Enumerating objects: 12, done.\nremote: Counting objects: 100% (12/12), done.\nremote: Compressing objects: 100% (10/10), done.\nremote: Total 100169 (delta 2), reused 7 (delta 2), pack-reused 100157\nReceiving objects: 100% (100169/100169), 34.71 MiB | 4.85 MiB/s, done.\nResolving deltas: 100% (83447/83447), done.\n\u0007Source Code\nSource code examples taken from the accompanying GitHub repository are shown with \nrelevant line numbers in a monospaced font. Below each code listing is a reference to \nthe line number or line number range with a brief description. Code comments use \nlanguage native styling. Most languages use the same syntax. Single line comments \nwill use // and block/multiline comments should use /*..*/. An example is shown in \nListing 1.\nListing 1.  A sample program using libpmemkv\n    37  #include <iostream>\n    38  #include \"libpmemkv.h\"\n    39  \n    40  using namespace pmemkv;\n    41  \n    42  /*\n    43   * kvprint -- print a single key-value pair\n    44   */\n    45  void kvprint(const string& k, const string& v) {\n    46      std::cout << \"key: \" << k << \", value: \" << v << \"\\n\";\n    47  }\nPreface",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "74.66 sec"
            },
            {
              "page_number": 29,
              "text": "xxxi\n•\t\nLine 45: Here we define a small helper routine, kvprint(), which prints \na key-value pair when called.\n\u0007Notes\nWe use a standard format for notes, cautions, and tips when we want to direct your \nattention to an important point, for example.\nNote  Notes are tips, shortcuts, or alternative approaches to the current \ndiscussion topic. Ignoring a note should have no negative consequences, but you \nmight miss out on a nugget of information that makes your life easier.\nPreface",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "76.30 sec"
            },
            {
              "page_number": 30,
              "text": "1\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_1\nCHAPTER 1\nIntroduction to Persistent \nMemory Programming\nThis book describes programming techniques for writing applications that use persistent \nmemory. It is written for experienced software developers, but we assume no previous \nexperience using persistent memory. We provide many code examples in a variety of \nprogramming languages. Most programmers will understand these examples, even if \nthey have not previously used the specific language.\nNote  All code examples are available on a GitHub repository (https://\ngithub.com/Apress/programming-persistent-memory), along with \ninstructions for building and running it.\nAdditional documentation for persistent memory, example programs, tutorials, and \ndetails on the Persistent Memory Development Kit (PMDK), which is used heavily in this \nbook, can be found on http://pmem.io.\nThe persistent memory products on the market can be used in various ways, and \nmany of these ways are transparent to applications. For example, all persistent memory \nproducts we encountered support the storage interfaces and standard file API’s just like \nany solid-state disk (SSD). Accessing data on an SSD is simple and well-understood, so \nwe consider these use cases outside the scope of this book. Instead, we concentrate on \nmemory-style access, where applications manage byte-addressable data structures that \nreside in persistent memory. Some use cases we describe are volatile, using the persistent \nmemory only for its capacity and ignoring the fact it is persistent. However, most of this \nbook is dedicated to the persistent use cases, where data structures placed in persistent \nmemory are expected to survive crashes and power failures, and the techniques \ndescribed in this book keep those data structures consistent across those events.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "82.24 sec"
            },
            {
              "page_number": 31,
              "text": "2\n\u0007A High-Level Example Program\nTo illustrate how persistent memory is used, we start with a sample program \ndemonstrating the key-value store provided by a library called libpmemkv. Listing 1-1 \nshows a full C++ program that stores three key-value pairs in persistent memory and \nthen iterates through the key-value store, printing all the pairs. This example may seem \ntrivial, but there are several interesting components at work here. Descriptions below the \nlisting show what the program does.\nListing 1-1.  A sample program using libpmemkv\n    37  #include <iostream>\n    38  #include <cassert>\n    39  #include <libpmemkv.hpp>\n    40\n    41  using namespace pmem::kv;\n    42  using std::cerr;\n    43  using std::cout;\n    44  using std::endl;\n    45  using std::string;\n    46\n    47  /*\n    48   * for this example, create a 1 Gig file\n    49   * called \"/daxfs/kvfile\"\n    50   */\n    51  auto PATH = \"/daxfs/kvfile\";\n    52  const uint64_t SIZE = 1024 * 1024 * 1024;\n    53\n    54  /*\n    55   * kvprint -- print a single key-value pair\n    56   */\n    57  int kvprint(string_view k, string_view v) {\n    58      cout << \"key: \"    << k.data() <<\n    59          \" value: \" << v.data() << endl;\n    60      return 0;\n    61  }\n    62\nChapter 1  Introduction to Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "85.93 sec"
            },
            {
              "page_number": 32,
              "text": "3\n    63  int main() {\n    64      // start by creating the db object\n    65      db *kv = new db();\n    66      assert(kv != nullptr);\n    67\n    68      // create the config information for\n    69      // libpmemkv's open method\n    70      config cfg;\n    71\n    72      if (cfg.put_string(\"path\", PATH) != status::OK) {\n    73          cerr << pmemkv_errormsg() << endl;\n    74          exit(1);\n    75      }\n    76      if (cfg.put_uint64(\"force_create\", 1) != status::OK) {\n    77          cerr << pmemkv_errormsg() << endl;\n    78          exit(1);\n    79      }\n    80      if (cfg.put_uint64(\"size\", SIZE) != status::OK) {\n    81          cerr << pmemkv_errormsg() << endl;\n    82          exit(1);\n    83      }\n    84\n    85\n    86      // open the key-value store, using the cmap engine\n    87      if (kv->open(\"cmap\", std::move(cfg)) != status::OK) {\n    88          cerr << db::errormsg() << endl;\n    89          exit(1);\n    90      }\n    91\n    92      // add some keys and values\n    93      if (kv->put(\"key1\", \"value1\") != status::OK) {\n    94          cerr << db::errormsg() << endl;\n    95          exit(1);\n    96      }\nChapter 1  Introduction to Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "90.53 sec"
            },
            {
              "page_number": 33,
              "text": "4\n    97      if (kv->put(\"key2\", \"value2\") != status::OK) {\n    98          cerr << db::errormsg() << endl;\n    99          exit(1);\n   100      }\n   101      if (kv->put(\"key3\", \"value3\") != status::OK) {\n   102          cerr << db::errormsg() << endl;\n   103          exit(1);\n   104      }\n   105\n   106      // iterate through the key-value store, printing them\n   107      kv->get_all(kvprint);\n   108\n   109      // stop the pmemkv engine\n   110      delete kv;\n   111\n   112      exit(0);\n   113  }\n•\t\nLine 57: We define a small helper routine, kvprint(), which prints a \nkey-value pair when called.\n•\t\nLine 63: This is the first line of main() which is where every C++ \nprogram begins execution. We start by instantiating a key-value \nengine using the engine name \"cmap\". We discuss other engine types \nin Chapter 9.\n•\t\nLine 70: The cmap engine takes config parameters from a config \nstructure. The parameter \"path\" is configured to \"/daxfs/kvfile\", \nwhich is the path to a persistent memory file on a DAX file system; \nthe parameter \"size\" is set to SIZE. Chapter 3 describes how to \ncreate and mount DAX file systems. \n•\t\nLine 93: We add several key-value pairs to the store. The trademark \nof a key-value store is the use of simple operations like put() and \nget(); we only show put() in this example.\n•\t\nLine 107: Using the get_all() method, we iterate through the \nentire key-value store, printing each pair when get_all() calls our \nkvprint() routine.\nChapter 1  Introduction to Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "96.68 sec"
            },
            {
              "page_number": 34,
              "text": "5\n\u0007What’s Different?\nA wide variety of key-value libraries are available in practically every programming \nlanguage. The persistent memory example in Listing 1-1 is different because the key-­\nvalue store itself resides in persistent memory. For comparison, Figure 1-1 shows how a \nkey-value store using traditional storage is laid out.\nWhen the application in Figure 1-1 wants to fetch a value from the key-value store, \na buffer must be allocated in memory to hold the result. This is because the values are \nkept on block storage, which cannot be addressed directly by the application. The only \nway to access a value is to bring it into memory, and the only way to do that is to read \nfull blocks from the storage device, which can only be accessed via block I/O. Now \nconsider Figure 1-2, where the key-value store resides in persistent memory like our \nsample code.\nFigure 1-1.  A key-value store on traditional storage\nChapter 1  Introduction to Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page34_img1.jpeg"
              ],
              "img_summary_files": [],
              "img_vision_files": [
                "output\\images\\img_vision\\Programming_Persistent_Memory_medium_457_page34_img1_vision.json"
              ],
              "summary": "Summary not available due to an error.",
              "time_taken": "100.06 sec"
            },
            {
              "page_number": 35,
              "text": "6\nWith the persistent memory key-value store, values are accessed by the application \ndirectly, without the need to first allocate buffers in memory. The kvprint() routine in \nListing 1-1 will be called with references to the actual keys and values, directly where \nthey live in persistence – something that is not possible with traditional storage. In \nfact, even the data structures used by the key-value store library to organize its data are \naccessed directly. When a storage-based key-value store library needs to make a small \nupdate, for example, 64 bytes, it must read the block of storage containing those 64 bytes \ninto a memory buffer, update the 64 bytes, and then write out the entire block to make it \npersistent. That is because storage accesses can only happen using block I/O, typically \n4K bytes at a time, so the task to update 64 bytes requires reading 4K and then writing \n4K. But with persistent memory, the same example of changing 64 bytes would only \nwrite the 64 bytes directly to persistence.\n\u0007The Performance Difference\nMoving a data structure from storage to persistent memory does not just mean smaller \nI/O sizes are supported; there is a fundamental performance difference. To illustrate this, \nFigure 1-3 shows a hierarchy of latency among the different types of media where data \ncan reside at any given time in a program.\nFigure 1-2.  A key-value store in persistent memory\nChapter 1  Introduction to Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page35_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page35_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "103.95 sec"
            },
            {
              "page_number": 36,
              "text": "7\nAs the pyramid shows, persistent memory provides latencies similar to memory, \nmeasured in nanoseconds, while providing persistency. Block storage provides \npersistency with latencies starting in the microseconds and increasing from there, \ndepending on the technology. Persistent memory is unique in its ability to act like both \nmemory and storage at the same time.\n\u0007Program Complexity\nPerhaps the most important point of our example is that the programmer still uses \nthe familiar get/put interfaces normally associated with key-value stores. The fact that \nthe data structures are in persistent memory is abstracted away by the high-level API \nprovided by libpmemkv. This principle of using the highest level of abstraction possible, \nas long as it meets the application’s needs, will be a recurring theme throughout this \nbook. We start by introducing very high-level APIs; later chapters delve into the lower-­\nlevel details for programmers who need them. At the lowest level, programming directly \nto raw persistent memory requires detailed knowledge of things like hardware atomicity, \ncache flushing, and transactions. High-level libraries like libpmemkv abstract away all \nthat complexity and provide much simpler, less error-prone interfaces.\nFigure 1-3.  The memory/storage hierarchy pyramid with estimated latencies\nChapter 1  Introduction to Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page36_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page36_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "108.56 sec"
            },
            {
              "page_number": 37,
              "text": "8\n\u0007How Does libpmemkv Work?\nAll the complexity hidden by high-level libraries like libpmemkv are described more fully \nin later chapters, but let’s look at the building blocks used to construct a library like this. \nFigure 1-4 shows the full software stack involved when an application uses libpmemkv.\nStarting from the bottom of Figure 1-4 and working upward are these components:\n•\t\nThe persistent memory hardware, typically connected to the system \nmemory bus and accessed using common memory load/store \noperations.\n•\t\nA pmem-aware file system, which is a kernel module that exposes \npersistent memory to applications as files. Those files can be memory \nmapped to give applications direct access (abbreviated as DAX). \nThis method of exposing persistent memory was published by SNIA \n(Storage Networking Industry Association) and is described in detail \nin Chapter 3.\n•\t\nThe libpmem library is part of the PMDK. This library abstracts \naway some of the low-level hardware details like cache flushing \ninstructions.\nFigure 1-4.  The software stack when using libpmemkv\nChapter 1  Introduction to Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page37_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page37_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "111.43 sec"
            },
            {
              "page_number": 38,
              "text": "9\n•\t\nThe libpmemobj library is a full-featured transaction and allocation \nlibrary for persistent memory. (Chapters 7 and 8 describe libpmemobj \nand its C++ cousin in more detail.) If you cannot find data structures \nthat meet your needs, you will most likely have to implement what \nyou need using this library, as described in Chapter 11.\n•\t\nThe cmap engine, a concurrent hash map optimized for persistent \nmemory.\n•\t\nThe libpmemkv library, providing the API demonstrated in Listing 1-1.\n•\t\nAnd finally, the application that uses the API provided by libpmemkv.\nAlthough there is quite a stack of components in use here, it does not mean there \nis necessarily a large amount of code that runs for each operation. Some components \nare only used during the initial setup. For example, the pmem-aware file system is \nused to find the persistent memory file and perform permission checks; it is out of the \napplication’s data path after that. The PMDK libraries are designed to leverage the direct \naccess allowed by persistent memory as much as possible.\n\u0007What’s Next?\nChapters 1 through 3 provide the essential background that programmers need to know to \nstart persistent memory programming. The stage is now set with a simple example; the next \ntwo chapters provide details about persistent memory at the hardware and operating system \nlevels. The later and more advanced chapters provide much more detail for those interested.\nBecause the immediate goal is to get you programming quickly, we recommend \nreading Chapters 2 and 3 to gain the essential background and then dive into Chapter 4 \nwhere we start to show more detailed persistent memory programming examples.\n\u0007Summary\nThis chapter shows how high-level APIs like libpmemkv can be used for persistent \nmemory programming, hiding complex details of persistent memory from the \napplication developer. Using persistent memory can allow finer-grained access and \nhigher performance than block-based storage. We recommend using the highest-level, \nsimplest APIs possible and only introducing the complexity of lower-level persistent \nmemory programming as necessary.\nChapter 1  Introduction to Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "117.06 sec"
            },
            {
              "page_number": 39,
              "text": "10\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 1  Introduction to Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "119.62 sec"
            },
            {
              "page_number": 40,
              "text": "11\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_2\nCHAPTER 2\nPersistent Memory \nArchitecture\nThis chapter provides an overview of the persistent memory architecture while focusing \non the hardware to emphasize requirements and decisions that developers need to know.\nApplications that are designed to recognize the presence of persistent memory in \na system can run much faster than using other storage devices because data does not \nhave to transfer back and forth between the CPU and slower storage devices. Because \napplications that only use persistent memory may be slower than dynamic random-­\naccess memory (DRAM), they should decide what data resides in DRAM, persistent \nmemory, and storage.\nThe capacity of persistent memory is expected to be many times larger than DRAM; \nthus, the volume of data that applications can potentially store and process in place is \nalso much larger. This significantly reduces the number of disk I/Os, which improves \nperformance and reduces wear on the storage media.\nOn systems without persistent memory, large datasets that cannot fit into DRAM \nmust be processed in segments or streamed. This introduces processing delays as the \napplication stalls waiting for data to be paged from disk or streamed from the network.\nIf the working dataset size fits within the capacity of persistent memory and DRAM, \napplications can perform in-memory processing without needing to checkpoint or page \ndata to or from storage. This significantly improves performance.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "123.30 sec"
            },
            {
              "page_number": 41,
              "text": "12\n\u0007Persistent Memory Characteristics\nAs with every new technology, there are always new things to consider. Persistent \nmemory is no exception. Consider these characteristics when architecting and \ndeveloping solutions:\n•\t\nPerformance (throughput, latency, and bandwidth) of persistent \nmemory is much better than NAND but potentially slower than \nDRAM.\n•\t\nPersistent memory is durable unlike DRAM. Its endurance is usually \norders of magnitude better than NAND and should exceed the \nlifetime of the server without wearing out.\n•\t\nPersistent memory module capacities can be much larger than \nDRAM DIMMs and can coexist on the same memory channels.\n•\t\nPersistent memory-enabled applications can update data in place \nwithout needing to serialize/deserialize the data.\n•\t\nPersistent memory is byte addressable like memory. Applications \ncan update only the data needed without any read-modify-write \noverhead.\n•\t\nData is CPU cache coherent.\n•\t\nPersistent memory provides direct memory access (DMA) and \nremote DMA (RDMA) operations.\n•\t\nData written to persistent memory is not lost when power is removed. \n•\t\nAfter permission checks are completed, data located on persistent \nmemory is directly accessible from user space. No kernel code, file \nsystem page caches, or interrupts are in the data path.\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "126.89 sec"
            },
            {
              "page_number": 42,
              "text": "13\n•\t\nData on persistent memory is instantly available, that is:\n•\t\nData is available as soon as power is applied to the system.\n•\t\nApplications do not need to spend time warming up caches. They \ncan access the data immediately upon memory mapping it.\n•\t\nData residing on persistent memory has no DRAM footprint \nunless the application copies data to DRAM for faster access.\n•\t\nData written to persistent memory modules is local to the system. \nApplications are responsible for replicating data across systems.\n\u0007Platform Support for Persistent Memory\nPlatform vendors such as Intel, AMD, ARM, and others will decide how persistent \nmemory should be implemented at the lowest hardware levels. We try to provide a \nvendor-agnostic perspective and only occasionally call out platform-specific details.\nFor systems with persistent memory, failure atomicity guarantees that systems can \nalways recover to a consistent state following a power or system failure. Failure atomicity \nfor applications can be achieved using logging, flushing, and memory store barriers that \norder such operations. Logging, either undo or redo, ensures atomicity when a failure \ninterrupts the last atomic operation from completion. Cache flushing ensures that \ndata held within volatile caches reach the persistence domain so it will not be lost if a \nsudden failure occurs. Memory store barriers, such as an SFENCE operation on the x86 \narchitecture, help prevent potential reordering in the memory hierarchy, as caches and \nmemory controllers may reorder memory operations. For example, a barrier ensures \nthat the undo log copy of the data gets persisted onto the persistent memory before the \nactual data is modified in place. This guarantees that the last atomic operation can be \nrolled back should a failure occur. However, it is nontrivial to add such failure atomicity \nin user applications with low-level operations such as write logging, cache flushing, and \nbarriers. The Persistent Memory Development Kit (PMDK) was developed to isolate \ndevelopers from having to re-implement the hardware intricacies.\nFailure atomicity should be a familiar concept, since most file systems implement \nand perform journaling and flushing of their metadata to storage devices.\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "132.52 sec"
            },
            {
              "page_number": 43,
              "text": "14\n\u0007Cache Hierarchy\nWe use load and store operations to read and write to persistent memory rather than \nusing block-based I/O to read and write to traditional storage. We suggest reading the \nCPU architecture documentation for an in-depth description because each successive \nCPU generation may introduce new features, methods, and optimizations.\nUsing the Intel architecture as an example, a CPU cache typically has three \ndistinct levels: L1, L2, and L3. The hierarchy makes references to the distance \nfrom the CPU core, its speed, and size of the cache. The L1 cache is closest to \nthe CPU. It is extremely fast but very small. L2 and L3 caches are increasingly \nlarger in capacity, but they are relatively slower. Figure 2-1 shows a typical CPU \nmicroarchitecture with three levels of CPU cache and a memory controller with \nthree memory channels. Each memory channel has a single DRAM and persistent \nmemory attached. On platforms where the CPU caches are not contained within \nthe power-fail protected domain, any modified data within the CPU caches that has \nnot been flushed to persistent memory will be lost when the system loses power or \ncrashes.  Platforms that do include CPU caches in the power-fail protected domain \nwill ensure modified data within the CPU caches are flushed to the persistent \nmemory should the system crash or loses power. We describe these requirements \nand features in the upcoming “Power-Fail Protected Domains” section. \nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "136.20 sec"
            },
            {
              "page_number": 44,
              "text": "15\nThe L1 (Level 1) cache is the fastest memory in a computer system. In terms of access \npriority, the L1 cache has the data the CPU is most likely to need while completing a \nspecific task. The L1 cache is also usually split two ways, into the instruction cache (L1 I)  \nand the data cache (L1 D). The instruction cache deals with the information about the \noperation that the CPU has to perform, while the data cache holds the data on which the \noperation is to be performed.\nThe L2 (Level 2) cache has a larger capacity than the L1 cache, but it is slower. L2 \ncache holds data that is likely to be accessed by the CPU next. In most modern CPUs, \nthe L1 and L2 caches are present on the CPU cores themselves, with each core getting \ndedicated caches.\nThe L3 (Level 3) cache is the largest cache memory, but it is also the slowest of the \nthree. It is also a commonly shared resource among all the cores on the CPU and may be \ninternally partitioned to allow each core to have dedicated L3 resources.\nData read from DRAM or persistent memory is transferred through the memory \ncontroller into the L3 cache, then propagated into the L2 cache, and finally the L1 cache \nwhere the CPU core consumes it. When the processor is looking for data to carry out an \noperation, it first tries to find it into the L1 cache. If the CPU can find it, the condition is \ncalled a cache hit. If the CPU cannot find the data within the L1 cache, it then proceeds to \nFigure 2-1.  CPU cache and memory hierarchy\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page44_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page44_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "141.32 sec"
            },
            {
              "page_number": 45,
              "text": "16\nsearch for it first within L2, then L3. If it cannot find the data in any of the three, it tries to \naccess it from memory. Each failure to find data in a cache is called a cache miss. Failure \nto locate the data in memory requires the operating system to page the data into memory \nfrom a storage device.\nWhen the CPU writes data, it is initially written to the L1 cache. Due to ongoing \nactivity within the CPU, at some point in time, the data will be evicted from the L1 cache \ninto the L2 cache. The data may be further evicted from L2 and placed into L3 and \neventually evicted from L3 into the memory controller’s write buffers where it is then \nwritten to the memory device.\nIn a system that does not possess persistent memory, software persists data by writing it \nto a non-volatile storage device such as an SSD, HDD, SAN, NAS, or a volume in the cloud. \nThis protects data from application or system crashes. Critical data can be manually flushed \nusing calls such as msync(), fsync(), or fdatasync(), which flush uncommitted dirty \npages from volatile memory to the non-volatile storage device. File systems provide fdisk \nor chkdsk utilities to check and attempt repairs on damaged file systems if required. File \nsystems do not protect user data from torn blocks. Applications have a responsibility to \ndetect and recovery from this situation. That’s why databases, for example, use a variety of \ntechniques such as transactional updates, redo/undo logging, and checksums. \n Applications memory map the persistent memory address range directly into its \nown memory address space. Therefore, the application must assume responsibility \nfor checking and guaranteeing data integrity. The rest of this chapter describes \nyour responsibilities in a persistent memory environment and how to achieve data \nconsistency and integrity.\n\u0007Power-Fail Protected Domains\nA computer system may include one or more CPUs, volatile or persistent memory \nmodules, and non-volatile storage devices such as SSDs or HDDs.\nSystem platform hardware supports the concept of a persistence domain, also called \npower-fail protected domains. Depending on the platform, a persistence domain may \ninclude the persistent memory controller and write queues, memory controller write \nqueues, and CPU caches. Once data has reached the persistence domain, it may be \nrecoverable during a process that results from a system restart. That is, if data is located \nwithin hardware write queues or buffers protected by power failure, domain applications \nshould assume it is persistent. For example, if a power failure occurs, the data will be flushed \nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "148.29 sec"
            },
            {
              "page_number": 46,
              "text": "17\nfrom the power-fail protected domain using stored energy guaranteed by the platform for \nthis purpose. Data that has not yet made it into the protected domain will be lost. \nMultiple persistence domains may exist within the same system, for example, on \nsystems with more than one physical CPU. Systems may also provide a mechanism for \npartitioning the platform resources for isolation. This must be done in such a way that \nSNIA NVM programming model behavior is assured from each compliant volume or file \nsystem. (Chapter 3 describes the programming model as it applies to operating systems \nand file systems. The “Detecting Platform Capabilities” section in that chapter describes \nthe logic that applications should perform to detect platform capabilities including \npower failure protected domains. Later chapters provide in-depth discussions into why, \nhow, and when applications should flush data, if required, to guarantee the data is safe \nwithin the protected domain and persistent memory.)\nVolatile memory loses its contents when the computer system’s power is interrupted. \nJust like non-volatile storage devices, persistent memory keeps its contents even in the \nabsence of system power. Data that has been physically saved to the persistent memory \nmedia is called data at rest. Data in-flight has the following meanings:\n•\t\nWrites sent to the persistent memory device but have not yet been \nphysically committed to the media\n•\t\nAny writes that are in progress but not yet complete\n•\t\nData that has been temporarily buffered or cached in either the CPU \ncaches or memory controller\nWhen a system is gracefully rebooted or shut down, the system maintains power \nand can ensure all contents of the CPU caches and memory controllers are flushed such \nthat any in-flight or uncommitted data is successfully written to persistent memory \nor non-volatile storage. When an unexpected power failure occurs, and assuming no \nuninterruptable power supply (UPS) is available, the system must have enough stored \nenergy within the power supplies and capacitors dotted around it to flush data before the \npower is completely exhausted. Any data that is not flushed is lost and not recoverable.\nAsynchronous DRAM Refresh (ADR) is a feature supported on Intel products which \nflushes the write-protected data buffers and places the DRAM in self-refresh. This \nprocess is critical during a power loss event or system crash to ensure the data is in a safe \nand consistent state on persistent memory. By default, ADR does not flush the processor \ncaches. A platform that supports ADR only includes persistent memory and the memory \ncontroller’s write pending queues within the persistence domain. This is the reason \nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "155.25 sec"
            },
            {
              "page_number": 47,
              "text": "18\ndata in the CPU caches must be flushed by the application using the CLWB, CLFLUSHOPT, \nCLFLUSH, non-temporal stores, or WBINVD machine instructions.\nEnhanced Asynchronous DRAM Refresh (eADR) requires that a non-maskable \ninterrupt (NMI) routine be called to flush the CPU caches before the ADR event can begin. \nApplications running on an eADR platform do not need to perform flush operations \nbecause the hardware should flush the data automatically, but they are still required \nto perform an SFENCE operation to maintain write order correctness. Stores should be \nconsidered persistent only when they are globally visible, which the SFENCE guarantees.\nFigure 2-2 shows both the ADR and eADR persistence domains.\nADR is a mandatory platform requirement for persistent memory. The write \npending queue (WPQ) within the memory controller acknowledges receipt of the data \nto the writer once all the data is received. Although the data has not yet made it to the \npersistent media, a platform supporting ADR guarantees that it will be successfully \nwritten should a power loss event occur. During a crash or power failure, data that is in-­\nflight through the CPU caches can only be guaranteed to be flushed to persistent media \nif the platform supports eADR. It will be lost on platforms that only support ADR.\nThe challenge with extending the persistence domain to include the CPU caches is \nthat the CPU caches are quite large and it would take considerably more energy than the \ncapacitors in a typical power supply can practically provide. This means the platform \nwould have to contain batteries or utilize an external uninterruptable power supply. \nRequiring a battery for every server supporting persistent memory is not generally \npractical or cost-effective. The lifetime of a battery is typically shorter than the server, \nFigure 2-2.  ADR and eADR power-fail protection domains\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page47_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page47_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "160.38 sec"
            },
            {
              "page_number": 48,
              "text": "19\nwhich introduces additional maintenance routines that reduce server uptime. There \nis also an environmental impact when using batteries as they must be disposed of \nor recycled correctly. It is entirely possible for server or appliance OEMs to include a \nbattery in their product.\nBecause some appliance and server vendors plan to use batteries, and because \nplatforms will someday include the CPU caches in the persistence domain, a property is \navailable within ACPI such that the BIOS can notify software when the CPU flushes can \nbe skipped. On platforms with eADR, there is no need for manual cache line flushing.\n\u0007The Need for Flushing, Ordering, and Fencing\nExcept for WBINVD, which is a kernel-mode-only operation, the machine instructions \nin Table 2-1 (in the “Intel Machine Instructions for Persistent Memory” section) \nare supported in user space by Intel and AMD CPUs. Intel adopted the SNIA NVM \nprogramming model for working with persistent memory. This model allows for \ndirect access (DAX) using byte-addressable operations (i.e., load/store). However, the \npersistence of the data in the cache is not guaranteed until it has entered the persistence \ndomain. The x86 architecture provides a set of instructions for flushing cache lines in \na more optimized way. In addition to existing x86 instructions, such as non-temporal \nstores, CLFLUSH, and WBINVD, two new instructions were added: CLFLUSHOPT and \nCLWB. Both new instructions must be followed by an SFENCE to ensure all flushes are \ncompleted before continuing. Flushing a cache line using CLWB, CLFLUSHOPT, or CLFLUSH \nand using non-temporal stores are all supported from user space. You can find details \nfor each machine instruction in the software developer manuals for the architecture. \nOn Intel platforms, for example, this information can be found in the Intel 64 and 32 \nArchitectures Software Developer Manuals (https://software.intel.com/en-us/\narticles/intel-sdm).\nNon-temporal stores imply that the data being written is not going to be read again \nsoon, so we bypass the CPU caches. That is, there is no temporal locality, so there is no \nbenefit to keeping the data in the processor’s cache(s), and there may be a penalty if the \nstored data displaces other useful data from the cache(s).\nFlushing to persistent memory directly from user space negates calling into the \nkernel, which makes it highly efficient. The feature is documented in the SNIA persistent \nmemory programming model specification as an optimized flush. The specification \nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "166.11 sec"
            },
            {
              "page_number": 49,
              "text": "20\ndocument1 describes optimized flush as optionally supported by the platform, \ndepending on the hardware and operating system support. Despite the CPU support, \nit is essential for applications to use only optimized flushes when the operating system \nindicates that it is safe to use. The operating system may require the control point \nprovided by calls like msync() when, for example, there are changes to file system \nmetadata that need to be written as part of the msync() operation.\nTo better understand instruction ordering, consider a very simple linked list \nexample. Our pseudocode described in the following has three simple steps to add a new \nnode into an existing list that already contains two nodes. These steps are depicted in \nFigure 2-3.\n\t 1.\t Create the new node (Node 2).\n\t 2.\t Update the node pointer (next pointer) to point to the last node in \nthe list (Node 2 → Node 1).\n\t 3.\t Update the head pointer to point at the new node (Head → Node 2).\nFigure 2-3 (Step 3) shows that the head pointer was updated in the CPU cached version, \nbut the Node 2 to Node 1 pointer has not yet been updated in persistent memory. This \nis because the hardware can choose which cache lines to commit and the order may not \nmatch the source code flow. If the system or application were to crash at this point, the \npersistent memory state would be inconsistent, and the data structure would no longer \nbe usable.\n1\u0007SNIA NVM programming model spec: https://www.snia.org/tech_activities/standards/\ncurr_standards/npm\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "169.79 sec"
            },
            {
              "page_number": 50,
              "text": "21\nTo solve this problem, we introduce a memory store barrier to ensure the order of the \nwrite operations is maintained. Starting from the same initial state, the pseudocode now \nlooks like this:\n\t 1.\t Create the new node.\n\t 2.\t Update the node pointer (next pointer) to point to the last node in \nthe list, and perform a store barrier/fence operation.\n\t 3.\t Update the head pointer to point at the new node.\nFigure 2-4 shows that the addition of the store barrier allows the code to work as \nexpected and maintains a consistent data structure in the volatile CPU caches and on \nFigure 2-3.  Adding a new node to an existing linked list without a store barrier\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page50_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page50_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "172.66 sec"
            },
            {
              "page_number": 51,
              "text": "22\npersistent memory. We can see in Step 3 that the store barrier/fence operation waited \nfor the pointer from Node 2 to Node 1 to update before updating the head pointer. The \nupdates in the CPU cache matches the persistent memory version, so it now globally \nvisible. This is a simplistic approach to solving the problem because store barriers do not \nprovide atomicity or data integrity. A complete solution should also use transactions to \nensure the data is atomically updated. \nThe PMDK detects the platform, CPU, and persistent memory features when the \nmemory pool is opened and then uses the optimal instructions and fencing to preserve \nwrite ordering. (Memory pools are files that are memory mapped into the process \naddress space; later chapters describe them in more detail.)\nFigure 2-4.  Adding a new node to an existing linked list using a store barrier\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page51_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page51_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "175.73 sec"
            },
            {
              "page_number": 52,
              "text": "23\nTo insulate application developers from the complexities of the hardware and to keep \nthem from having to research and implement code specific to each platform or device, \nthe libpmem library provides a function that tells the application when optimized flush is \nsafe to use or fall back to the standard way of flushing stores to memory-mapped files.\nTo simplify programming, we encourage developers to use libraries, such as libpmem \nand others within the PMDK. The libpmem library is also designed to detect the case of \nthe platform with a battery that automatically converts flush calls into simple SFENCE \ninstructions. Chapter 5 introduces and describes the core libraries within the PMDK in \nmore detail, and later chapters take an in-depth look into each of the libraries to help \nyou understand their APIs and features.\n\u0007Data Visibility\nWhen data is visible to other processes or threads, and when it is safe in the persistence \ndomain, is critical to understand when using persistent memory in applications. In the \nFigure 2-2 and 2-3 examples, updates made to data in the CPU caches could become \nvisible to other processes or threads. Visibility and persistence are often not the same \nthing, and changes made to persistent memory are often visible to other running threads \nin the system before they are persistent. Visibility works the same way as it does for \nnormal DRAM, described by the memory model ordering and visibility rules for a given \nplatform (for example, see the Intel Software Development Manual for the visibility rules \nfor Intel platforms). Persistence of changes is achieved in one of three ways: either by \ncalling the standard storage API for persistence (msync on Linux or FlushFileBuffers \non Windows), by using optimized flush when supported, or by achieving visibility on \na platform where the CPU caches are considered persistent. This is one reason we use \nflushing and fencing operations.\nA pseudo C code example may look like this:\nopen()   // Open a file on a file system\n...\nmmap()   // Memory map the file\n...\nstrcpy() // Execute a store operation\n...      // Data is globally visible\nmsync()  // Data is now persistent \nDeveloping for persistent memory follows this decades-old model. \nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "181.57 sec"
            },
            {
              "page_number": 53,
              "text": "24\n\u0007Intel Machine Instructions for Persistent Memory\nApplicable to Intel- and AMD-based ADR platforms, executing an Intel 64 and 32 architecture \nstore instruction is not enough to make data persistent since the data may be sitting in the \nCPU caches indefinitely and could be lost by a power failure. Additional cache flush actions \nare required to make the stores persistent. Importantly, these non-­privileged cache flush \noperations can be called from user space, meaning applications decide when and where to \nfence and flush data. Table 2-1 summarizes each of these instructions. For more detailed \ninformation, the Intel 64 and 32 Architectures Software Developer Manuals are online at \nhttps://software.intel.com/en-us/articles/intel-sdm.\nDevelopers should primarily focus on CLWB and Non-Temporal Stores if available \nand fall back to the others as necessary. Table 2-1 lists other opcodes for completeness.\nTable 2-1.  Intel architecture instructions for persistent memory\nOPCODE\nDescription\nCLFLUSH\nThis instruction, supported in many generations of CPU, flushes a single \ncache line. Historically, this instruction is serialized, causing multiple CLFLUSH \ninstructions to execute one after the other, without any concurrency.\nCLFLUSHOPT \n(followed by an \nSFENCE)\nThis instruction, newly introduced for persistent memory support, is like \nCLFLUSH but without the serialization. To flush a range, the software executes a \nCLFLUSHOPT instruction for each 64-byte cache line in the range, followed by a \nsingle SFENCE instruction to ensure the flushes are complete before continuing. \nCLFLUSHOPT is optimized, hence the name, to allow some concurrency when \nexecuting multiple CLFLUSHOPT instructions back-to-back.\nCLWB (followed by \nan SFENCE)\nThe effect of cache line writeback (CLWB) is the same as CLFLUSHOPT except \nthat the cache line may remain valid in the cache but is no longer dirty since it \nwas flushed. This makes it more likely to get a cache hit on this line if the data \nis accessed again later.\nNon-temporal \nstores (followed \nby an SFENCE)\nThis feature has existed for a while in x86 CPUs. These stores are “write \ncombining” and bypass the CPU cache; using them does not require a flush. A \nfinal SFENCE instruction is still required to ensure the stores have reached the \npersistence domain.\n(continued)\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "189.15 sec"
            },
            {
              "page_number": 54,
              "text": "25\n\u0007Detecting Platform Capabilities\nServer platform, CPU, and persistent memory features and capabilities are exposed to \nthe operating system through the BIOS and ACPI that can be queried by applications. \nApplications should not assume they are running on hardware with all the optimizations \navailable. Even if the physical hardware supports it, virtualization technologies may or \nmay not expose those features to the guests, or your operating system may or may not \nimplement them. As such, we encourage developers to use libraries, such as those in the \nPMDK, that perform the required feature checks or implement the checks within the \napplication code base.\nOPCODE\nDescription\nSFENCE\nPerforms a serializing operation on all store-to-memory instructions that were \nissued prior to the SFENCE instruction. This serializing operation guarantees \nthat every store instruction that precedes in program order the SFENCE \ninstruction is globally visible before any store instruction that follows the \nSFENCE instruction can be globally visible. The SFENCE instruction is ordered \nwith respect to store instructions, other SFENCE instructions, any MFENCE \ninstructions, and any serializing instructions (such as the CPUID instruction). It is \nnot ordered with respect to load instructions or the LFENCE instruction.\nWBINVD\nThis kernel-mode-only instruction flushes and invalidates every cache line on \nthe CPU that executes it. After executing this on all CPUs, all stores to persistent \nmemory are certainly in the persistence domain, but all cache lines are empty, \nimpacting performance. Also, the overhead of sending a message to each CPU \nto execute this instruction can be significant. Because of this, WBINVD is only \nexpected to be used by the kernel for flushing very large ranges (at least many \nmegabytes).\nTable 2-1.  (continued)\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "192.83 sec"
            },
            {
              "page_number": 55,
              "text": "26\nFigure 2-5 shows the flow implemented by libpmem, which initially verifies the \nmemory-mapped file (called a memory pool), resides on a file system that has the DAX \nfeature enabled, and is backed by physical persistent memory. Chapter 3 describes DAX \nin more detail.\nOn Linux, direct access is achieved by mounting an XFS or ext4 file system with \nthe \"-o dax\" option. On Microsoft Windows, NTFS enables DAX when the volume \nis created and formatted using the DAX option. If the file system is not DAX-enabled, \napplications should fall back to the legacy approach of using msync(), fsync(), or \nFlushFileBuffers(). If the file system is DAX-enabled, the next check is to determine \nwhether the platform supports ADR or eADR by verifying whether or not the CPU caches \nare considered persistent. On an eADR platform where CPU caches are considered \npersistent, no further action is required. Any data written will be considered persistent, \nand thus there is no requirement to perform any flushes, which is a significant \nperformance optimization. On an ADR platform, the next sequence of events identifies \nthe most optimal flush operation based on Intel machine instructions previously \ndescribed.\nFigure 2-5.  Flowchart showing how applications can detect platform features\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page55_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page55_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "196.52 sec"
            },
            {
              "page_number": 56,
              "text": "27\n\u0007Application Startup and Recovery\nIn addition to detecting platform features, applications should verify whether the \nplatform was previously stopped and restarted gracefully or ungracefully. Figure 2-6 \nshows the checks performed by the Persistent Memory Development Kit. \nSome persistent memory devices, such as Intel Optane DC persistent memory, \nprovide SMART counters that can be queried to check the health and status. Several \nlibraries such as libpmemobj query the BIOS, ACPI, OS, and persistent memory module \ninformation then perform the necessary validation steps to decide which flush operation \nis most optimal to use.\nWe described earlier that if a system loses power, there should be enough stored \nenergy within the power supplies and platform to successfully flush the contents of \nthe memory controller’s WPQ and the write buffers on the persistent memory devices. \nData will be considered consistent upon successful completion. If this process fails, \ndue to exhausting all the stored energy before all the data was successfully flushed, the \npersistent memory modules will report a dirty shutdown. A dirty shutdown indicates that \ndata on the device may be inconsistent. This may or may not result in needing to restore \nthe data from backups. You can find more information on this process – and what errors \nand signals are sent – in the RAS (reliability, availability, serviceability) documentation \nfor your platform and the persistent memory device. Chapter 17 also discusses this \nfurther.\nAssuming no dirty shutdown is indicated, the application should check to see if \nthe persistent memory media is reporting any known poison blocks (see Figure 2-6). \nPoisoned blocks are areas on the physical media that are known to be bad.\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "201.33 sec"
            },
            {
              "page_number": 57,
              "text": "28\nIf an application were not to check these things at startup, due to the persistent \nnature of the media, it could get stuck in an infinite loop, for example:\n\t 1.\t Application starts.\n\t 2.\t Reads a memory address.\n\t 3.\t Encounters poison.\n\t 4.\t Crashes or system crashes and reboots.\n\t 5.\t Starts and resumes operation from where it left off.\n\t 6.\t Performs a read on the same memory address that triggered the \nprevious restart.\n\t 7.\t Application or system crashes.\n\t 8.\t …\n\t 9.\t Repeats infinitely until manual intervention.\nThe ACPI specification defines an Address Range Scrub (ARS) operation that the \noperating system implements. This allows the operating system to perform a runtime \nbackground scan operation across the memory address range of the persistent memory. \nFigure 2-6.  Application startup and recovery flow\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page57_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page57_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "203.38 sec"
            },
            {
              "page_number": 58,
              "text": "29\nSystem administrators may manually initiate an ARS. The intent is to identify bad \nor potentially bad memory regions before the application does. If ARS identifies an \nissue, the hardware can provide a status notification to the operating system and the \napplication that can be consumed and handled gracefully. If the bad address range \ncontains data, some method to reconstruct or restore the data needs to be implemented. \nChapter 17 describes ARS in more detail.\nDevelopers are free to implement these features directly within the application code. \nHowever, the libraries in the PMDK handle these complex conditions, and they will be \nmaintained for each product generation while maintaining stable APIs. This gives you \na future-proof option without needing to understand the intricacies of each CPU or \npersistent memory product.\n\u0007What’s Next?\nChapter 3 continues to provide foundational information from the perspective of the \nkernel and user spaces. We describe how operating systems such as Linux and Windows \nhave adopted and implemented the SNIA non-volatile programming model that defines \nrecommended behavior between various user space and operating system kernel \ncomponents supporting persistent memory. Later chapters build on the foundations \nprovided in Chapters 1 through 3.\n\u0007Summary\nThis chapter defines persistent memory and its characteristics, recaps how CPU caches \nwork, and describes why it is crucial for applications directly accessing persistent \nmemory to assume responsibility for flushing CPU caches. We focus primarily on \nhardware implementations. User libraries, such as those delivered with the PMDK, \nassume the responsibilities for architecture and hardware-specific operations and allow \ndevelopers to use simple APIs to implement them. Later chapters describe the PMDK \nlibraries in more detail and show how to use them in your application.\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "209.11 sec"
            },
            {
              "page_number": 59,
              "text": "30\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 2  Persistent Memory Architecture",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "210.65 sec"
            },
            {
              "page_number": 60,
              "text": "31\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_3\nCHAPTER 3\nOperating System Support \nfor Persistent Memory\nThis chapter describes how operating systems manage persistent memory as a platform \nresource and describes the options they provide for applications to use persistent \nmemory. We first compare memory and storage in popular computer architectures and \nthen describe how operating systems have been extended for persistent memory.\n\u0007Operating System Support for Memory and Storage\nFigure 3-1 shows a simplified view of how operating systems manage storage and volatile \nmemory. As shown, the volatile main memory is attached directly to the CPU through a \nmemory bus. The operating system manages the mapping of memory regions directly \ninto the application’s visible memory address space. Storage, which usually operates at \nspeeds much slower than the CPU, is attached through an I/O controller. The operating \nsystem handles access to the storage through device driver modules loaded into the \noperating system’s I/O subsystem.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "214.23 sec"
            },
            {
              "page_number": 61,
              "text": "32\nThe combination of direct application access to volatile memory combined with the \noperating system I/O access to storage devices supports the most common application \nprogramming model taught in introductory programming classes. In this model, \ndevelopers allocate data structures and operate on them at byte granularity in memory. \nWhen the application wants to save data, it uses standard file API system calls to write \nthe data to an open file. Within the operating system, the file system executes this write \nby performing one or more I/O operations to the storage device. Because these I/O \noperations are usually much slower than CPU speeds, the operating system typically \nsuspends the application until the I/O completes.\nSince persistent memory can be accessed directly by applications and can persist \ndata in place, it allows operating systems to support a new programming model that \ncombines the performance of memory while persisting data like a non-volatile storage \ndevice. Fortunately for developers, while the first generation of persistent memory \nwas under development, Microsoft Windows and Linux designers, architects and \nFigure 3-1.  Storage and volatile memory in the operating system\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page61_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page61_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "217.20 sec"
            },
            {
              "page_number": 62,
              "text": "33\ndevelopers collaborated in the Storage and Networking Industry Association (SNIA) to \ndefine a common programming model, so the methods for using persistent memory \ndescribed in this chapter are available in both operating systems. More details can be \nfound in the SNIA NVM programming model specification ­(https://www.snia.org/\ntech_activities/standards/curr_standards/npm).\n\u0007Persistent Memory As Block Storage\nThe first operating system extension for persistent memory is the ability to detect the \nexistence of persistent memory modules and load a device driver into the operating \nsystem’s I/O subsystem as shown in Figure 3-2. This NVDIMM driver serves two \nimportant functions. First, it provides an interface for management and system \nadministrator utilities to configure and monitor the state of the persistent memory \nhardware. Second, it functions similarly to the storage device drivers.\nThe NVDIMM driver presents persistent memory to applications and operating \nsystem modules as a fast block storage device. This means applications, file systems, \nvolume managers, and other storage middleware layers can use persistent memory the \nsame way they use storage today, without modifications.\nFigure 3-2.  Persistent memory as block storage\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page62_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page62_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "221.30 sec"
            },
            {
              "page_number": 63,
              "text": "34\nFigure 3-2 also shows the Block Translation Table (BTT) driver, which can be \noptionally configured into the I/O subsystem. Storage devices such as HDDs and SSDs \npresent a native block size with 512k and 4k bytes as two common native block sizes. \nSome storage devices, especially NVM Express SSDs, provide a guarantee that when a \npower failure or server failure occurs while a block write is in-flight, either all or none \nof the block will be written. The BTT driver provides the same guarantee when using \npersistent memory as a block storage device. Most applications and file systems depend \non this atomic write guarantee and should be configured to use the BTT driver, although \noperating systems also provide the option to bypass the BTT driver for applications that \nimplement their own protection against partial block updates.\n\u0007Persistent Memory-Aware File Systems\nThe next extension to the operating system is to make the file system aware of and be \noptimized for persistent memory. File systems that have been extended for persistent \nmemory include Linux ext4 and XFS, and Microsoft Windows NTFS. As shown in \nFigure 3-3, these file systems can either use the block driver in the I/O subsystem (as \ndescribed in the previous section) or bypass the I/O subsystem to directly use persistent \nmemory as byte-addressable load/store memory as the fastest and shortest path to data \nstored in persistent memory. In addition to eliminating the I/O operation, this path \nenables small data writes to be executed faster than traditional block storage devices that \nrequire the file system to read the device’s native block size, modify the block, and then \nwrite the full block back to the device.\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "225.90 sec"
            },
            {
              "page_number": 64,
              "text": "35\nThese persistent memory-aware file systems continue to present the familiar, \nstandard file APIs to applications including the open, close, read, and write system \ncalls. This allows applications to continue using the familiar file APIs while benefiting \nfrom the higher performance of persistent memory.\n\u0007Memory-Mapped Files\nBefore describing the next operating system option for using persistent memory, \nthis section reviews memory-mapped files in Linux and Windows. When memory \nmapping a file, the operating system adds a range to the application’s virtual \naddress space which corresponds to a range of the file, paging file data into physical \nmemory as required. This allows an application to access and modify file data as \nbyte-addressable in-memory data structures. This has the potential to improve \nperformance and simplify application development, especially for applications that \nmake frequent, small updates to file data.\nApplications memory map a file by first opening the file, then passing the resulting \nfile handle as a parameter to the mmap() system call in Linux or to MapViewOfFile() in \nWindows. Both return a pointer to the in-memory copy of a portion of the file. Listing 3-1  \nshows an example of Linux C code that memory maps a file, writes data into the file \nby accessing it like memory, and then uses the msync system call to perform the I/O \nFigure 3-3.  Persistent memory-aware file system\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page64_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page64_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "230.00 sec"
            },
            {
              "page_number": 65,
              "text": "36\noperation to write the modified data to the file on the storage device. Listing 3-2 shows \nthe equivalent operations on Windows. We walk through and highlight the key steps in \nboth code samples.\nListing 3-1.  mmap_example.c – Memory-mapped file on Linux example\n    50  #include <err.h>\n    51  #include <fcntl.h>\n    52  #include <stdio.h>\n    53  #include <stdlib.h>\n    54  #include <string.h>\n    55  #include <sys/mman.h>\n    56  #include <sys/stat.h>\n    57  #include <sys/types.h>\n    58  #include <unistd.h>\n    59\n    60  int\n    61  main(int argc, char *argv[])\n    62  {\n    63      int fd;\n    64      struct stat stbuf;\n    65      char *pmaddr;\n    66\n    67      if (argc != 2) {\n    68          fprintf(stderr, \"Usage: %s filename\\n\",\n    69              argv[0]);\n    70          exit(1);\n    71      }\n    72\n    73      if ((fd = open(argv[1], O_RDWR)) < 0)\n    74          err(1, \"open %s\", argv[1]);\n    75\n    76      if (fstat(fd, &stbuf) < 0)\n    77          err(1, \"stat %s\", argv[1]);\n    78\n    79      /*\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "233.59 sec"
            },
            {
              "page_number": 66,
              "text": "37\n    80       * Map the file into our address space for read\n    81       * & write. Use MAP_SHARED so stores are visible\n    82       * to other programs.\n    83       */\n    84      if ((pmaddr = mmap(NULL, stbuf.st_size,\n    85                  PROT_READ|PROT_WRITE,\n    86                  MAP_SHARED, fd, 0)) == MAP_FAILED)\n    87          err(1, \"mmap %s\", argv[1]);\n    88\n    89      /* Don't need the fd anymore because the mapping\n    90       * stays around */\n    91      close(fd);\n    92\n    93      /* store a string to the Persistent Memory */\n    94      strcpy(pmaddr, \"This is new data written to the\n    95              file\");\n    96\n    97      /*\n    98       * Simplest way to flush is to call msync().\n    99       * The length needs to be rounded up to a 4k page.\n   100       */\n   101      if (msync((void *)pmaddr, 4096, MS_SYNC) < 0)\n   102          err(1, \"msync\");\n   103\n   104      printf(\"Done.\\n\");\n   105      exit(0);\n   106  }\n•\t\nLines 67-74: We verify the caller passed a file name that can be \nopened. The open call will create the file if it does not already exist.\n•\t\nLine 76: We retrieve the file statistics to use the length when we \nmemory map the file.\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "238.30 sec"
            },
            {
              "page_number": 67,
              "text": "38\n•\t\nLine 84: We map the file into the application’s address space to allow \nour program to access the contents as if in memory. In the second \nparameter, we pass the length of the file, requesting Linux to initialize \nmemory with the full file. We also map the file with both READ and \nWRITE access and also as SHARED allowing other processes to map \nthe same file.\n•\t\nLine 91: We retire the file descriptor which is no longer needed once \na file is mapped.\n•\t\nLine 94: We write data into the file by accessing it like memory \nthrough the pointer returned by mmap.\n•\t\nLine 101: We explicitly flush the newly written string to the backing \nstorage device.\nListing 3-2 shows an example of C code that memory maps a file, writes data into  \nthe file, and then uses the FlushViewOfFile() and FlushFileBuffers() system calls to \nflush the modified data to the file on the storage device.\nListing 3-2.  Memory-mapped file on Windows example\n    45  #include <fcntl.h>\n    46  #include <stdio.h>\n    47  #include <stdlib.h>\n    48  #include <string.h>\n    49  #include <sys/stat.h>\n    50  #include <sys/types.h>\n    51  #include <Windows.h>\n    52\n    53  int\n    54  main(int argc, char *argv[])\n    55  {\n    56      if (argc != 2) {\n    57          fprintf(stderr, \"Usage: %s filename\\n\",\n    58              argv[0]);\n    59          exit(1);\n    60      }\n    61\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "242.95 sec"
            },
            {
              "page_number": 68,
              "text": "39\n    62      /* Create the file or open if the file exists */\n    63      HANDLE fh = CreateFile(argv[1],\n    64          GENERIC_READ|GENERIC_WRITE,\n    65          0,\n    66          NULL,\n    67          OPEN_EXISTING,\n    68          FILE_ATTRIBUTE_NORMAL,\n    69          NULL);\n    70\n    71      if (fh == INVALID_HANDLE_VALUE) {\n    72          fprintf(stderr, \"CreateFile, gle: 0x%08x\",\n    73              GetLastError());\n    74          exit(1);\n    75      }\n    76\n    77      /*\n    78       * Get the file length for use when\n    79       * memory mapping later\n    80       * */\n    81      DWORD filelen = GetFileSize(fh, NULL);\n    82      if (filelen == 0) {\n    83          fprintf(stderr, \"GetFileSize, gle: 0x%08x\",\n    84              GetLastError());\n    85          exit(1);\n    86      }\n    87\n    88      /* Create a file mapping object */\n    89      HANDLE fmh = CreateFileMapping(fh,\n    90          NULL, /* security attributes */\n    91          PAGE_READWRITE,\n    92          0,\n    93          0,\n    94          NULL);\n    95\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "247.61 sec"
            },
            {
              "page_number": 69,
              "text": "40\n    96      if (fmh == NULL) {\n    97          fprintf(stderr, \"CreateFileMapping,\n    98              gle: 0x%08x\", GetLastError());\n    99          exit(1);\n   100      }\n   101\n   102      /*\n   103       * Map into our address space and get a pointer\n   104       * to the beginning\n   105       * */\n   106      char *pmaddr = (char *)MapViewOfFileEx(fmh,\n   107          FILE_MAP_ALL_ACCESS,\n   108          0,\n   109          0,\n   110          filelen,\n   111          NULL); /* hint address */\n   112\n   113      if (pmaddr == NULL) {\n   114          fprintf(stderr, \"MapViewOfFileEx,\n   115              gle: 0x%08x\", GetLastError());\n   116          exit(1);\n   117      }\n   118\n   119      /*\n   120       * On windows must leave the file handle(s)\n   121       * open while mmaped\n   122       * */\n   123\n   124      /* Store a string to the beginning of the file  */\n   125      strcpy(pmaddr, \"This is new data written to\n   126          the file\");\n   127\n   128      /*\n   129       * Flush this page with length rounded up to 4K\n   130       * page size\n   131       * */\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "252.23 sec"
            },
            {
              "page_number": 70,
              "text": "41\n   132      if (FlushViewOfFile(pmaddr, 4096) == FALSE) {\n   133          fprintf(stderr, \"FlushViewOfFile,\n   134              gle: 0x%08x\", GetLastError());\n   135          exit(1);\n   136      }\n   137\n   138      /* Flush the complete file to backing storage */\n   139      if (FlushFileBuffers(fh) == FALSE) {\n   140          fprintf(stderr, \"FlushFileBuffers,\n   141              gle: 0x%08x\", GetLastError());\n   142          exit(1);\n   143      }\n   144\n   145      /* Explicitly unmap before closing the file */\n   146      if (UnmapViewOfFile(pmaddr) == FALSE) {\n   147          fprintf(stderr, \"UnmapViewOfFile,\n   148              gle: 0x%08x\", GetLastError());\n   149          exit(1);\n   150      }\n   151\n   152      CloseHandle(fmh);\n   153      CloseHandle(fh);\n   154\n   155      printf(\"Done.\\n\");\n   156      exit(0);\n   157  }\n•\t\nLines 45-75: As in the previous Linux example, we take the file name \npassed through argv and open the file.\n•\t\nLine 81: We retrieve the file size to use later when memory mapping.\n•\t\nLine 89: We take the first step to memory mapping a file by creating \nthe file mapping. This step does not yet map the file into our \napplication’s memory space.\n•\t\nLine 106: This step maps the file into our memory space.\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "256.84 sec"
            },
            {
              "page_number": 71,
              "text": "42\n•\t\nLine 125: As in the previous Linux example, we write a string to the \nbeginning of the file, accessing the file like memory.\n•\t\nLine 132: We flush the modified memory page to the backing storage.\n•\t\nLine 139: We flush the full file to backing storage, including any \nadditional file metadata maintained by Windows.\n•\t\nLine 146-157: We unmap the file, close the file, then exit the program.\nFigure 3-4 shows what happens inside the operating system when an application \ncalls mmap() on Linux or CreateFileMapping() on Windows. The operating system \nallocates memory from its memory page cache, maps that memory into the application’s \naddress space, and creates the association with the file through a storage device driver.\nAs the application reads pages of the file in memory, and if those pages are not \npresent in memory, a page fault exception is raised to the operating system which will \nthen read that page into main memory through storage I/O operations. The operating \nFigure 3-4.  Memory-mapped files with storage\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page71_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page71_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "261.03 sec"
            },
            {
              "page_number": 72,
              "text": "43\nsystem also tracks writes to those memory pages and schedules asynchronous I/O \noperations to write the modifications back to the primary copy of the file on the storage \ndevice. Alternatively, if the application wants to ensure updates are written back to \nstorage before continuing as we did in our code example, the msync system call on \nLinux or FlushViewOfFile on Windows executes the flush to disk. This may cause the \noperating system to suspend the program until the write finishes, similar to the file-write \noperation described earlier.\nThis description of memory-mapped files using storage highlights some of the \ndisadvantages. First, a portion of the limited kernel memory page cache in main \nmemory is used to store a copy of the file. Second, for files that cannot fit in memory, the \napplication may experience unpredictable and variable pauses as the operating system \nmoves pages between memory and storage through I/O operations. Third, updates to \nthe in-memory copy are not persistent until written back to storage so can be lost in the \nevent of a failure.\n\u0007Persistent Memory Direct Access (DAX)\nThe persistent memory direct access feature in operating systems, referred to as DAX in \nLinux and Windows, uses the memory-mapped file interfaces described in the previous \nsection but takes advantage of persistent memory’s native ability to both store data \nand to be used as memory. Persistent memory can be natively mapped as application \nmemory, eliminating the need for the operating system to cache files in volatile main \nmemory.\nTo use DAX, the system administrator creates a file system on the persistent memory \nmodule and mounts that file system into the operating system’s file system tree. For \nLinux users, persistent memory devices will appear as /dev/pmem* device special files. To \nshow the persistent memory physical devices, system administrators can use the ndctl \nand ipmctl utilities shown in Listings 3-3 and 3-4.\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "266.73 sec"
            },
            {
              "page_number": 73,
              "text": "44\nListing 3-3.  Displaying persistent memory physical devices and regions on Linux\n# ipmctl show -dimm\n DimmID | Capacity  | HealthState | ActionRequired | LockState | FWVersion\n==============================================================================\n 0x0001 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n 0x0011 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n 0x0021 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n 0x0101 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n 0x0111 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n 0x0121 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n 0x1001 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n 0x1011 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n 0x1021 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n 0x1101 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n 0x1111 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n 0x1121 | 252.4 GiB | Healthy     | 0              | Disabled  | 01.02.00.5367\n# ipmctl show -region\nSocketID | ISetID             | PersistentMemoryType | Capacity   | FreeCapacity | HealthState\n===========================================================================================\n0x0000  | 0x2d3c7f48f4e22ccc | AppDirect            | 1512.0 GiB | 0.0 GiB      | Healthy\n0x0001  | 0xdd387f488ce42ccc | AppDirect            | 1512.0 GiB | 1512.0 GiB   | Healthy\nListing 3-4.  Displaying persistent memory physical devices, regions, and \nnamespaces on Linux\n# ndctl list -DRN\n{\n  \"dimms\":[\n    {\n      \"dev\":\"nmem1\",\n      \"id\":\"8089-a2-1837-00000bb3\",\n      \"handle\":17,\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "271.48 sec"
            },
            {
              "page_number": 74,
              "text": "45\n      \"phys_id\":44,\n      \"security\":\"disabled\"\n    },\n    {\n      \"dev\":\"nmem3\",\n      \"id\":\"8089-a2-1837-00000b5e\",\n      \"handle\":257,\n      \"phys_id\":54,\n      \"security\":\"disabled\"\n    },\n    [...snip...]\n    {\n      \"dev\":\"nmem8\",\n      \"id\":\"8089-a2-1837-00001114\",\n      \"handle\":4129,\n      \"phys_id\":76,\n      \"security\":\"disabled\"\n    }\n  ],\n  \"regions\":[\n    {\n      \"dev\":\"region1\",\n      \"size\":1623497637888,\n      \"available_size\":1623497637888,\n      \"max_available_extent\":1623497637888,\n      \"type\":\"pmem\",\n      \"iset_id\":-2506113243053544244,\n      \"mappings\":[\n        {\n          \"dimm\":\"nmem11\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":5\n        },\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "277.11 sec"
            },
            {
              "page_number": 75,
              "text": "46\n        {\n          \"dimm\":\"nmem10\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":1\n        },\n        {\n          \"dimm\":\"nmem9\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":3\n        },\n        {\n          \"dimm\":\"nmem8\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":2\n        },\n        {\n          \"dimm\":\"nmem7\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":4\n        },\n        {\n          \"dimm\":\"nmem6\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":0\n        }\n      ],\n      \"persistence_domain\":\"memory_controller\"\n    },\n    {\n      \"dev\":\"region0\",\n      \"size\":1623497637888,\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "280.69 sec"
            },
            {
              "page_number": 76,
              "text": "47\n      \"available_size\":0,\n      \"max_available_extent\":0,\n      \"type\":\"pmem\",\n      \"iset_id\":3259620181632232652,\n      \"mappings\":[\n        {\n          \"dimm\":\"nmem5\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":5\n        },\n        {\n          \"dimm\":\"nmem4\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":1\n        },\n        {\n          \"dimm\":\"nmem3\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":3\n        },\n        {\n          \"dimm\":\"nmem2\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":2\n        },\n        {\n          \"dimm\":\"nmem1\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":4\n        },\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "284.38 sec"
            },
            {
              "page_number": 77,
              "text": "48\n        {\n          \"dimm\":\"nmem0\",\n          \"offset\":268435456,\n          \"length\":270582939648,\n          \"position\":0\n        }\n      ],\n      \"persistence_domain\":\"memory_controller\",\n      \"namespaces\":[\n        {\n          \"dev\":\"namespace0.0\",\n          \"mode\":\"fsdax\",\n          \"map\":\"dev\",\n          \"size\":1598128390144,\n          \"uuid\":\"06b8536d-4713-487d-891d-795956d94cc9\",\n          \"sector_size\":512,\n          \"align\":2097152,\n          \"blockdev\":\"pmem0\"\n        }\n      ]\n    }\n  ]\n}\nWhen a file system is created and mounted using /dev/pmem* devices, they can be \nidentified using the df command as shown in Listing 3-5.\nListing 3-5.  Locating persistent memory on Linux.\n$ df -h /dev/pmem*\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/pmem0      1.5T   77M  1.4T   1% /mnt/pmemfs0\n/dev/pmem1      1.5T   77M  1.4T   1% /mnt/pmemfs1\nWindows developers will use PowerShellCmdlets as shown in Listing 3-6. In either \ncase, assuming the administrator has granted you rights to create files, you can create \none or more files in the persistent memory and then memory map those files to your \napplication using the same method shown in code Listings 3-1 and 3-2.\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "288.97 sec"
            },
            {
              "page_number": 78,
              "text": "49\nListing 3-6.  Locating persistent memory on Windows\nPS C:\\Users\\Administrator> Get-PmemDisk\nNumber Size   Health  Atomicity Removable Physical device IDs Unsafe shutdowns\n------ ----   ------  --------- --------- ------------------- ----------------\n2      249 GB Healthy None      True      {1}                 36\nPS C:\\Users\\Administrator> Get-Disk 2 | Get-Partition\nPartitionNumber  DriveLetter Offset   Size         Type\n---------------  ----------- ------   ----         ----\n1                            24576    15.98 MB     Reserved\n2                D           16777216 248.98 GB    Basic\nManaging persistent memory as files has several benefits:\n•\t\nYou can leverage the rich features of leading file systems for \norganizing, managing, naming, and limiting access for user’s \npersistent memory files and directories.\n•\t\nYou can apply the familiar file system permissions and access rights \nmanagement for protecting data stored in persistent memory and for \nsharing persistent memory between multiple users.\n•\t\nSystem administrators can use existing backup tools that rely on file \nsystem revision-history tracking.\n•\t\nYou can build on existing memory mapping APIs as described earlier \nand applications that currently use memory-mapped files and can \nuse direct persistent memory without modifications.\nOnce a file backed by persistent memory is created and opened, an application still \ncalls mmap() or MapViewOfFile() to get a pointer to the persistent media. The difference, \nshown in Figure 3-5, is that the persistent memory-aware file system recognizes that \nthe file is on persistent memory and programs the memory management unit (MMU) \nin the CPU to map the persistent memory directly into the application’s address space. \nNeither a copy in kernel memory nor synchronizing to storage through I/O operations \nis required. The application can use the pointer returned by mmap() or MapViewOfFile() \nto operate on its data in place directly in the persistent memory. Since no kernel I/O \nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "295.66 sec"
            },
            {
              "page_number": 79,
              "text": "50\noperations are required, and because the full file is mapped into the application’s \nmemory, it can manipulate large collections of data objects with higher and more \nconsistent performance as compared to files on I/O-accessed storage.\nListing 3-7 shows a C source code example that uses DAX to write a string directly \ninto persistent memory. This example uses one of the persistent memory API libraries \nincluded in Linux and Windows called libpmem. Although we discuss these libraries in \ndepth in later chapters, we describe the use of two of the functions available in libpmem \nin the following steps. The APIs in libpmem are common across Linux and Windows and \nabstract the differences between underlying operating system APIs, so this sample code \nis portable across both operating system platforms.\nFigure 3-5.  Direct access (DAX) I/O and standard file API I/O paths through the \nkernel\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page79_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page79_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "297.59 sec"
            },
            {
              "page_number": 80,
              "text": "51\nListing 3-7.  DAX programming example\n    32  #include <sys/types.h>\n    33  #include <sys/stat.h>\n    34  #include <fcntl.h>\n    35  #include <stdio.h>\n    36  #include <errno.h>\n    37  #include <stdlib.h>\n    38  #ifndef _WIN32\n    39  #include <unistd.h>\n    40  #else\n    41  #include <io.h>\n    42  #endif\n    43  #include <string.h>\n    44  #include <libpmem.h>\n    45\n    46  /* Using 4K of pmem for this example */\n    47  #define PMEM_LEN 4096\n    48\n    49  int\n    50  main(int argc, char *argv[])\n    51  {\n    52      char *pmemaddr;\n    53      size_t mapped_len;\n    54      int is_pmem;\n    55\n    56      if (argc != 2) {\n    57          fprintf(stderr, \"Usage: %s filename\\n\",\n    58              argv[0]);\n    59          exit(1);\n    60      }\n    61\n    62      /* Create a pmem file and memory map it. */\n    63      if ((pmemaddr = pmem_map_file(argv[1], PMEM_LEN,\n    64              PMEM_FILE_CREATE, 0666, &mapped_len,\n    65              &is_pmem)) == NULL) {\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "301.38 sec"
            },
            {
              "page_number": 81,
              "text": "52\n    66          perror(\"pmem_map_file\");\n    67          exit(1);\n    68      }\n    69\n    70      /* Store a string to the persistent memory. */\n    71      char s[] = \"This is new data written to the file\";\n    72      strcpy(pmemaddr, s);\n    73\n    74      /* Flush our string to persistence. */\n    75      if (is_pmem)\n    76          pmem_persist(pmemaddr, sizeof(s));\n    77      else\n    78          pmem_msync(pmemaddr, sizeof(s));\n    79\n    80      /* Delete the mappings. */\n    81      pmem_unmap(pmemaddr, mapped_len);\n    82\n    83      printf(\"Done.\\n\");\n    84      exit(0);\n    85  }\n•\t\nLines 38-42: We handle the differences between Linux and Windows \nfor the include files.\n•\t\nLine 44: We include the header file for the libpmem API used in this \nexample.\n•\t\nLines 56-60: We take the pathname argument from the command \nline argument.\n•\t\nLine 63-68: The pmem_map_file function in libpmem handles \nopening the file and mapping it into our address space on both \nWindows and Linux. Since the file resides on persistent memory, the \noperating system programs the hardware MMU in the CPU to map \nthe persistent memory region into our application’s virtual address \nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "312.01 sec"
            },
            {
              "page_number": 82,
              "text": "53\nspace. Pointer pmemaddr is set to the beginning of that region. The \npmem_map_file function can also be used for memory mapping disk-­\nbased files through kernel main memory as well as directly mapping \npersistent memory, so is_pmem is set to TRUE if the file resides on \npersistent memory and FALSE if mapped through main memory.\n•\t\nLine 72: We write a string into persistent memory.\n•\t\nLines 75-78: If the file resides on persistent memory, the pmem_\npersist function uses the user space machine instructions \n(described in Chapter 2) to ensure our string is flushed through \nCPU cache levels to the power-fail safe domain and ultimately to \npersistent memory. If our file resided on disk-based storage, Linux \nmmap or Windows FlushViewOfFile would be used to flushed to \nstorage. Note that we can pass small sizes here (the size of the string \nwritten is used in this example) instead of requiring flushes at page \ngranularity when using msync() or FlushViewOfFile().\n•\t\nLine 81: Finally, we unmap the persistent memory region.\n\u0007Summary\nFigure 3-6 shows the complete view of the operating system support that this chapter \ndescribes. As we discussed, an application can use persistent memory as a fast SSD, \nmore directly through a persistent memory-aware file system, or mapped directly into \nthe application’s memory space with the DAX option. DAX leverages operating system \nservices for memory-mapped files but takes advantage of the server hardware’s ability \nto map persistent memory directly into the application’s address space. This avoids the \nneed to move data between main memory and storage. The next few chapters describe \nconsiderations for working with data directly in persistent memory and then discuss the \nAPIs for simplifying development.\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "312.47 sec"
            },
            {
              "page_number": 83,
              "text": "54\nFigure 3-6.  Persistent memory programming interfaces\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 3  Operating System Support for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page83_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page83_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "314.69 sec"
            },
            {
              "page_number": 84,
              "text": "55\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_4\nCHAPTER 4\nFundamental Concepts \nof Persistent Memory \nProgramming\nIn Chapter 3, you saw how operating systems expose persistent memory to applications \nas memory-mapped files. This chapter builds on this fundamental model and examines \nthe programming challenges that arise. Understanding these challenges is an essential \npart of persistent memory programming, especially when designing a strategy for \nrecovery after application interruption due to issues like crashes and power failures. \nHowever, do not let these challenges deter you from persistent memory programming! \nChapter 5 describes how to leverage existing solutions to save you programming time \nand reduce complexity.\n\u0007What’s Different?\nApplication developers typically think in terms of memory-resident data structures and \nstorage-resident data structures. For data center applications, developers are careful to \nmaintain consistent data structures on storage, even in the face of a system crash. This \nproblem is commonly solved using logging techniques such as write-ahead logging, \nwhere changes are first written to a log and then flushed to persistent storage. If the data \nmodification process is interrupted, the application has enough information in the log \nto finish the operation on restart. Techniques like this have been around for many years; \nhowever, correct implementations are challenging to develop and time-consuming to \nmaintain. Developers often rely on a combination of databases, libraries, and modern \nfile systems to provide consistency. Even so, it is ultimately the application developer’s",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "319.30 sec"
            },
            {
              "page_number": 85,
              "text": "56\nresponsibility to design in a strategy to maintain consistent data structures on storage, \nboth at runtime and when recovering from application and system crashes.\nUnlike storage-resident data structures, application developers are concerned \nabout maintaining consistency of memory-resident data structures at runtime. When \nan application has multiple threads accessing the same data structure, techniques like \nlocking are used so that one thread can perform complex changes to a data structure \nwithout another thread seeing only part of the change. When an application exits or \ncrashes, or the system crashes, the memory contents are gone, so there is no need \nto maintain consistency of memory-resident data structures between runs of an \napplication like there is with storage-resident data structures.\nThese explanations may seem obvious, but these assumptions that the storage state \nstays around between runs and memory contents are volatile are so fundamental in \nthe way applications are developed that most developers don’t give it much thought. \nWhat’s different about persistent memory is, of course, that it is persistent, so all the \nconsiderations of both storage and memory apply. The application is responsible for \nmaintaining consistent data structures between runs and reboots, as well as the thread-­\nsafe locking used with memory-resident data structures.\nIf persistent memory has these attributes and requirements just like storage, why \nnot use code developed over the years for storage? This approach does work; using the \nstorage APIs on persistent memory is part of the programming model we described \nin Chapter 3. If the existing storage APIs on persistent memory are fast enough and \nmeet the application’s needs, then no further work is necessary. But to fully leverage \nthe advantages of persistent memory, where data structures are read and written in \nplace on persistence and accesses happen at the byte granularity, instead of using the \nblock storage stack, applications will want to memory map it and access it directly. This \neliminates the buffer-based storage APIs in the data path.\n\u0007Atomic Updates\nEach platform supporting persistent memory will have a set of native memory \noperations that are atomic. On Intel hardware, the atomic persistent store is 8 bytes. \nThus, if the program or system crashes while an aligned 8-byte store to persistent \nmemory is in-flight, on recovery those 8 bytes will either contain the old contents or \nthe new contents. The Intel processor has instructions that store more than 8 bytes, \nbut those are not failure atomic, so they can be torn by events like a power failure. \nChapter 4  Fundamental Concepts of Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "326.26 sec"
            },
            {
              "page_number": 86,
              "text": "57\nSometimes an update to a memory-resident data structure will require multiple \ninstructions, so naturally those changes can be torn by power failure as well since power \ncould be lost between any two instructions. Runtime locking prevents other threads from \nseeing a partially done change, but locking doesn’t provide any failure atomicity. When \nan application needs to make a change that is larger than 8 bytes to persistent memory, it \nmust construct the atomic operation by building on top of the basic atomics provided by \nhardware, such as the 8-byte failure atomicity provided by Intel hardware.\n\u0007Transactions\nCombining multiple operations into a single atomic operation is usually referred to as \na transaction. In the database world, the acronym ACID describes the properties of a \ntransaction: atomicity, consistency, isolation, and durability.\n\u0007Atomicity\nAs described earlier, atomicity is when multiple operations are composed into a single \natomic action that either happens entirely or does not happen at all, even in the face of \nsystem failure. For persistent memory, the most common techniques used are\n•\t\nRedo logging, where the full change is first written to a log, so during \nrecovery, it can be rolled forward if interrupted.\n•\t\nUndo logging, where information is logged that allows a partially \ndone change to be rolled back during recovery.\n•\t\nAtomic pointer updates, where a change is made active by updating \na single pointer atomically, usually changing it from pointing to old \ndata to new data.\nThe preceding list is not exhaustive, and it ignores the details that can get relatively \ncomplex. One common consideration is that transactions often include memory \nallocation/deallocation. For example, a transaction that adds a node to a tree data \nstructure usually includes the allocation of the new node. If the transaction is rolled back, \nthe memory must be freed to prevent a memory leak. Now imagine a transaction that \nperforms multiple persistent memory allocations and free operations, all of which must \nbe part of the same atomic operation. The implementation of this transaction is clearly \nmore complex than just writing the new value to a log or updating a single pointer.\nChapter 4  Fundamental Concepts of Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "330.97 sec"
            },
            {
              "page_number": 87,
              "text": "58\n\u0007Consistency\nConsistency means that a transaction can only move a data structure from one valid \nstate to another. For persistent memory, programmers usually find that the locking they \nuse to make updates thread-safe often indicates consistency points as well. If it is not \nvalid for a thread to see an intermediate state, locking prevents it from happening, and \nwhen it is safe to drop the lock, that is because it is safe for another thread to observe the \ncurrent state of the data structure.\n\u0007Isolation\nMultithreaded (concurrent) execution is commonplace in modern applications. When \nmaking transactional updates, the isolation is what allows the concurrent updates \nto have the same effect as if they were executed sequentially. At runtime, isolation \nfor persistent memory updates is typically achieved by locking. Since the memory is \npersistent, the isolation must be considered for transactions that were in-flight when \nthe application was interrupted. Persistent memory programmers typically detect \nthis situation on restart and roll partially done transactions forward or backward \nappropriately before allowing general-purpose threads access to the data structures.\n\u0007Durability\nA transaction is considered durable if it is on persistent media when it is complete. Even if the \nsystem loses power or crashes at that point, the transaction remains completed. As described \nin Chapter 2, this usually means the changes must be flushed from the CPU caches. This can \nbe done using standard APIs, such as the Linux msync() call, or platform-specific instructions \nsuch as Intel’s CLWB. When implementing transactions on persistent memory, pay careful \nattention to ensure that log entries are flushed to persistence before changes are started and \nflush changes to persistence before a transaction is considered complete.\nAnother aspect of the durable property is the ability to find the persistent \ninformation again when an application starts up. This is so fundamental to how storage \nworks that we take it for granted. Metadata such as file names and directory names are \nused to find the durable state of an application on storage. For persistent memory, the \nsame is true due to the programming model described in Chapter 3, where persistent \nmemory is accessed by first opening a file on a direct access (DAX) file system and then \nmemory mapping that file. However, a memory-mapped file is just a range of raw data; \nChapter 4  Fundamental Concepts of Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "338.03 sec"
            },
            {
              "page_number": 88,
              "text": "59\nhow does the application find the data structures resident in that range? For persistent \nmemory, there must be at least one well-known location of a data structure to use as a \nstarting point. This is often referred to as a root object (described in Chapter 7). The root \nobject is used by many of the higher-level libraries within PMDK to access the data.\n\u0007Flushing Is Not Transactional\nIt is important to separate the ideas of flushing to persistence from transactional \nupdates. Flushing changes to storage using calls like msync() or fsync() on Linux \nand FlushFileBuffers() on Windows have never provided transactional updates. \nApplications assume the responsibility for maintaining consistent storage data structures \nin addition to flushing changes to storage. With persistent memory, the same is true. In \nChapter 3, a simple program stored a string to persistent memory and then flushed it to \nmake sure the change was persistent. But that code was not transactional, and in the face \nof failure, the change could be in just about any state – from completely lost to partially \nlost to fully completed.\nA fundamental property of caches is that they hold data temporarily for \nperformance, but they do not typically hold data until a transaction is ready to commit. \nNormal system activity can cause cache pressure and evict data at any time and in any \norder. If the examples in Chapter 3 were interrupted by power failure, it is possible for \nany part of the string being stored to be lost and any part to be persistent, in any order. \nIt is important to think of the cache flush operation as flush anything that hasn’t already \nbeen flushed and not as flush all my changes now.\nFinally, we showed a decision tree in Chapter 2 (Figure 2-5) where an application can \ndetermine at startup that no cache flushing is required for persistent memory. This can \nbe the case on platforms where the CPU cache is flushed automatically on power failure, \nfor example. Even on platforms where flush instructions are not needed, transactions are \nstill required to keep data structures consistent in the face of failure.\n\u0007Start-Time Responsibilities\nIn Chapter 2 (Figures 2-5 and 2-6), we showed flowcharts outlining the application’s \nresponsibilities when using persistent memory. These responsibilities included \ndetecting platform details, available instructions, media failures, and so on. For storage, \nthese types of things happen in the storage stack in the operating system. Persistent \nChapter 4  Fundamental Concepts of Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "343.67 sec"
            },
            {
              "page_number": 89,
              "text": "60\nmemory, however, allows direct access, which removes the kernel from the data path \nonce the file is memory mapped.\nAs a programmer, you may be tempted to map persistent memory and start using it, \nas shown in the Chapter 3 examples. For production-quality programming, you want to \nensure these start-time responsibilities are met. For example, if you skip the checks in \nFigure 2-5, you will end up with an application that flushes CPU caches even when it is \nnot required, and that will perform poorly on hardware that does not need the flushing. \nIf you skip the checks in Figure 2-6, you will have an application that ignores media \nerrors and may use corrupted data resulting in unpredictable and undefined behavior.\n\u0007Tuning for Hardware Configurations\nWhen storing a large data structure to persistent memory, there are several ways to copy \nthe data and make it persistent. You can either copy the data using the common store \noperations and then flush the caches (if required) or use special instructions like Intel’s \nnon-temporal store instructions that bypass the CPU caches. Another consideration \nis that persistent memory write performance may be slower than writing to normal \nmemory, so you may want to take steps to store to persistent memory as efficiently as \npossible, by combining multiple small writes into larger changes before storing them to \npersistent memory. The optimal write size for persistent memory will depend on both \nthe platform it is plugged into and the persistent memory product itself. These examples \nshow that different platforms will have different characteristics when using persistent \nmemory, and any production-quality application will be tuned to perform best on the \nintended target platforms. Naturally, one way to help with this tuning work is to leverage \nlibraries or middleware that has already been tuned and validated.\n\u0007Summary\nThis chapter provides an overview of the fundamental concepts of persistent memory \nprogramming. When developing an application that uses persistent memory, you must \ncarefully consider several areas:\n•\t\nAtomic updates.\n•\t\nFlushing is not transactional.\nChapter 4  Fundamental Concepts of Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "348.28 sec"
            },
            {
              "page_number": 90,
              "text": "61\n•\t\nStart-time responsibilities.\n•\t\nTuning for hardware configurations.\nHandling these challenges in a production-quality application requires some \ncomplex programming and extensive testing and performance analysis. The next chapter \nintroduces the Persistent Memory Development Kit, designed to assist application \ndevelopers in solving these challenges.\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 4  Fundamental Concepts of Persistent Memory Programming",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "350.94 sec"
            },
            {
              "page_number": 91,
              "text": "63\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_5\nCHAPTER 5\nIntroducing the Persistent \nMemory Development Kit\nPrevious chapters introduced the unique properties of persistent memory that make it \nspecial, and you are correct in thinking that writing software for such a novel technology \nis complicated. Anyone who has researched or developed code for persistent memory \ncan testify to this. To make your job easier, Intel created the Persistent Memory \nDevelopment Kit (PMDK). The team of PMDK developers envisioned it to be the \nstandard library for all things persistent memory that would provide solutions to the \ncommon challenges of persistent memory programming.\n\u0007Background\nThe PMDK has evolved to become a large collection of open source libraries and \ntools for application developers and system administrators to simplify managing and \naccessing persistent memory devices. It was developed alongside evolving support for \npersistent memory in operating systems, which ensures the libraries take advantage of \nall the features exposed through the operating system interfaces.\nThe PMDK libraries build on the SNIA NVM programming model (described in \nChapter 3). They extend it to varying degrees, some by simply wrapping around the \nprimitives exposed by the operating system with easy-to-use functions and others by \nproviding complex data structures and algorithms for use with persistent memory. \nThis means you are responsible for making an informed decision about which level of \nabstraction is the best for your use case.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "355.77 sec"
            },
            {
              "page_number": 92,
              "text": "64\nAlthough the PMDK was created by Intel to support its hardware products, Intel is \ncommitted to ensuring the libraries and tools are both vendor and platform neutral. This \nmeans that the PMDK is not tied to Intel processors or Intel persistent memory devices. \nIt can be made to work on any other platform that exposes the necessary interfaces \nthrough the operating system, including Linux and Microsoft Windows. We welcome \nand encourage contributions to PMDK from individuals, hardware vendors, and ISVs. \nThe PMDK has a BSD 3-Clause License, allowing developers to embed it in any software, \nwhether it’s open source or proprietary. This allows you to pick and choose individual \ncomponents of PMDK by integrating only the bits of code required.\nThe PMDK is available at no cost on GitHub (https://github.com/pmem/pmdk) and \nhas a dedicated web site at https://pmem.io. Man pages are delivered with PMDK and \nare available online under each library’s own page. Appendix B of this book describes \nhow to install it on your system.\nAn active persistent memory community is available through Google Forums at \nhttps://groups.google.com/forum/#!forum/pmem. This forum allows developers, \nsystem administrators, and others with an interest in persistent memory to ask questions \nand get assistance. This is a great resource.\n\u0007Choosing the Right Semantics\nWith so many libraries available within the PMDK, it is important to carefully consider \nyour options. The PMDK offers two library categories:\n\t 1.\t Volatile libraries are for use cases that only wish to exploit the \ncapacity of persistent memory.\n\t 2.\t Persistent libraries are for use in software that wishes to \nimplement fail-safe persistent memory algorithms.\nWhile you are deciding how to best solve a problem, carefully consider which \ncategory it fits into. The challenges that fail-safe persistent programs present are \nsignificantly different from volatile ones. Choosing the right approach upfront will \nminimize the risk of having to rewrite any code.\nYou may decide to use libraries from both categories for different parts of the \napplication, depending on feature and functional requirements.\nChapter 5  Introducing the Persistent Memory Development Kit",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "361.38 sec"
            },
            {
              "page_number": 93,
              "text": "65\n\u0007Volatile Libraries\nVolatile libraries are typically simpler to use because they can fall back to dynamic \nrandom-access memory (DRAM) when persistent memory is not available. This \nprovides a more straightforward implementation. Depending on the workload, they may \nalso have lower overall overhead compared to similar persistent libraries because they \ndo not need to ensure consistency of data in the presence of failures.\nThis section explores the available libraries for volatile use cases in applications, \nincluding what the library is and when to use it. The libraries may have overlapping \nsituation use cases.\n\u0007libmemkind\nWhat is it?\nThe memkind library, called libmemkind, is a user-extensible heap manager built \non top of jemalloc. It enables control of memory characteristics and partitioning of the \nheap between different kinds of memory. The kinds of memory are defined by operating \nsystem memory policies that have been applied to virtual address ranges. Memory \ncharacteristics supported by memkind without user extension include control of \nnonuniform memory access (NUMA) and page size features. The jemalloc nonstandard \ninterface has been extended to enable specialized kinds to make requests for virtual \nmemory from the operating system through the memkind partition interface. Through \nthe other memkind interfaces, you can control and extend memory partition features \nand allocate memory while selecting enabled features. The memkind interface allows \nyou to create and control file-backed memory from persistent memory with PMEM kind.\n Chapter 10 describes this library in more detail. You can download memkind and \nread the architecture specification and API documentation at http://memkind.github.\nio/memkind/. memkind is an open source project on GitHub at https://github.com/\nmemkind/memkind.\nWhen to use it?\nChoose libmemkind when you want to manually move select memory objects to \npersistent memory in a volatile application while retaining the traditional programming \nmodel. The memkind library provides familiar malloc() and free() semantics. This is \nthe recommended memory allocator for most volatile use cases of persistent memory.\nChapter 5  Introducing the Persistent Memory Development Kit",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "366.99 sec"
            },
            {
              "page_number": 94,
              "text": "66\nModern memory allocators usually rely on anonymous memory mapping to \nprovision memory pages from the operating system. For most systems, this means that \nactual physical memory is allocated only when a page is first accessed, allowing the OS \nto overprovision virtual memory. Additionally, anonymous memory can be paged out \nif needed. When using memkind with file-based kinds, such as PMEM kind, physical \nspace is still only allocated on first access to a page and the other described techniques \nno longer apply. Memory allocation will fail when there is no memory available to be \nallocated, so it is important to handle such failures within the application.\nThe described techniques also play an important role in hiding the inherent \ninefficiencies of manual dynamic memory allocation such as fragmentation, which \ncauses allocation failures when not enough contiguous free space is available. Thus, file-­\nbased kinds can exhibit low space utilization for applications with irregular allocation/\ndeallocation patterns. Such workloads may be better served with libvmemcache.\n\u0007libvmemcache\nWhat is it?\nlibvmemcache is an embeddable and lightweight in-memory caching solution that \ntakes full advantage of large-capacity memory, such as persistent memory with direct \nmemory access (DAX), through memory mapping in an efficient and scalable way. \nlibvmemcache has unique characteristics:\n•\t\nAn extent-based memory allocator sidesteps the fragmentation \nproblem that affects most in-memory databases and allows the cache \nto achieve very high space utilization for most workloads.\n•\t\nThe buffered least recently used (LRU) algorithm combines a \ntraditional LRU doubly linked list with a non-blocking ring buffer to \ndeliver high degrees of scalability on modern multicore CPUs.\n•\t\nThe critnib indexing structure delivers high performance while \nbeing very space efficient.\nThe cache is tuned to work optimally with relatively large value sizes. The smallest \npossible size is 256 bytes, but libvmemcache works best if the expected value sizes are \nabove 1 kilobyte.\nChapter 10 describes this library in more detail. libvmemcache is an open source \nproject on GitHub at https://github.com/pmem/vmemcache.\nChapter 5  Introducing the Persistent Memory Development Kit",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "371.73 sec"
            },
            {
              "page_number": 95,
              "text": "67\nWhen to use it?\nUse libvmemcache when implementing caching for workloads that typically would \nhave low space efficiency when cached using a system with a normal memory allocation \nscheme.\n\u0007libvmem\nWhat is it?\nlibvmem is a deprecated predecessor to libmemkind. It is a jemalloc-derived \nmemory allocator, with both metadata and objects allocations placed in file-based \nmapping. The libvmem library is an open source project available from https://pmem.\nio/pmdk/libvmem/.\nWhen to use it?\nUse libvmem only if you have an existing application that uses libvmem or if you \nneed to have multiple completely separate heaps of memory. Otherwise, consider using \nlibmemkind.\n\u0007Persistent Libraries\nPersistent libraries help applications maintain data structure consistency in the presence \nof failures. In contrast to the previously described volatile libraries, these provide new \nsemantics and take full advantage of the unique possibilities enabled by persistent \nmemory.\n\u0007libpmem\nWhat is it?\nlibpmem is a low-level C library that provides basic abstraction over the primitives \nexposed by the operating system. It automatically detects features available in the \nplatform and chooses the right durability semantics and memory transfer (memcpy()) \nmethods optimized for persistent memory. Most applications will need at least parts of \nthis library.\nChapter 5  Introducing the Persistent Memory Development Kit",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "375.30 sec"
            },
            {
              "page_number": 96,
              "text": "68\nChapter 4 describes the requirements for applications using persistent memory, and \nChapter 6 describes libpmem in more depth.\nWhen to use it?\nUse libpmem when modifying an existing application that already uses memory-­\nmapped I/O. Such applications can leverage the persistent memory synchronization \nprimitives, such as user space flushing, to replace msync(), thus reducing the kernel \noverhead.\nAlso use libpmem when you want to build everything from the ground up. It \nsupports implementation of low-level persistent data structures with custom memory \nmanagement and recovery logic.\n\u0007libpmemobj\nWhat is it?\nlibpmemobj is a C library that provides a transactional object store, with a manual \ndynamic memory allocator, transactions, and general facilities for persistent memory \nprogramming. This library solves many of the commonly encountered algorithmic and \ndata structure problems when programming for persistent memory. Chapter 7 describes \nthis library in detail.\nWhen to use it?\nUse libpmemobj when the programming language of choice is C and when you need \nflexibility in terms of data structures design but can use a general-purpose memory \nallocator and transactions.\n\u0007libpmemobj-cpp\nWhat is it?\nlibpmemobj-cpp, also known as libpmemobj++, is a C++ header-only library that uses \nthe metaprogramming features of C++ to provide a simpler, less error-prone interface to \nlibpmemobj. It enables rapid development of persistent memory applications by reusing \nmany concepts C++ programmers are already familiar with, such as smart pointers and \nclosure-based transactions.\nThis library also ships with custom-made, STL-compatible data structures and \ncontainers, so that application developers do not have to reinvent the basic algorithms \nfor persistent memory.\nChapter 5  Introducing the Persistent Memory Development Kit",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "379.92 sec"
            },
            {
              "page_number": 97,
              "text": "69\nWhen to use it?\nWhen C++ is an option, libpmemobj-cpp is preferred for general-purpose persistent \nmemory programming over libpmemobj. Chapter 7 describes this library in detail.\n\u0007libpmemkv\nWhat is it?\nlibpmemkv is a generic embedded local key-value store optimized for persistent \nmemory. It is easy to use and ships with many different language integrations, including \nC, C++, and JavaScript.\nThis library has a pluggable back end for different storage engines. Thus, it can \nbe used as a volatile library, although it was originally designed primarily to support \npersistent use cases.\nChapter 9 describes this library in detail.\nWhen to use it?\nThis library is the recommended starting point into the world of persistent memory \nprogramming because it is approachable and has a simple interface. Use it when \ncomplex and custom data structures are not needed and a generic key-value store \ninterface is enough to solve the current problem.\n\u0007libpmemlog\nWhat is it?\nlibpmemlog is a C library that implements a persistent memory append-only log file \nwith power fail-safe operations.\nWhen to use it?\nUse libpmemlog when your use case exactly fits into the provided log API; otherwise, \na more generic library such as libpmemobj or libpmemobj-cpp might be more useful.\n\u0007libpmemblk\nWhat is it?\nlibpmemblk is a C library for managing fixed-size arrays of blocks. It provides fail-safe \ninterfaces to update the blocks through buffer-based functions.\nChapter 5  Introducing the Persistent Memory Development Kit",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "384.53 sec"
            },
            {
              "page_number": 98,
              "text": "70\nWhen to use it?\nUse libpmemblk only when a simple array of fixed blocks is needed and direct byte-­\nlevel access to blocks is not required.\n\u0007Tools and Command Utilities\nPMDK comes with a wide variety of tools and utilities to assist in the development and \ndeployment of persistent memory applications.\n\u0007pmempool\nWhat is it?\nThe pmempool utility is a tool for managing and offline analysis of persistent \nmemory pools. Its variety of functionalities, useful throughout the entire life cycle of an \napplication, include\n•\t\nObtaining information and statistics from a memory pool\n•\t\nChecking a memory pool’s consistency and repairing it if possible\n•\t\nCreating memory pools\n•\t\nRemoving/deleting a previously created memory pool\n•\t\nUpdating internal metadata to the latest layout version\n•\t\nSynchronizing replicas within a poolset\n•\t\nModifying internal data structures within a poolset\n•\t\nEnabling or disabling pool and poolset features\nWhen to use it?\nUse pmempool whenever you are creating persistent memory pools for applications \nusing any of the persistent libraries from PMDK.\n\u0007pmemcheck\nWhat is it?\nThe pmemcheck utility is a Valgrind-based tool for dynamic runtime analysis \nof common persistent memory errors, such as a missing flush or incorrect use of \ntransactions. Chapter 12 describes this utility in detail.\nChapter 5  Introducing the Persistent Memory Development Kit",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "388.21 sec"
            },
            {
              "page_number": 99,
              "text": "71\nWhen to use it?\nThe pmemcheck utility is useful when developing an application using libpmemobj, \nlibpmemobj-cpp, or libpmem because it can help you find bugs that are common in \npersistent applications. We suggest running error-checking tools early in the lifetime of a \ncodebase to avoid a pileup of hard-to-debug problems. The PMDK developers integrate \npmemcheck tests into the continuous integration pipeline of PMDK, and we recommend \nthe same for any persistent applications.\n\u0007pmreorder\nWhat is it?\nThe pmreorder utility helps detect data structure consistency problems of persistent \napplications in the presence of failures. It does this by first recording and then replaying \nthe persistent state of the application while verifying consistency of the application’s \ndata structures at any possible intermediate state. Chapter 12 describes this utility in \ndetail.\nWhen to use it?\nJust like pmemcheck, pmreorder is an essential tool for finding hard-to-debug \npersistent problems and should be integrated into the development and testing cycle of \nany persistent memory application.\n\u0007Summary\nThis chapter provides a brief listing of the libraries and tools available in PMDK \nand when to use them. You now have enough information to know what is possible. \nThroughout the rest of this book, you will learn how to create software using these \nlibraries and tools.\nThe next chapter introduces libpmem and describes how to use it to create simple \npersistent applications.\nChapter 5  Introducing the Persistent Memory Development Kit",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "392.92 sec"
            },
            {
              "page_number": 100,
              "text": "72\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 5  Introducing the Persistent Memory Development Kit",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "395.58 sec"
            },
            {
              "page_number": 101,
              "text": "73\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_6\nCHAPTER 6\nlibpmem: Low-Level \nPersistent Memory \nSupport\nThis chapter introduces libpmem, one of the smallest libraries in PMDK. This C library \nis very low level, dealing with things like CPU instructions related to persistent memory, \noptimal ways to copy data to persistence, and file mapping. Programmers who only want \ncompletely raw access to persistent memory, without libraries to provide allocators or \ntransactions, will likely want to use libpmem as a basis for their development.\nThe code in libpmem that detects the available CPU instructions, for example, is a \nmundane boilerplate code that you do not want to invent repeatedly in applications. \nLeveraging this small amount of code from libpmem will save time, and you get the \nbenefit of fully tested and tuned code in the library.\nFor most programmers, libpmem is too low level, and you can safely skim this \nchapter quickly (or skip it altogether) and move on to the higher-level, friendlier \nlibraries available in PMDK. All the PMDK libraries that deal with persistence, such as \nlibpmemobj, are built on top of libpmem to meet their low-level needs.\nLike all PMDK libraries, online man pages are available. For libpmem, they are at \nhttp://pmem.io/pmdk/libpmem/. This site includes links to the man pages for both the \nLinux and Windows version. Although the goal of the PMDK project was to make the \ninterfaces similar across operating systems, some small differences appear as necessary. \nThe C code examples used in this chapter build and run on both Linux and Windows.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "400.19 sec"
            },
            {
              "page_number": 102,
              "text": "74\nThe examples used in this chapter are\n•\t\nsimple_copy.c is a small program that copies a 4KiB block from a \nsource file to a destination file on persistent memory.\n•\t\nfull_copy.c is a more complete copy program, copying  \nthe entire file.\n•\t\nmanpage.c is the simple example used in the libpmem man page.\n\u0007Using the Library\nTo use libpmem, start by including the appropriate header, as shown in Listing 6-1.\nListing 6-1.  Including the libpmem headers\n    32\n    33  /*\n    34   * simple_copy.c\n    35   *\n    36   * usage: simple_copy src-file dst-file\n    37   *\n    38   * Reads 4KiB from src-file and writes it to dst-file.\n    39   */\n    40\n    41  #include <sys/types.h>\n    42  #include <sys/stat.h>\n    43  #include <fcntl.h>\n    44  #include <stdio.h>\n    45  #include <errno.h>\n    46  #include <stdlib.h>\n    47  #ifndef _WIN32\n    48  #include <unistd.h>\n    49  #else\n    50  #include <io.h>\n    51  #endif\n    52  #include <string.h>\n    53  #include <libpmem.h>\nChapter 6  libpmem: Low-Level Persistent Memory Support",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "403.78 sec"
            },
            {
              "page_number": 103,
              "text": "75\nNotice the include on line 53. To use libpmem, use this include line, and link the C \nprogram with libpmem using the -lpmem option when building under Linux.\n\u0007Mapping a File\nThe libpmem library contains some convenience functions for memory mapping files. \nOf course, your application can call mmap() on Linux or MapViewOfFile() on Windows \ndirectly, but using libpmem has some advantages:\n•\t\nlibpmem knows the correct arguments to the operating system \nmapping calls. For example, on Linux, it is not safe to flush changes \nto persistent memory using the CPU instructions directly unless the \nmapping is created with the MAP_SYNC flag to mmap().\n•\t\nlibpmem detects if the mapping is actually persistent memory and if \nusing the CPU instructions directly for flushing is safe.\nListing 6-2 shows how to memory map a file on a persistent memory-aware file \nsystem into the application.\nListing 6-2.  Mapping a persistent memory file\n    80      /* create a pmem file and memory map it */\n    81      if ((pmemaddr = pmem_map_file(argv[2], BUF_LEN,\n    82              PMEM_FILE_CREATE|PMEM_FILE_EXCL,\n    83              0666, &mapped_len, &is_pmem)) == NULL) {\n    84          perror(\"pmem_map_file\");\n    85          exit(1);\n    86      }\nAs part of the persistent memory detection mentioned earlier, the flag is_pmem is \nreturned by pmem_map_file. It is the caller’s responsibility to use this flag to determine \nhow to flush changes to persistence. When making a range of memory persistent, the \ncaller can use the optimal flush provided by libpmem, pmem_persist, only if the is_pmem \nflag is set. This is illustrated in the man page example excerpt in Listing 6-3.\nChapter 6  libpmem: Low-Level Persistent Memory Support",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "409.82 sec"
            },
            {
              "page_number": 104,
              "text": "76\nListing 6-3.  manpage.c: Using the is_pmem flag\n    74      /* Flush above strcpy to persistence */\n    75      if (is_pmem)\n    76          pmem_persist(pmemaddr, mapped_len);\n    77      else\n    78          pmem_msync(pmemaddr, mapped_len);\nListing 6-3 shows the convenience function pmem_msync(), which is just a small \nwrapper around msync() or the Windows equivalent. You do not need to build in \ndifferent logic for Linux and Windows because libpmem handles this.\n\u0007Copying to Persistent Memory\nThere are several interfaces in libpmem for optimally copying or zeroing ranges of \npersistent memory. The simplest interface shown in Listing 6-4 is used to copy the block \nof data from the source file to the persistent memory in the destination file and flush it to \npersistence.\nListing 6-4.  simple_copy.c: Copying to persistent memory\n    88      /* read up to BUF_LEN from srcfd */\n    89      if ((cc = read(srcfd, buf, BUF_LEN)) < 0) {\n    90          pmem_unmap(pmemaddr, mapped_len);\n    91          perror(\"read\");\n    92          exit(1);\n    93      }\n    94\n    95      /* write it to the pmem */\n    96      if (is_pmem) {\n    97          pmem_memcpy_persist(pmemaddr, buf, cc);\n    98      } else {\n    99          memcpy(pmemaddr, buf, cc);\n   100          pmem_msync(pmemaddr, cc);\n   101      }\nChapter 6  libpmem: Low-Level Persistent Memory Support",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "414.53 sec"
            },
            {
              "page_number": 105,
              "text": "77\nNotice how the is_pmem flag on line 96 is used just like it would be for calls to pmem_\npersist(), since the pmem_memcpy_persist() function includes the flush to persistence.\nThe interface pmem_memcpy_persist() includes the flush to persistent because it \nmay determine that the copy is more optimally performed by using non-temporal stores, \nwhich bypass the CPU cache and do not require subsequent cache flush instructions for \npersistence. By providing this API, which both copies and flushes, libpmem is free to use \nthe most optimal way to perform both steps.\n\u0007Separating the Flush Steps\nFlushing to persistence involves two steps:\n\t 1.\t Flush the CPU caches or bypass them entirely as explained in the \nprevious example.\n\t 2.\t Wait for any hardware buffers to drain, to ensure writes have \nreached the media.\nThese steps are performed together when pmem_persist() is called, or they can be \ncalled individually by calling pmem_flush() for the first step and pmem_drain() for the \nsecond. Note that either of these steps may be unnecessary on a given platform, and \nthe library knows how to check for that and do what is correct. For example, on Intel \nplatforms, pmem_drain is an empty function.\nWhen does it make sense to break flushing into steps? The example in Listing 6-5 \nillustrates one reason you might want to do this. Since the example copies data using \nmultiple calls to memcpy(), it uses the version of libpmem copy (pmem_memcpy_nodrain()) \nthat only performs the flush, postponing the final drain step to the end. This works \nbecause, unlike the flush step, the drain step does not take an address range; it is a \nsystem-wide drain operation so can happen at the end of the loop that copies individual \nblocks of data.\nListing 6-5.  full_copy.c: Separating the flush steps\n    58  /*\n    59   * do_copy_to_pmem\n    60   */\n    61  static void\n    62  do_copy_to_pmem(char *pmemaddr, int srcfd, off_t len)\nChapter 6  libpmem: Low-Level Persistent Memory Support",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "420.16 sec"
            },
            {
              "page_number": 106,
              "text": "78\n    63  {\n    64      char buf[BUF_LEN];\n    65      int cc;\n    66\n    67      /*\n    68       * Copy the file,\n    69       * saving the last flush & drain step to the end\n    70       */\n    71      while ((cc = read(srcfd, buf, BUF_LEN)) > 0) {\n    72          pmem_memcpy_nodrain(pmemaddr, buf, cc);\n    73          pmemaddr += cc;\n    74      }\n    75\n    76      if (cc < 0) {\n    77          perror(\"read\");\n    78          exit(1);\n    79      }\n    80\n    81      /* Perform final flush step */\n    82      pmem_drain();\n    83  }\nIn Listing 6-5, pmem_memcpy_nodrain() is specifically designed for persistent \nmemory. When using other libraries and standard functions like memcpy(), remember \nthey were written before persistent memory existed and do not perform any flushing \nto persistence. In particular, the memcpy() provided by the C runtime environment \noften chooses between regular stores (which require flushing) and non-temporal stores \n(which do not require flushing). It is making that choice based on performance, not \npersistence. Since you will not know which instructions it chooses, you will need to \nperform the flush to persistence yourself using pmem_persist() or msync().\nThe choice of instructions used when copying ranges to persistent memory is fairly \nimportant to the performance in many applications. The same is true when zeroing out \nranges of persistent memory. To meet these needs, libpmem provides pmem_memmove(), \npmem_memcpy(), and pmem_memset(), which all take a flags argument to give the \ncaller more control over which instructions they use. For example, passing the flag \nChapter 6  libpmem: Low-Level Persistent Memory Support",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "425.79 sec"
            },
            {
              "page_number": 107,
              "text": "79\nPMEM_F_MEM_NONTEMPORAL will tell these functions to use non-temporal stores instead of \nchoosing which instructions to use based on the size of the range. The full list of flags is \ndocumented in the man pages for these functions.\n\u0007Summary\nThis chapter demonstrated some of the fairly small set of APIs provided by libpmem. \nThis library does not track what changed for you, does not provide power fail-safe \ntransactions, and does not provide an allocator. Libraries like libpmemobj (described in \nthe next chapter) provide all those tasks and use libpmem internally for simple flushing \nand copying.\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 6  libpmem: Low-Level Persistent Memory Support",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "430.41 sec"
            },
            {
              "page_number": 108,
              "text": "81\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_7\nCHAPTER 7\nlibpmemobj: A Native \nTransactional Object Store\nIn the previous chapter, we described libpmem, the low-level persistent memory library \nthat provides you with an easy way to directly access persistent memory. libpmem is a \nsmall, lightweight, and feature-limited library that is designed for software that tracks \nevery store to pmem and needs to flush those changes to persistence. It excels at what \nit does. However, most developers will find higher-level libraries within the Persistent \nMemory Development Kit (PMDK), like libpmemobj, to be much more convenient.\nThis chapter describes libpmemobj, which builds upon libpmem and turns persistent \nmemory-mapped files into a flexible object store. It supports transactions, memory \nmanagement, locking, lists, and several other features.\n\u0007What is libpmemobj?\nThe libpmemobj library provides a transactional object store in persistent memory for \napplications that require transactions and persistent memory management using direct \naccess (DAX) to the memory. Briefly recapping our DAX discussion in Chapter 3, DAX \nallows applications to memory map files on a persistent memory-aware file system to \nprovide direct load/store operations without paging blocks from a block storage device. \nIt bypasses the kernel, avoids context switches and interrupts, and allows applications to \nread and write directly to the byte-addressable persistent storage.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "435.01 sec"
            },
            {
              "page_number": 109,
              "text": "82\n\u0007Why not malloc( )?\nUsing libpmem seems simple. You need to flush anything you have written and use \ndiscipline when ordering such that data needs to be persisted before any pointers to it \ngo live.\nIf only persistent memory programming were so simple. Apart from some specific \npatterns that can be done in a simpler way, such as append-only records that can be \nefficiently handled by libpmemlog, any new piece of data needs to have its memory \nallocated. When and how should the allocator mark the memory as in use? Should the \nallocator mark the memory as allocated before writing data or after? Neither approach \nworks for these reasons:\n•\t\nIf the allocator marks the memory as allocated before the data is \nwritten, a power outage during the write can cause torn updates and \na so-called “persistent leak.”\n•\t\nIf the allocator writes the data, then marks it as allocated, a power \noutage that occurs between the write completing and the allocator \nmarking it as allocated can overwrite the data when the application \nrestarts since the allocator believes the block is available.\nAnother problem is that a significant number of data structures include cyclical \nreferences and thus do not form a tree. They could be implemented as a tree, but this \napproach is usually harder to implement.\nByte-addressable memory guarantees atomicity of only a single write. For current \nprocessors, that is generally one 64-bit word (8-bytes) that should be aligned, but this is \nnot a requirement in practice.\nAll of the preceding problems could be solved if multiple writes occurred \nsimultaneously. In the event of a power failure, any incomplete writes should either \nbe replayed as though the power failure never happened or discarded as though the \nwrite never occurred. Applications solve this in different ways using atomic operations, \ntransactions, redo/undo logging, etc. Using libpmemobj can solve those problems \nbecause it uses atomic transactions and redo/undo logs.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "440.64 sec"
            },
            {
              "page_number": 110,
              "text": "83\n\u0007Grouping Operations\nWith the exception of modifying a single scalar value that fits within the processor’s \nword, a series of data modifications must be grouped together and accompanied by a \nmeans of detecting an interruption before completion.\n\u0007Memory Pools\nMemory-mapped files are at the core of the persistent memory programming model. \nThe libpmemobj library provides a convenient API to easily manage pool creation and \naccess, avoiding the complexity of directly mapping and synchronizing data. PMDK \nalso provides a pmempool utility to administer memory pools from the command line. \nMemory pools reside on DAX-mounted file systems.\n\u0007Creating Memory Pools\nUse the pmempool utility to create persistent memory pools for use with applications. \nSeveral pool types can be created including pmemblk, pmemlog, and pmemobj. When using \nlibpmemobj in applications, you want to create a pool of type obj (pmemobj). Refer \nto the pmempool-create(1) man page for all available commands and options. The \nfollowing examples are for reference:\nExample 1.  Create a libpmemobj (obj) type pool of minimum allowed size and \nlayout called “my_layout” in the mounted file system /mnt/pmemfs0/\n$ pmempool create --layout my_layout obj /mnt/pmemfs0/pool.obj\nExample 2.  Create a libpmemobj (obj) pool of 20GiB and layout called “my_\nlayout” in the mounted file system /mnt/pmemfs0/\n$ pmempool create --layout my_layout –-size 20G obj \\\n/mnt/pmemfs0/pool.obj\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "444.23 sec"
            },
            {
              "page_number": 111,
              "text": "84\nExample 3.  Create a libpmemobj (obj) pool using all available capacity within \nthe /mnt/pmemfs0/ file system using the layout name of “my_layout”\n$ pmempool create --layout my_layout –-max-size obj \\\n/mnt/pmemfs0/pool.obj\nApplications can programmatically create pools that do not exist at application start \ntime using pmemobj_create(). pmemobj_create() has the following arguments:\nPMEMobjpool *pmemobj_create(const char *path,\n    const char *layout, size_t poolsize, mode_t mode);\n•\t\npath specifies the name of the memory pool file to be created, \nincluding a full or relative path to the file.\n•\t\nlayout specifies the application’s layout type in the form of a string to \nidentify the pool.\n•\t\npoolsize specifies the required size for the pool. The memory pool \nfile is fully allocated to the size poolsize using posix_fallocate(3). \nThe minimum size for a pool is defined as PMEMOBJ_MIN_POOL in \n<libpmemobj.h>. If the pool already exists, pmemobj_create() will \nreturn an EEXISTS error. Specifying poolsize as zero will take the \npool size from the file size and will verify that the file appears to be \nempty by searching for any nonzero data in the pool header at the \nbeginning of the file.\n•\t\nmode specifies the ACL permissions to use when creating the file, as \ndescribed by create(2).\nListing 7-1 shows how to create a pool using the pmemobj_create() function.\nListing 7-1.  pwriter.c – An example showing how to create a pool using \npmemobj_create()\n    33  /*\n    34   * pwriter.c -  Write a string to a\n    35   *              persistent memory pool\n    36   */\n    37\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "449.24 sec"
            },
            {
              "page_number": 112,
              "text": "85\n    38  #include <stdio.h>\n    39  #include <string.h>\n    40  #include <libpmemobj.h>\n    41\n    42  #define LAYOUT_NAME \"rweg\"\n    43  #define MAX_BUF_LEN 31\n    44\n    45  struct my_root {\n    46      size_t len;\n    47      char buf[MAX_BUF_LEN];\n    48  };\n    49\n    50  int\n    51  main(int argc, char *argv[])\n    52  {\n    53      if (argc != 2) {\n    54          printf(\"usage: %s file-name\\n\", argv[0]);\n    55          return 1;\n    56      }\n    57\n    58      PMEMobjpool *pop = pmemobj_create(argv[1],\n    59          LAYOUT_NAME, PMEMOBJ_MIN_POOL, 0666);\n    60\n    61      if (pop == NULL) {\n    62          perror(\"pmemobj_create\");\n    63          return 1;\n    64      }\n    65\n    66      PMEMoid root = pmemobj_root(pop,\n    67          sizeof(struct my_root));\n    68\n    69      struct my_root *rootp = pmemobj_direct(root);\n    70\n    71      char buf[MAX_BUF_LEN] = \"Hello PMEM World\";\n    72\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "452.83 sec"
            },
            {
              "page_number": 113,
              "text": "86\n    73      rootp->len = strlen(buf);\n    74      pmemobj_persist(pop, &rootp->len,\n    75          sizeof(rootp->len));\n    76\n    77      pmemobj_memcpy_persist(pop, rootp->buf, buf,\n    78          rootp->len);\n    79\n    80      pmemobj_close(pop);\n    81\n    82      return 0;\n    83  }\n•\t\nLine 42: We define the name for our pool layout to be “rweg” (read-­\nwrite example). This is just a name and can be any string that \nuniquely identifies the pool to the application. A NULL value is valid. \nIn the case where multiple pools are opened by the application, this \nname uniquely identifies it.\n•\t\nLine 43: We define the maximum length of the write buffer.\n•\t\nLines 45-47: This defines the root object data structure which has \nmembers len and buf. buf contains the string we want to write, and \nthe len is the length of the buffer.\n•\t\nLines 53- 56: The pwriter command accepts one argument: the path \nand pool name to write to. For example, /mnt/pmemfs0/helloworld_\nobj.pool. The file name extension is arbitrary and optional.\n•\t\nLines 58-59: We call pmemobj_create() to create the pool using \nthe file name passed in from the command line, the layout name \nof “rweg,” a size we set to be the minimum size for an object pool \ntype, and permissions of 0666. We cannot create a pool smaller than \ndefined by PMEMOBJ_MIN_POOL or larger than the available space \non the file system. Since the string in our example is very small, we \nonly require a minimally sized pool. On success, pmemobj_create() \nreturns a pool object pointer (POP) of type PMEMobjpool, that we can \nuse to acquire a pointer to the root object.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "459.79 sec"
            },
            {
              "page_number": 114,
              "text": "87\n•\t\nLines 61-64: If pmemobj_create() fails, we will exit the program and \nreturn an error.\n•\t\nLine 66: Using the pop acquired from line 58, we use the pmemobj_\nroot() function to locate the root object.\n•\t\nLine 69: We use the pmemobj_direct() function to get a pointer to the \nroot object we found in line 66.\n•\t\nLine 71: We set the string/buffer to “Hello PMEM World.”\n•\t\nLines 73-78. After determining the length of the buffer, we first write \nthe len and then the buf member of our root object to persistent \nmemory.\n•\t\nLine 80: We close the persistent memory pool by unmapping it.\n\u0007Pool Object Pointer (POP) and the Root Object\nDue to the address space layout randomization (ASLR) feature used by most operating \nsystems, the location of the pool – once memory mapped into the application address \nspace – can differ between executions and system reboots. Without a way to access \nthe data within the pool, you would find it challenging to locate the data within a pool. \nPMDK-based pools have a small amount of metadata to solve this problem.\nEvery pmemobj (obj) type pool has a root object. This root object is necessary \nbecause it is used as an entry point from which to find all the other objects created in a \npool, that is, user data. An application will locate the root object using a special object \ncalled pool object pointer (POP). The POP object resides in volatile memory and is \ncreated with every program invocation. It keeps track of metadata related to the pool, \nsuch as the offset to the root object inside the pool. Figure 7-1 depicts the POP and \nmemory pool layout.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "464.50 sec"
            },
            {
              "page_number": 115,
              "text": "88\nUsing a valid pop pointer, you can use the pmemobj_root() function to get a pointer \nof the root object. Internally, this function creates a valid pointer by adding the current \nmemory address of the mapped pool plus the internal offset to the root.\n\u0007Opening and Reading from Memory Pools\nYou create a pool using pmemobj_create(), and you open an existing pool using \npmemobj_open(). Both functions return a PMEMobjpool *pop pointer. The pwriter \nexample in Listing 7-1 shows how to create a pool and write a string to it. Listing 7-2 \nshows how to open the same pool to read and display the string.\nListing 7-2.  preader.c – An example showing how to open a pool and access the \nroot object and data\n    33  /*\n    34   * preader.c -  Read a string from a\n    35   *              persistent memory pool\n    36   */\n    37\n    38  #include <stdio.h>\n    39  #include <string.h>\n    40  #include <libpmemobj.h>\n    41\nFigure 7-1.  A high-level overview of a persistent memory pool with a pool object \npointer (POP) pointing to the root object\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page115_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page115_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "468.60 sec"
            },
            {
              "page_number": 116,
              "text": "89\n    42  #define LAYOUT_NAME \"rweg\"\n    43  #define MAX_BUF_LEN 31\n    44\n    45  struct my_root {\n    46      size_t len;\n    47      char buf[MAX_BUF_LEN];\n    48  };\n    49\n    50  int\n    51  main(int argc, char *argv[])\n    52  {\n    53      if (argc != 2) {\n    54          printf(\"usage: %s file-name\\n\", argv[0]);\n    55          return 1;\n    56      }\n    57\n    58      PMEMobjpool *pop = pmemobj_open(argv[1],\n    59          LAYOUT_NAME);\n    60\n    61      if (pop == NULL) {\n    62          perror(\"pmemobj_open\");\n    63          return 1;\n    64      }\n    65\n    66      PMEMoid root = pmemobj_root(pop,\n    67          sizeof(struct my_root));\n    68      struct my_root *rootp = pmemobj_direct(root);\n    69\n    70      if (rootp->len == strlen(rootp->buf))\n    71          printf(\"%s\\n\", rootp->buf);\n    72\n    73      pmemobj_close(pop);\n    74\n    75      return 0;\n    76  }\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "472.28 sec"
            },
            {
              "page_number": 117,
              "text": "90\n•\t\nLines 42-48: We use the same data structure declared in pwriter.c. In \npractice, this should be declared in a header file for consistency.\n•\t\nLine 58: Open the pool and return a pop pointer to it\n•\t\nLine 66: Upon success, pmemobj_root() returns a handle to the root \nobject associated with the persistent memory pool pop.\n•\t\nLine 68: pmemobj_direct() returns a pointer to the root object.\n•\t\nLines 70-71: Determine the length of the buffer pointed to by  \nrootp->buf. If it matches the length of the buffer we wrote, the \ncontents of the buffer is printed to STDOUT.\n\u0007Memory Poolsets\nThe capacity of multiple pools can be combined into a poolset. Besides providing a \nway to increase the available space, a poolset can be used to span multiple persistent \nmemory devices and provide both local and remote replication.\nYou open a poolset the same way as a single pool using pmemobj_open(). (At the \ntime of publication, pmemobj_create() and the pmempool utility cannot create poolsets. \nEnhancement requests exist for these features.) Although creating poolsets requires \nmanual administration, poolset management can be automated via libpmempool or the \npmempool utility; full details appear in the poolset(5) man page.\n\u0007Concatenated Poolsets\nIndividual pools can be concatenated using pools on a single or multiple file systems. \nConcatenation only works with the same pool type: block, object, or log pools. Listing 7-­3 \nshows an example “myconcatpool.set” poolset file that concatenates three smaller pools \ninto a larger pool. For illustrative purposes, each pool is a different size and located on \ndifferent file systems. An application using this poolset would see a single 700GiB memory \npool.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "477.91 sec"
            },
            {
              "page_number": 118,
              "text": "91\nListing 7-3.  myconcatpool.set – An example of a concatenated poolset created \nfrom three individual pools on three different file systems\nPMEMPOOLSET\nOPTION NOHDRS\n100G /mountpoint0/myfile.part0\n200G /mountpoint1/myfile.part1\n400G /mountpoint2/myfile.part2\nNote  Data will be preserved if it exists in /mountpoint0/myfile.part0, but \nany data in /mountpoint0/myfile.part1 or /mountpoint0/myfile.part2 \nwill be lost. We recommend that you only add new and empty pools to a poolset.\n\u0007Replica Poolsets\nBesides combining multiple pools to provide more space, a poolset can also maintain \nmultiple copies of the same data to increase resiliency. Data can be replicated to another \npoolset on a different file of the local host and a poolset on a remote host.\nListing 7-4 shows a poolset file called “myreplicatedpool.set” that will replicate \nlocal writes into the /mnt/pmem0/pool1 pool to another local pool, /mnt/pmem1/pool1, \non a different file system, and to a remote-objpool.set poolset on a remote host called \nexample.com.\nListing 7-4.  myreplicatedpool.set – An example demonstrating how to replicate \nlocal data locally and remote host\nPMEMPOOLSET\n256G /mnt/pmem0/pool1\nREPLICA\n256G /mnt/pmem1/pool1\nREPLICA user@example.com remote-objpool.set\nThe librpmem library, a remote persistent memory support library, underpins this \nfeature. Chapter 18 discusses librpmem and replica pools in more detail.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "482.62 sec"
            },
            {
              "page_number": 119,
              "text": "92\n\u0007Managing Memory Pools and Poolsets\nThe pmempool utility has several features that developers and system administrators \nmay find useful. We do not present their details here because each command has a \ndetailed man page:\n•\t\npmempool info prints information and statistics in human-readable \nformat about the specified pool.\n•\t\npmempool check checks the pool’s consistency and repairs pool if it \nis not consistent.\n•\t\npmempool create creates a pool of specified type with additional \nproperties specific for this type of pool.\n•\t\npmempool dump dumps usable data from a pool in hexadecimal or \nbinary format.\n•\t\npmempool rm removes pool file or all pool files listed in pool set \nconfiguration file.\n•\t\npmempool convert updates the pool to the latest available layout \nversion.\n•\t\npmempool sync synchronizes replicas within a poolset.\n•\t\npmempool transform modifies the internal structure of a poolset.\n•\t\npmempool feature toggles or queries a poolset’s features.\n\u0007Typed Object Identifiers (TOIDs)\nWhen we write data to a persistent memory pool or device, we commit it at a physical \naddress. With the ASLR feature of operating systems, when applications open a pool and \nmemory map it into the address space, the virtual address will change each time. For this \nreason, a type of handle (pointer) that does not change is needed; this handle is called \nan OID (object identifier). Internally, it is a pair of the pool or poolset unique identifier \n(UUID) and an offset within the pool or poolset. The OID can be translated back and \nforth between its persistent form and pointers that are fit for direct use by this particular \ninstance of your program.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "488.26 sec"
            },
            {
              "page_number": 120,
              "text": "93\nAt a low level, the translation can be done manually via functions such as \npmemobj_direct() that appear in the preader.c example in Listing 7-2. Because manual \ntranslations require explicit type casts and are error prone, we recommend tagging every \nobject with a type. This allows some form of type safety, and thanks to macros, can be \nchecked at compile time.\nFor example, a persistent variable declared via TOID(struct foo) x can be read via \nD_RO(x)->field. In a pool with the following layout: \nPOBJ_LAYOUT_BEGIN(cathouse);\nPOBJ_LAYOUT_TOID(cathouse, struct canaries);\nPOBJ_LAYOUT_TOID(cathouse, int);\nPOBJ_LAYOUT_END(cathouse);\nThe field val declared on the first line can be accessed using any of the subsequent \nthree operations: \nTOID(int) val;\nTOID_ASSIGN(val, oid_of_val); // Assigns 'oid_of_val' to typed OID 'val' \nD_RW(val) = 42; // Returns a typed write pointer to 'val' and writes 42 \nreturn D_RO(val); // Returns a typed read-only (const) pointer to 'val' \n\u0007Allocating Memory\nUsing malloc() to allocate memory is quite normal to C developers and those who use \nlanguages that do not fully handle automatic memory allocation and deallocation. For \npersistent memory, you can use pmemobj_alloc(), pmemobj_reserve(), or pmemobj_\nxreserve() to reserve memory for a transient object and use it the same way you would \nuse malloc(). We recommend that you free allocated memory using pmemobj_free() or \nPOBJ_FREE() when the application no longer requires it to avoid a runtime memory leak. \nBecause these are volatile memory allocations, they will not cause a persistent leak after \na crash or graceful application exit.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "493.18 sec"
            },
            {
              "page_number": 121,
              "text": "94\n\u0007Persisting Data\nThe typical intent of using persistent memory is to save data persistently. For this, you \nneed to use one of three APIs that libpmemobj provides:\n•\t\nAtomic operations\n•\t\nReserve/publish\n•\t\nTransactional\n\u0007Atomic Operations\nThe pmemobj_alloc() and its variants shown below are easy to use, but they are limited \nin features, so additional coding is required by the developer:\nint pmemobj_alloc(PMEMobjpool *pop, PMEMoid *oidp,\n    size_t size, uint64_t type_num, pmemobj_constr\n    constructor, void *arg);\nint pmemobj_zalloc(PMEMobjpool *pop, PMEMoid *oidp,\n    size_t size, uint64_t type_num);\nvoid pmemobj_free(PMEMoid *oidp);\nint pmemobj_realloc(PMEMobjpool *pop, PMEMoid *oidp,\n    size_t size, uint64_t type_num);\nint pmemobj_zrealloc(PMEMobjpool *pop, PMEMoid *oidp,\n    size_t size, uint64_t type_num);\nint pmemobj_strdup(PMEMobjpool *pop, PMEMoid *oidp,\n    const char *s, uint64_t type_num);\nint pmemobj_wcsdup(PMEMobjpool *pop, PMEMoid *oidp,\n    const wchar_t *s, uint64_t type_num);\nThe TOID-based wrappers for most of these functions include: \nPOBJ_NEW(PMEMobjpool *pop, TOID *oidp, TYPE,\n    pmemobj_constr constructor, void *arg)\nPOBJ_ALLOC(PMEMobjpool *pop, TOID *oidp, TYPE, size_t size,\n    pmemobj_constr constructor, void *arg)\nPOBJ_ZNEW(PMEMobjpool *pop, TOID *oidp, TYPE)\nPOBJ_ZALLOC(PMEMobjpool *pop, TOID *oidp, TYPE, size_t size)\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "498.98 sec"
            },
            {
              "page_number": 122,
              "text": "95\nPOBJ_REALLOC(PMEMobjpool *pop, TOID *oidp, TYPE, size_t size)\nPOBJ_ZREALLOC(PMEMobjpool *pop, TOID *oidp, TYPE, size_t size)\nPOBJ_FREE(TOID *oidp)\nThese functions reserve the object in a temporary state, call the constructor you \nprovided, and then in one atomic action, mark the allocation as persistent. They will \ninsert the pointer to the newly initialized object into a variable of your choice.\nIf the new object needs to be merely zeroed, pmemobj_zalloc() does so without \nrequiring a constructor.\nBecause copying NULL-terminated strings is a common operation, libpmemobj \nprovides pmemobj_strdup() and its wide-char variant pmemobj_wcsdup() to handle \nthis. pmemobj_strdup() provides the same semantics as strdup(3) but operates on the \npersistent memory heap associated with the memory pool.\nOnce you are done with the object, pmemobj_free() will deallocate the object while \nzeroing the variable that stored the pointer to it. The pmemobj_free() function frees the \nmemory space represented by oidp, which must have been allocated by a previous call \nto pmemobj_alloc(), pmemobj_xalloc(), pmemobj_zalloc(), pmemobj_realloc(), \nor pmemobj_zrealloc(). The pmemobj_free() function provides the same semantics as \nfree(3), but instead of operating on the process heap supplied by the system, it operates \non the persistent memory heap.\nListing 7-5 shows a small example of allocating and freeing memory using the \nlibpmemobj API.\nListing 7-5.  Using pmemobj_alloc() to allocate memory and using pmemobj_\nfree() to free it\n    33  /*\n    34   * pmemobj_alloc.c - An example to show how to use\n    35   *                   pmemobj_alloc()\n    36   */\n    ..\n    47  typedef uint32_t color;\n    48\n    49  static int paintball_init(PMEMobjpool *pop,\n    50          void *ptr, void *arg)\n    51  {\n    52      *(color *)ptr = time(0) & 0xffffff;\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "505.97 sec"
            },
            {
              "page_number": 123,
              "text": "96\n    53      pmemobj_persist(pop, ptr, sizeof(color));\n    54      return 0;\n    55  }\n    56\n    57  int main()\n    58  {\n    59      PMEMobjpool *pool = pmemobj_open(POOL, LAYOUT);\n    60      if (!pool) {\n    61          pool = pmemobj_create(POOL, LAYOUT,\n    62          PMEMOBJ_MIN_POOL, 0666);\n    63          if (!pool)\n    64              die(\"Couldn't open pool: %m\\n\");\n    65\n    66      }\n    67      PMEMoid root = pmemobj_root(pool,\n    68              sizeof(PMEMoid) * 6);\n    69      if (OID_IS_NULL(root))\n    70          die(\"Couldn't access root object.\\n\");\n    71\n    72      PMEMoid *chamber = (PMEMoid *)pmemobj_direct(root)\n    73          + (getpid() % 6);\n    74      if (OID_IS_NULL(*chamber)) {\n    75          printf(\"Reloading.\\n\");\n    76          if (pmemobj_alloc(pool, chamber, sizeof(color)\n    77              , 0, paintball_init, 0))\n    78              die(\"Failed to alloc: %m\\n\");\n    79      } else {\n    80          printf(\"Shooting %06x colored bullet.\\n\",\n    81          *(color *)pmemobj_direct(*chamber));\n    82          pmemobj_free(chamber);\n    83      }\n    84\n    85      pmemobj_close(pool);\n    86      return 0;\n    87  }\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "509.66 sec"
            },
            {
              "page_number": 124,
              "text": "97\n•\t\nLine 47: Defines a color that will be stored in the pool.\n•\t\nLines 49-54: The paintball_init() function is called when we \nallocate memory (line 76). This function takes a pool and object \npointer, calculates a random hex value for the paintball color, and \npersistently writes it to the pool. The program exits when the write \ncompletes.\n•\t\nLines 59-70: Opens or creates a pool and acquires a pointer to the \nroot object within the pool.\n•\t\nLine 72: Obtain a pointer to an offset within the pool.\n•\t\nLines 74-78: If the pointer in line 72 is not a valid object, we allocate \nsome space and call paintball_init().\n•\t\nLines 79-80: If the pointer in line 72 is a valid object, we read the color \nvalue, print the string, and free the object.\n\u0007Reserve/Publish API\nThe atomic allocation API will not help if\n•\t\nThere is more than one reference to the object that needs to be \nupdated\n•\t\nThere are multiple scalars that need to be updated\nFor example, if your program needs to subtract money from account A and add it \nto account B, both operations must be done together. This can be done via the reserve/\npublish API.\nTo use it, you specify any number of operations to be done. The operations may be \nsetting a scalar 64-bit value using pmemobj_set_value(), freeing an object with pmemobj_\ndefer_free(), or allocating it using pmemobj_reserve(). Of these, only the allocation \nhappens immediately, letting you do any initialization of the newly reserved object. \nModifications will not become persistent until pmemobj_publish() is called.\nFunctions provided by libpmemobj related to the reserve/publish feature are\nPMEMoid pmemobj_reserve(PMEMobjpool *pop,\n    struct pobj_action *act, size_t size, uint64_t type_num);\nvoid pmemobj_defer_free(PMEMobjpool *pop, PMEMoid oid,\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "515.70 sec"
            },
            {
              "page_number": 125,
              "text": "98\n    struct pobj_action *act);\nvoid pmemobj_set_value(PMEMobjpool *pop,\n    struct pobj_action *act, uint64_t *ptr, uint64_t value);\nint pmemobj_publish(PMEMobjpool *pop,\n    struct pobj_action *actv, size_t actvcnt);\nvoid pmemobj_cancel(PMEMobjpool *pop,\n    struct pobj_action *actv, size_t actvcnt);\nListing 7-6 is a simple banking example that demonstrates how to change multiple \nscalars (account balances) before publishing the updates into the pool.\nListing 7-6.  Using the reserve/publish API to modify bank account balances\n    32\n    33  /*\n    34   * reserve_publish.c – An example using the\n    35   *                     reserve/publish libpmemobj API\n    36   */\n    37\n    ..\n    44  #define POOL \"/mnt/pmem/balance\"\n    45\n    46  static PMEMobjpool *pool;\n    47\n    48  struct account {\n    49      PMEMoid name;\n    50      uint64_t balance;\n    51  };\n    52  TOID_DECLARE(struct account, 0);\n    53\n    ..\n    60  static PMEMoid new_account(const char *name,\n    61                  int deposit)\n    62  {\n    63      int len = strlen(name) + 1;\n    64\n    65      struct pobj_action act[2];\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "519.80 sec"
            },
            {
              "page_number": 126,
              "text": "99\n    66      PMEMoid str = pmemobj_reserve(pool, act + 0,\n    67                      len, 0);\n    68      if (OID_IS_NULL(str))\n    69          die(\"Can't allocate string: %m\\n\");\n    ..\n    75      pmemobj_memcpy(pool, pmemobj_direct(str), name,\n    76                      len, PMEMOBJ_F_MEM_NODRAIN);\n    77      TOID(struct account) acc;\n    78      PMEMoid acc_oid = pmemobj_reserve(pool, act + 1,\n    79                      sizeof(struct account), 1);\n    80      TOID_ASSIGN(acc, acc_oid);\n    81      if (TOID_IS_NULL(acc))\n    82          die(\"Can't allocate account: %m\\n\");\n    83      D_RW(acc)->name = str;\n    84      D_RW(acc)->balance = deposit;\n    85      pmemobj_persist(pool, D_RW(acc),\n    86                      sizeof(struct account));\n    87      pmemobj_publish(pool, act, 2);\n    88      return acc_oid;\n    89  }\n    90\n    91  int main()\n    92  {\n    93      if (!(pool = pmemobj_create(POOL, \" \",\n    94                             PMEMOBJ_MIN_POOL, 0600)))\n    95          die(\"Can't create pool \"%s\": %m\\n\", POOL);\n    96\n    97      TOID(struct account) account_a, account_b;\n    98      TOID_ASSIGN(account_a,\n    99                    new_account(\"Julius Caesar\", 100));\n   100      TOID_ASSIGN(account_b,\n   101                    new_account(\"Mark Anthony\", 50));\n   102\n   103      int price = 42;\n   104      struct pobj_action act[2];\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "525.43 sec"
            },
            {
              "page_number": 127,
              "text": "100\n   105      pmemobj_set_value(pool, &act[0],\n   106                      &D_RW(account_a)->balance,\n   107                      D_RW(account_a)->balance – price);\n   108      pmemobj_set_value(pool, &act[1],\n   109                      &D_RW(account_b)->balance,\n   110                      D_RW(account_b)->balance + price);\n   111      pmemobj_publish(pool, act, 2);\n   112\n   113      pmemobj_close(pool);\n   114      return 0;\n   115  }\n•\t\nLine 44: Defines the location of the memory pool.\n•\t\nLines 48-52: Declares an account data structure with a name and \nbalance.\n•\t\nLines 60-89: The new_account() function reserves the memory (lines \n66 and 78), updates the name and balance (lines 83 and 84), persists \nthe changes (line 85), and then publishes the updates (line 87).\n•\t\nLines 93-95: Create a new pool or exit on failure.\n•\t\nLine 97: Declare two account instances.\n•\t\nLines 98-101: Create a new account for each owner with initial \nbalances.\n•\t\nLines 103-111: We subtract 42 from Julius Caesar’s account and add \n42 to Mark Anthony’s account. The modifications are published on \nline 111.\n\u0007Transactional API\nThe reserve/publish API is fast, but it does not allow reading data you have just written. \nIn such cases, you can use the transactional API.\nThe first time a variable is written, it must be explicitly added to the transaction. This \ncan be done via pmemobj_tx_add_range() or its variants (xadd, _direct). Convenient \nmacros such as TX_ADD() or TX_SET() can perform the same operation. The transaction-­\nbased functions and macros provided by libpmemobj include\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "531.06 sec"
            },
            {
              "page_number": 128,
              "text": "101\nint pmemobj_tx_add_range(PMEMoid oid, uint64_t off,\n    size_t size);\nint pmemobj_tx_add_range_direct(const void *ptr, size_t size);\nTX_ADD(TOID o)\nTX_ADD_FIELD(TOID o, FIELD)\nTX_ADD_DIRECT(TYPE *p)\nTX_ADD_FIELD_DIRECT(TYPE *p, FIELD)\nTX_SET(TOID o, FIELD, VALUE)\nTX_SET_DIRECT(TYPE *p, FIELD, VALUE)\nTX_MEMCPY(void *dest, const void *src, size_t num)\nTX_MEMSET(void *dest, int c, size_t num)\nThe transaction may also allocate entirely new objects, reserve their memory, and \nthen persistently allocate them only one transaction commit. These functions include\nPMEMoid pmemobj_tx_alloc(size_t size, uint64_t type_num);\nPMEMoid pmemobj_tx_zalloc(size_t size, uint64_t type_num);\nPMEMoid pmemobj_tx_realloc(PMEMoid oid, size_t size,\n    uint64_t type_num);\nPMEMoid pmemobj_tx_zrealloc(PMEMoid oid, size_t size,\n    uint64_t type_num);\nPMEMoid pmemobj_tx_strdup(const char *s, uint64_t type_num);\nPMEMoid pmemobj_tx_wcsdup(const wchar_t *s,\n    uint64_t type_num);\nWe can rewrite the banking example from Listing 7-6 using the transaction API. Most \nof the code remains the same except when we want to add or subtract amounts from the \nbalance; we encapsulate those updates in a transaction, as shown in Listing 7-7.\nListing 7-7.  Using the transaction API to modify bank account balances\n    33  /*\n    34   * tx.c - An example using the transaction API\n    35   */\n    36\n    ..\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "534.64 sec"
            },
            {
              "page_number": 129,
              "text": "102\n    94  int main()\n    95  {\n    96      if (!(pool = pmemobj_create(POOL, \" \",\n    97                          PMEMOBJ_MIN_POOL, 0600)))\n    98          die(\"Can't create pool \"%s\": %m\\n\", POOL);\n    99\n   100      TOID(struct account) account_a, account_b;\n   101      TOID_ASSIGN(account_a,\n   102                    new_account(\"Julius Caesar\", 100));\n   103      TOID_ASSIGN(account_b,\n   104                    new_account(\"Mark Anthony\", 50));\n   105\n   106      int price = 42;\n   107      TX_BEGIN(pool) {\n   108          TX_ADD_DIRECT(&D_RW(account_a)->balance);\n   109          TX_ADD_DIRECT(&D_RW(account_b)->balance);\n   110          D_RW(account_a)->balance -= price;\n   111          D_RW(account_b)->balance += price;\n   112      } TX_END\n   113\n   114      pmemobj_close(pool);\n   115      return 0;\n   116  }\n•\t\nLine 107: We start the transaction.\n•\t\nLines 108-111: Make balance modifications to multiple accounts.\n•\t\nLine 112: Finish the transaction. All updates will either complete \nentirely or they will be rolled back if the application or system crashes \nbefore the transaction completes.\nEach transaction has multiple stages in which an application can interact. These \ntransaction stages include\n•\t\nTX_STAGE_NONE: No open transaction in this thread.\n•\t\nTX_STAGE_WORK: Transaction in progress.\n•\t\nTX_STAGE_ONCOMMIT: Successfully committed.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "540.38 sec"
            },
            {
              "page_number": 130,
              "text": "103\n•\t\nTX_STAGE_ONABORT: The transaction start either failed or was aborted.\n•\t\nTX_STAGE_FINALLY: Ready for cleanup.\nThe example in Listing 7-7 uses the two mandatory stages: TX_BEGIN and TX_END. \nHowever, we could easily have added the other stages to perform actions for the other \nstages, for example:\nTX_BEGIN(Pop) {\n        /* the actual transaction code goes here... */\n} TX_ONCOMMIT {\n        /*\n         * optional - executed only if the above block\n         * successfully completes\n         */\n} TX_ONABORT {\n        /*\n         * optional - executed only if starting the transaction\n         * fails, or if transaction is aborted by an error or a\n         * call to pmemobj_tx_abort()\n         */\n} TX_FINALLY {\n        /*\n         * optional - if exists, it is executed after\n         * TX_ONCOMMIT or TX_ONABORT block\n         */\n} TX_END /* mandatory */\nOptionally, you can provide a list of parameters for the transaction. Each parameter \nconsists of a type followed by one of these type-specific number of values:\n•\t\nTX_PARAM_NONE is used as a termination marker with no following \nvalue.\n•\t\nTX_PARAM_MUTEX is followed by one value, a pmem-resident \nPMEMmutex.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "544.99 sec"
            },
            {
              "page_number": 131,
              "text": "104\n•\t\nTX_PARAM_RWLOCK is followed by one value, a pmem-resident \nPMEMrwlock.\n•\t\nTX_PARAM_CB is followed by two values: a callback function of type \npmemobj_tx_callback and a void pointer.\nUsing TX_PARAM_MUTEX or TX_PARAM_RWLOCK causes the specified lock to be acquired \nat the beginning of the transaction. TX_PARAM_RWLOCK acquires the lock for writing. \nIt is guaranteed that pmemobj_tx_begin() will acquire all locks prior to successful \ncompletion, and they will be held by the current thread until the outermost transaction \nis finished. Locks are taken in order from left to right. To avoid deadlocks, you are \nresponsible for proper lock ordering.\nTX_PARAM_CB registers the specified callback function to be executed at each \ntransaction stage. For TX_STAGE_WORK, the callback is executed prior to commit. For all \nother stages, the callback is executed as the first operation after a stage change. It will \nalso be called after each transaction.\n\u0007Optional Flags\nMany of the functions discussed for the atomic, reserve/publish, and transactional APIs \nhave a variant with a \"flags\" argument that accepts these values:\n•\t\nPOBJ_XALLOC_ZERO zeroes the object allocated.\n•\t\nPOBJ_XALLOC_NO_FLUSH suppresses automatic flushing. It is expected \nthat you flush the data in some way; otherwise, it may not be durable \nin case of an unexpected power loss.\n\u0007Persisting Data Summary\nThe atomic, reserve/publish, and transactional APIs have different strengths:\n•\t\nAtomic allocations are the simplest and fastest, but their use is \nlimited to allocating and initializing wholly new blocks.\n•\t\nThe reserve/publish API can be as fast as atomic allocations when \nall operations involve either allocating or deallocating whole objects \nor modifying scalar values. However, being able to read the data you \nhave just written may be desirable.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "549.59 sec"
            },
            {
              "page_number": 132,
              "text": "105\n•\t\nThe transactional API requires slow synchronization whenever \na variable is added to the transaction. If the variable is changed \nmultiple times during the transaction, subsequent operations are \nfree. It also allows conveniently mutating pieces of data larger than a \nsingle machine word.\n\u0007Guarantees of libpmemobj's APIs\nThe transactional, atomic allocation, and reserve/publish APIs within libpmemobj all \nprovide fail-safe atomicity and consistency.\nThe transactional API ensures the durability of any modifications of memory for \nan object that has been added to the transaction. An exception is when the POBJ_X***_\nNO_FLUSH flag is used, in which case the application is responsible for either flushing \nthat memory range itself or using the memcpy-like functions from libpmemobj. The \nno-flush flag does not provide any isolation between threads, meaning partial writes are \nimmediately visible to other threads.\nThe atomic allocation API requires that applications flush the writes done by the \nobject’s constructor. This ensures durability if the operation succeeded. It is the only API \nthat provides full isolation between threads.\nThe reserve/publish API requires explicit flushes of writes to memory blocks \nallocated via pmemobj_reserve() that will flush writes done via pmemobj_set_value(). \nThere is no isolation between threads, although no modifications go live until pmemobj_\npublish() starts, allowing you to take explicit locks for just the publishing stage.\nUsing terms known from databases, the isolation levels provided are\n•\t\nTransactional API: READ_UNCOMMITTED\n•\t\nAtomic allocations API: READ_COMMITTED\n•\t\nReserve/publish API: READ_COMMITTED until publishing starts, then \nREAD_UNCOMMITTED\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "555.64 sec"
            },
            {
              "page_number": 133,
              "text": "106\n\u0007Managing Library Behavior\nThe pmemobj_set_funcs() function allows an application to override memory allocation \ncalls used internally by libpmemobj. Passing in NULL for any of the handlers will cause \nthe libpmemobj default function to be used. The library does not make heavy use of the \nsystem malloc() functions, but it does allocate approximately 4–8 kilobytes for each \nmemory pool in use.\nBy default, libpmemobj supports up to 1024 parallel transactions/allocations. For \ndebugging purposes, it is possible to decrease this value by setting the PMEMOBJ_NLANES \nshell environment variable to the desired limit. For example, at the shell prompt, run \n\"export PMEMOBJ_NLANES=512\" then run the application:\n$ export PMEMOBJ_NLANES=512\n$ ./my_app\nTo return to the default behavior, unset PMEMOBJ_NLANES using\n$ unset PMEMOBJ_NLANES\n\u0007Debugging and Error Handling\nIf an error is detected during the call to a libpmemobj function, the application \nmay retrieve an error message describing the reason for the failure from pmemobj_\nerrormsg(). This function returns a pointer to a static buffer containing the last error \nmessage logged for the current thread. If errno was set, the error message may include \na description of the corresponding error code as returned by strerror(3). The error \nmessage buffer is thread local; errors encountered in one thread do not affect its value \nin other threads. The buffer is never cleared by any library function; its content is \nsignificant only when the return value of the immediately preceding call to a libpmemobj \nfunction indicated an error, or if errno was set. The application must not modify or free \nthe error message string, but it may be modified by subsequent calls to other library \nfunctions.\nTwo versions of libpmemobj are typically available on a development system. The \nnon-debug version is optimized for performance and used when a program is linked \nusing the -lpmemobj option. This library skips checks that impact performance, never \nlogs any trace information, and does not perform any runtime assertions.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "560.35 sec"
            },
            {
              "page_number": 134,
              "text": "107\nA debug version of libpmemobj is provided and available in /usr/lib/pmdk_debug \nor /usr/local/lib64/pmdk_debug. The debug version contains runtime assertions and \ntracepoints.\nThe common way to use the debug version is to set the environment variable LD_\nLIBRARY_PATH. Alternatively, you can use LD_PRELOAD to point to /usr/lib/pmdk_debug \nor /usr/lib64/pmdk_debug, as appropriate. These libraries may reside in a different \nlocation, such as /usr/local/lib/pmdk_debug and /usr/local/lib64/pmdk_debug, \ndepending on your Linux distribution or if you compiled installed PMDK from source \ncode and chose /usr/local as the installation path. The following examples are \nequivalent methods for loading and using the debug versions of libpmemobj with an \napplication called my_app:\n$ export LD_LIBRARY_PATH=/usr/lib64/pmdk_debug\n$ ./my_app\nOr\n$ LD_PRELOAD=/usr/lib64/pmdk_debug ./my_app\nThe output provided by the debug library is controlled using the PMEMOBJ_LOG_LEVEL \nand PMEMOBJ_LOG_FILE environment variables. These variables have no effect on the \nnon-debug version of the library.\nPMEMOBJ_LOG_LEVEL\nThe value of PMEMOBJ_LOG_LEVEL enables tracepoints in the debug version of the \nlibrary, as follows:\n\t 1.\t This is the default level when PMEMOBJ_LOG_LEVEL is not set. No \nlog messages are emitted at this level.\n\t 2.\t Additional details on any errors detected are logged, in addition to \nreturning the errno-based errors as usual. The same information \nmay be retrieved using pmemobj_errormsg().\n\t 3.\t A trace of basic operations is logged.\n\t 4.\t Enables an extensive amount of function-call tracing in the \nlibrary.\n\t 5.\t Enables voluminous and fairly obscure tracing information that is \nlikely only useful to the libpmemobj developers.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "564.95 sec"
            },
            {
              "page_number": 135,
              "text": "108\nDebug output is written to STDERR unless PMEMOBJ_LOG_FILE is set. To set a debug \nlevel, use\n$ export PMEMOBJ_LOG_LEVEL=2\n$ ./my_app\nPMEMOBJ_LOG_FILE\nThe value of PMEMOBJ_LOG_FILE includes the full path and file name of a file where all \nlogging information should be written. If PMEMOBJ_LOG_FILE is not set, logging output is \nwritten to STDERR.\nThe following example defines the location of the log file to /var/tmp/libpmemobj_\ndebug.log, ensures we are using the debug version of libpmemobj when executing \nmy_app in the background, sets the debug log level to 2, and monitors the log in real time \nusing tail -f:\n$ export PMEMOBJ_LOG_FILE=/var/tmp/libpmemobj_debug.log\n$ export PMEMOBJ_LOG_LEVEL=2\n$ LD_PRELOAD=/usr/lib64/pmdk_debug ./my_app &\n$ tail –f /var/tmp/libpmemobj_debug.log\nIf the last character in the debug log file name is \"-\", the process identifier (PID) of \nthe current process will be appended to the file name when the log file is created. This is \nuseful if you are debugging multiple processes.\n\u0007Summary\nThis chapter describes the libpmemobj library, which is designed to simplify persistent \nmemory programming. By providing APIs that deliver atomic operations, transactions, \nand reserve/publish features, it makes creating applications less error prone while \ndelivering guarantees for data integrity.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "569.56 sec"
            },
            {
              "page_number": 136,
              "text": "109\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 7  libpmemobj: A Native Transactional Object Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "572.33 sec"
            },
            {
              "page_number": 137,
              "text": "111\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_8\nCHAPTER 8\nlibpmemobj-cpp: \nThe Adaptable Language - \nC++ and Persistent \nMemory\n\u0007Introduction\nThe Persistent Memory Development Kit (PMDK) includes several separate libraries; \neach is designed with a specific use in mind. The most flexible and powerful one is \nlibpmemobj. It complies with the persistent memory programming model without \nmodifying the compiler. Intended for developers of low-level system software and \nlanguage creators, the libpmemobj library provides allocators, transactions, and a way \nto automatically manipulate objects. Because it does not modify the compiler, its API is \nverbose and macro heavy.\nTo make persistent memory programming easier and less error prone, higher-­\nlevel language bindings for libpmemobj were created and included in PMDK. The C++ \nlanguage was chosen to create new and friendly API to libpmemobj called libpmemobj-­\ncpp, which is also referred to as libpmemobj++. C++ is versatile, feature rich, has a \nlarge developer base, and it is constantly being improved with updates to the C++ \nprogramming standard.\nThe main goal for the libpmemobj-cpp bindings design was to focus modifications to \nvolatile programs on data structures and not on the code. In other words, libpmemobj-­\ncpp bindings are for developers, who want to modify volatile applications, provided with \na convenient API for modifying structures and classes with only slight modifications to \nfunctions.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "577.04 sec"
            },
            {
              "page_number": 138,
              "text": "112\nThis chapter describes how to leverage the C++ language features that support \nmetaprogramming to make persistent memory programming easier. It also describes \nhow to make it more C++ idiomatic by providing persistent containers. Finally, we \ndiscuss C++ standard limitations for persistent memory programming, including an \nobject’s lifetime and the internal layout of objects stored in persistent memory.\n\u0007Metaprogramming to the Rescue\nMetaprogramming is a technique in which computer programs have the ability to treat \nother programs as their data. It means that a program can be designed to read, generate, \nanalyze or transform other programs, and even modify itself while running. In some \ncases, this allows programmers to minimize the number of lines of code to express a \nsolution, in turn reducing development time. It also allows programs greater flexibility to \nefficiently handle new situations without recompilation.\nFor the libpmemobj-cpp library, considerable effort was put into encapsulating \nthe PMEMoids (persistent memory object IDs) with a type-safe container. Instead of a \nsophisticated set of macros for providing type safety, templates and metaprogramming \nare used. This significantly simplifies the native C libpmemobj API.\n\u0007Persistent Pointers\nThe persistent memory programming model created by the Storage Networking Industry \nAssociation (SNIA) is based on memory-mapped files. PMDK uses this model for its \narchitecture and design implementation. We discussed the SNIA programming model in \nChapter 3.\nMost operating systems implement address space layout randomization (ASLR). \nASLR is a computer security technique involved in preventing exploitation of memory \ncorruption vulnerabilities. To prevent an attacker from reliably jumping to, for example, \na particular exploited function in memory, ASLR randomly arranges the address space \npositions of key data areas of a process, including the base of the executable and the \npositions of the stack, heap, and libraries. Because of ASLR, files can be mapped at \ndifferent addresses of the process address space each time the application executes. \nAs a result, traditional pointers that store absolute addresses cannot be used. Upon \neach execution, a traditional pointer might point to uninitialized memory for which \nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "584.00 sec"
            },
            {
              "page_number": 139,
              "text": "113\ndereferencing it may result in a segmentation fault. Or it might point to a valid memory \nrange, but not the one that the user expects it to point to, resulting in unexpected and \nundetermined behavior.\nTo solve this problem in persistent memory programming, a different type of pointer \nis needed. libpmemobj introduced a C struct called PMEMoid, which consists of an \nidentifier of the pool and an offset from its beginning. This fat pointer is encapsulated \nin libpmemobj C++ bindings as a template class pmem::obj::persistent_ptr. Both \nthe C and C++ implementations have the same 16-byte footprint. A constructor \nfrom raw PMEMoid is provided so that mixing the C API with C++ is possible. The \npmem::obj::persistent_ptr is similar in concept and implementation to the smart \npointers introduced in C++11 (std::shared_ptr, std::auto_ptr, std::unique_ptr, and \nstd::weak_ptr), with one big difference – it does not manage the object’s life cycle.\nBesides operator*, operator->, operator[], and typedefs for compatibility with \nstd::pointer_traits and std::iterator_traits, the pmem::obj::persistent_ptr \nalso has defined methods for persisting its contents. The pmem::obj::persistent_ptr \ncan be used in standard library algorithms and containers.\n\u0007Transactions\nBeing able to modify more than 8 bytes of storage at a time atomically is imperative for \nmost nontrivial algorithms one might want to use in persistent memory. Commonly, a \nsingle logical operation requires multiple stores. For example, an insert into a simple list-­\nbased queue requires two separate stores: a tail pointer and the next pointer of the last \nelement. To enable developers to modify larger amounts of data atomically, with respect \nto power-fail interruptions, the PMDK library provides transaction support in some of \nits libraries. The C++ language bindings wrap these transactions into two concepts: one, \nbased on the resource acquisition is initialization (RAII) idiom and the other based on \na callable std::function object. Additionally, because of some C++ standard issues, \nthe scoped transactions come in two flavors: manual and automatic. In this chapter we \nonly describe the approach with std::function object. For information about RAII-­\nbased transactions, refer to libpmemobj-cpp documentation (https://pmem.io/pmdk/\ncpp_obj/).\nThe method which uses std::function is declared as\nvoid pmem::obj::transaction::run(pool_base &pop,\n    std::function<void ()> tx, Locks&... locks)\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "589.62 sec"
            },
            {
              "page_number": 140,
              "text": "114\nThe locks parameter is a variadic template. Thanks to the std::function, a myriad \nof types can be passed in to run. One of the preferred ways is to pass a lambda function \nas the tx parameter. This makes the code compact and easier to analyze. Listing 8-1 \nshows how lambda can be used to perform work in a transaction.\nListing 8-1.  Function object transaction\n    45        // execute a transaction\n    46        pmem::obj::transaction::run(pop, [&]() {\n    47            // do transactional work\n    48        });\nOf course, this API is not limited to just lambda functions. Any callable target can \nbe passed as tx, such as functions, bind expressions, function objects, and pointers \nto member functions. Since run is a normal static member function, it has the benefit \nof being able to throw exceptions. If an exception is thrown during the execution of \na transaction, it is automatically aborted, and the active exception is rethrown so \ninformation about the interruption is not lost. If the underlying C library fails for any \nreason, the transaction is also aborted, and a C++ library exception is thrown. The \ndeveloper is no longer burdened with the task of checking the status of the previous \ntransaction.\nlibpmemobj-cpp transactions provide an entry point for persistent memory resident \nsynchronization primitives such as pmem::obj::mutex, pmem::obj::shared_mutex and \npmem::obj::timed_mutex. libpmemobj ensures that all locks are properly reinitialized \nwhen one attempts to acquire a lock for the first time. The use of pmem locks is \ncompletely optional, and transactions can be executed without them. The number of \nsupplied locks is arbitrary, and the types can be freely mixed. The locks are held until \nthe end of the given transaction, or the outermost transaction in the case of nesting. This \nmeans when transactions are enclosed by a try-catch statement, the locks are released \nbefore reaching the catch clause. This is extremely important in case some kind of \ntransaction abort cleanup needs to modify the shared state. In such a case, the necessary \nlocks need to be reacquired in the correct order.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "600.48 sec"
            },
            {
              "page_number": 141,
              "text": "115\n\u0007Snapshotting\nThe C library requires manual snapshots before modifying data in a transaction. The \nC++ bindings do all of the snapshotting automatically, to reduce the probability of \nprogrammer error. The pmem::obj::p template wrapper class is the basic building block \nfor this mechanism. It is designed to work with basic types and not compound types \nsuch as classes or PODs (Plain Old Data, structures with fields only and without any \nobject-oriented features). This is because it does not define operator->() and there is \nno possibility to implement operator.(). The implementation of pmem::obj::p is based \non the operator=(). Each time the assignment operator is called, the value wrapped \nby p will be changed, and the library needs to snapshot the old value. In addition to \nsnapshotting, the p<> template ensures the variable is persisted correctly, flushing data if \nnecessary. Listing 8-2 provides an example of using the p<> template.\nListing 8-2.  Using the p<> template to persist values correctly\n    39    struct bad_example {\n    40        int some_int;\n    41        float some_float;\n    42    };\n    43\n    44    struct good_example {\n    45        pmem::obj::p<int> pint;\n    46        pmem::obj::p<float> pfloat;\n    47    };\n    48\n    49    struct root {\n    50        bad_example bad;\n    51        good_example good;\n    52    };\n    53\n    54    int main(int argc, char *argv[]) {\n    55        auto pop = pmem::obj::pool<root>::open(\"/daxfs/file\", \"p\");\n    56\n    57        auto r = pop.root();\n    58\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "601.00 sec"
            },
            {
              "page_number": 142,
              "text": "116\n    59        pmem::obj::transaction::run(pop, [&]() {\n    60            r->bad.some_int = 10;\n    61            r->good.pint = 10;\n    62\n    63            r->good.pint += 1;\n    64        });\n    65\n    66        return 0;\n    67    }\n•\t\nLines 39-42: Here, we declare a bad_example structure with two \nvariables – some_int and some_float. Storing this structure on \npersistent memory and modifying it are dangerous because data is \nnot snapshotted automatically.\n•\t\nLines 44-47: We declare the good_example structure with two p<> \ntype variables – pint and pfloat. This structure can be safely stored \non persistent memory as every modification of pint or pfloat in a \ntransaction will perform a snapshot.\n•\t\nLines 55-57: Here, we open a persistent memory pool, created \nalready using the pmempool command, and obtain a pointer to the \nroot object stored within the root variable.\n•\t\nLine 60: We modify the integer value from the bad_example structure. \nThis modification is not safe because we do not add this variable to \nthe transaction; hence it will not be correctly made persistent if there \nis an unexpected application or system crash or power failure.\n•\t\nLine 61: Here, we modify integer value wrapped by p<> template. This \nis safe because operator=() will automatically snapshot the element.\n•\t\nLine 63: Using arithmetic operators on p<> (if the underlying type \nsupports it) is also safe.\n\u0007Allocating\nAs with std::shared_ptr, the pmem::obj::persistent_ptr comes with a set of allocating \nand deallocating functions. This helps allocate memory and create objects, as well as \ndestroy and deallocate the memory. This is especially important in the case of persistent \nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "606.94 sec"
            },
            {
              "page_number": 143,
              "text": "117\nmemory because all allocations and object construction/destruction must be done \natomically with respect to power-fail interruptions. The transactional allocations use \nperfect forwarding and variadic templates for object construction. This makes object \ncreation similar to calling the constructor and identical to std::make_shared. The \ntransactional array creation, however, requires the objects to be default constructible. \nThe created arrays can be multidimensional. The pmem::obj::make_persistent and \npmem::obj::make_persistent_array must be called within a transaction; otherwise, an \nexception is thrown. During object construction, other transactional allocations can be \nmade, and that is what makes this API very flexible. The specifics of persistent memory \nrequired the introduction of the pmem::obj::delete_persistent function, which \ndestroys objects and arrays of objects. Since the pmem::obj::persistent_ptr does not \nautomatically handle the lifetime of pointed to objects, the user is responsible for disposing \nof the ones that are no longer in use. Listing 8-3 shows example of transaction allocation.\nAtomic allocations behave differently as they do not return a pointer. Developers \nmust provide a reference to one as the function’s argument. Because atomic allocations \nare not executed in the context of a transaction, the actual pointer assignment must be \ndone through other means. For example, by redo logging the operation. Listing 8-3 also \nprovides an example of atomic allocation.\nListing 8-3.  Example of transactional and atomic allocations\n    39    struct my_data {\n    40        my_data(int a, int b): a(a), b(b) {\n    41\n    42        }\n    43\n    44        int a;\n    45        int b;\n    46    };\n    47\n    48    struct root {\n    49        pmem::obj::persistent_ptr<my_data> mdata;\n    50    };\n    51\n    52    int main(int argc, char *argv[]) {\n    53        auto pop = pmem::obj::pool<root>::open(\"/daxfs/file\", \"tx\");\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "612.53 sec"
            },
            {
              "page_number": 144,
              "text": "118\n    54\n    55        auto r = pop.root();\n    56\n    57        pmem::obj::transaction::run(pop, [&]() {\n    58            r->mdata = pmem::obj::make_persistent<my_data>(1, 2);\n    59        });\n    60\n    61        pmem::obj::transaction::run(pop, [&]() {\n    62            pmem::obj::delete_persistent<my_data>(r->mdata);\n    63        });\n    64        \u0007pmem::obj::make_persistent_atomic<my_data>(pop, r->mdata,  \n2, 3);\n    65\n    66        return 0;\n    67    }\n•\t\nLine 58: Here, we allocate my_data object transactionally. Parameters \npassed to make_persistent will be forwarded to my_data constructor. \nNote that assignment to r->mdata will perform a snapshot of old \npersistent pointer’s value.\n•\t\nLine 62: Here, we delete the my_data object. delete_persistent will \ncall the object’s destructor and free the memory.\n•\t\nLine 64: We allocate my_data object atomically. Calling this function \ncannot be done inside of a transaction.\n\u0007C++ Standard limitations\nThe C++ language restrictions and persistent memory programming paradigm imply \nserious restrictions on objects which may be stored on persistent memory. Applications \ncan access persistent memory with memory-mapped files to take advantage of its byte \naddressability thanks to libpmemobj and SNIA programming model. No serialization \ntakes place here, so applications must be able to read and modify directly from the \npersistent memory media even after the application was closed and reopened or after a \npower failure event.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "617.28 sec"
            },
            {
              "page_number": 145,
              "text": "119\nWhat does the preceding mean from a C++ and libpmemobj’s perspective? There are \nfour major problems:\n\t 1.\t Object lifetime\n\t 2.\t Snapshotting objects in transactions\n\t 3.\t Fixed on-media layout of stored objects\n\t 4.\t Pointers as object members\nThese four problems will be described in next four sections.\n\u0007An Object’s Lifetime\nThe lifetime of an object is described in the [basic.life] section of the C++ standard \n(https://isocpp.org/std/the-standard):\nThe lifetime of an object or reference is a runtime property of the object or \nreference. A variable is said to have vacuous initialization if it is default-­\ninitialized and, if it is of class type or a (possibly multi-dimensional) array \nthereof, that class type has a trivial default constructor. The lifetime of an \nobject of type T begins when:\n(1.1) storage with the proper alignment and size for type T is obtained, and\n(1.2) its initialization (if any) is complete (including vacuous initializa-\ntion) ([dcl.init]), except that if the object is a union member or subobject \nthereof, its lifetime only begins if that union member is the initialized mem-\nber in the union ([dcl.init.aggr], [class.base.init]), or as described in [class.\nunion]. The lifetime of an object of type T ends when:\n(1.3) if T is a non-class type, the object is destroyed, or\n(1.4) if T is a class type, the destructor call starts, or\n(1.5) the storage which the object occupies is released, or is reused by an \nobject that is not nested within o ([intro.object]).\nThe standard states that properties ascribed to objects apply for a given object only \nduring its lifetime. In this context, the persistent memory programming problem is \nsimilar to transmitting data over a network, where the C++ application is given an array \nof bytes but might be able to recognize the type of object sent. However, the object was \nnot constructed in this application, so using it would result in undefined behavior.  \nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "628.94 sec"
            },
            {
              "page_number": 146,
              "text": "120\nThis problem is well known and is being addressed by the WG21 C++ Standards \nCommittee Working Group (https://isocpp.org/std/the-committee and http://\nwww.open-std.org/jtc1/sc22/wg21/).\nCurrently, there is no possible way to overcome the object-lifetime obstacle and \nstop relying on undefined behavior from C++ standard’s point of view. libpmemobj-cpp \nis tested and validated with various C++11 compliant compilers and use case scenarios. \nThe only recommendation for libpmemobj-cpp users is that they must keep this \nlimitation in mind when developing persistent memory applications.\n\u0007Trivial Types\nTransactions are the heart of libpmemobj. That is why libpmemobj-cpp was implemented \nwith utmost care while designing the C++ versions so they are as easy to use as possible. \nDevelopers do not have to know the implementation details and do not have to worry about \nsnapshotting modified data to make undo log–based transaction works. A special semi-\ntransparent template property class has been implemented to automatically add variable \nmodifications to the transaction undo log, which is described in the “Snapshotting” section.\nBut what does snapshotting data mean? The answer is very simple, but the \nconsequences for C++ are not. libpmemobj implements snapshotting by copying data of \ngiven length from a specified address to another address using memcpy(). If a transaction \naborts or a system power loss occurs, the data will be written from the undo log when the \nmemory pool is reopened. Consider a definition of the following C++ object, presented \nin Listing 8-4, and think about the consequences that a memcpy() has on it.\nListing 8-4.  An example showing an unsafe memcpy() on an object\n    35    class nonTriviallyCopyable {\n    36    private:\n    37        int* i;\n    38    public:\n    39        nonTriviallyCopyable (const nonTriviallyCopyable & from)\n    40        {\n    41            /* perform non-trivial copying routine */\n    42            i = new int(*from.i);\n    43        }\n    44    };\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "629.46 sec"
            },
            {
              "page_number": 147,
              "text": "121\nDeep and shallow copying is the simplest example. The gist of the problem is that \nby copying the data manually, we may break the inherent behavior of the object which \nmay rely on the copy constructor. Any shared or unique pointer would be another great \nexample – by simple copying it with memcpy(), we break the \"deal\" we made with that \nclass when we used it, and it may lead to leaks or crashes.\nThe application must handle many more sophisticated details when it manually \ncopies the contents of an object. The C++11 standard provides a <type_traits> \ntype trait and std::is_trivially_copyable, which ensure a given type satisfies the \nrequirements of TriviallyCopyable. Referring to C++ standard, an object satisfies the \nTriviallyCopyable requirements when\nA trivially copyable class is a class that:\n— has no non-trivial copy constructors (12.8),\n— has no non-trivial move constructors (12.8),\n— has no non-trivial copy assignment operators (13.5.3, 12.8),\n— has no non-trivial move assignment operators (13.5.3, 12.8), and\n— has a trivial destructor (12.4).\nA trivial class is a class that has a trivial default constructor (12.1) and is \ntrivially copyable.\n[Note: In particular, a trivially copyable or trivial class does not have vir-\ntual functions or virtual base classes.]\nThe C++ standard defines nontrivial methods as follows:\nA copy/move constructor for class X is trivial if it is not user-provided and if\n— class X has no virtual functions (10.3) and no virtual base classes (10.1), \nand\n— the constructor selected to copy/move each direct base class subobject is \ntrivial, and\n— for each non-static data member of X that is of class type (or array \nthereof), the constructor selected to copy/move that member is trivial;\notherwise, the copy/move constructor is non-trivial.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "635.05 sec"
            },
            {
              "page_number": 148,
              "text": "122\nThis means that a copy or move constructor is trivial if it is not user provided.  \nThe class has nothing virtual in it, and this property holds recursively for all the members \nof the class and for the base class. As you can see, the C++ standard and libpmemobj \ntransaction implementation limit the possible objects type to store on persistent \nmemory to satisfy requirements of trivial types, but the layout of our objects must be \ntaken into account.\n\u0007Object Layout\nObject representation, also referred to as the layout, might differ between compilers, \ncompiler flags, and application binary interface (ABI). The compiler may do some \nlayout-related optimizations and is free to shuffle order of members with same specifier \ntype – for example, public then protected, then public again. Another problem related \nto unknown object layout is connected to polymorphic types. Currently there is no \nreliable and portable way to implement vtable rebuilding after reopening the memory \npool, so polymorphic objects cannot be supported with persistent memory.\nIf we want to store objects on persistent memory using memory-mapped files and \nto follow the SNIA NVM programming model, we must ensure that the following casting \nwill be always valid:\nsomeType A = *reinterpret_cast<someType*>(mmap(...));\nThe bit representation of a stored object type must be always the same, and our \napplication should be able to retrieve the stored object from the memory-mapped file \nwithout serialization.\nIt is possible to ensure that specific types satisfy the aforementioned requirements. \nC++11 provides another type trait called std::is_standard_layout. The standard \nmentions that it is useful for communicating with other languages, such as for creating \nlanguage bindings to native C++ libraries as an example, and that's why a standard-­\nlayout class has the same memory layout of the equivalent C struct or union. A general \nrule is that standard-layout classes must have all non-static data members with the same \naccess control. We mentioned this at the beginning of this section – that a C++ compliant \ncompiler is free to shuffle access ranges of the same class definition.\nWhen using inheritance, only one class in the whole inheritance tree can have non-­\nstatic data members, and the first non-static data member cannot be of a base class type \nbecause this could break aliasing rules. Otherwise, it is not a standard-layout class.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "641.96 sec"
            },
            {
              "page_number": 149,
              "text": "123\nThe C++11 standard defines std::is_standard_layout as follows:\nA standard-layout class is a class that:\n— has no non-static data members of type non-standard-layout class (or \narray of such types) or reference,\n— has no virtual functions (10.3) and no virtual base classes (10.1),\n— has the same access control (Clause 11) for all non-static data members,\n— has no non-standard-layout base classes,\n— either has no non-static data members in the most derived class and at \nmost one base class with non-static data members, or has no base classes \nwith non-static data members, and\n— has no base classes of the same type as the first non-static data member.\nA standard-layout struct is a standard-layout class defined with the class-­\nkey struct or the class-key class.\nA standard-layout union is a standard-layout class defined with the class-­\nkey union.\n[ Note: Standard-layout classes are useful for communicating with code \nwritten in other programming languages. Their layout is specified in 9.2.]\nHaving discussed object layouts, we look at another interesting problem with pointer \ntypes and how to store them on persistent memory.\n\u0007Pointers\nIn previous sections, we quoted parts of the C++ standard. We were describing the limits \nof types which were safe to snapshot and copy and which we can binary-cast without \nthinking of fixed layout. But what about pointers? How do we deal with them in our \nobjects as we come to grips with the persistent memory programming model? Consider \nthe code snippet presented in Listing 8-5 which provides an example of a class that uses \na volatile pointer as a class member.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "646.57 sec"
            },
            {
              "page_number": 150,
              "text": "124\nListing 8-5.  Example of class with a volatile pointer as a class member\n    39    struct root {\n    40        int* vptr1;\n    41        int* vptr2;\n    42    };\n    43\n    44    int main(int argc, char *argv[]) {\n    45        auto pop = pmem::obj::pool<root>::open(\"/daxfs/file\", \"tx\");\n    46\n    47        auto r = pop.root();\n    48\n    49        int a1 = 1;\n    50\n    51        pmem::obj::transaction::run(pop, [&](){\n    52            auto ptr = pmem::obj::make_persistent<int>(0);\n    53            r->vptr1 = ptr.get();\n    54            r->vptr2 = &a1;\n    55        });\n    56\n    57        return 0;\n    58    }\n•\t\nLines 39-42: We create a root structure with two volatile pointers as \nmembers.\n•\t\nLines 51-52: Our application is assigning, transactionally, two virtual \naddresses. One to an integer residing on the stack and the second to \nan integer residing on persistent memory. What will happen if the \napplication crashes or exits after execution of the transaction and we \nexecute the application again? Since the variable a1 was residing on \nthe stack, the old value vanished. But what is the value assigned to \nvptr1? Even if it resides on persistent memory, the volatile pointer \nis no longer valid. With ASLR we are not guaranteed to get the same \nvirtual address again if we call mmap(). The pointer could point to \nsomething, nothing, or garbage.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "651.28 sec"
            },
            {
              "page_number": 151,
              "text": "125\nAs shown in the preceding example, it is very important to realize that storing \nvolatile memory pointers in persistent memory is almost always a design error. \nHowever, using the pmem::obj::persistent_ptr<> class template is safe. It provides \nthe only way to safely access specific memory after an application crash. However, \nthe pmem::obj::persistent_ptr<> type does not satisfy TriviallyCopyable \nrequirements because of explicitly defined constructors. As a result, an object with a \npmem::obj::persistent_ptr<> member will not pass the std::is_trivially_copyable \nverification check. Every persistent memory developer should always check whether \npmem::obj::persistent_ptr<> could be copied in that specific case and that it will \nnot cause errors and persistent memory leaks. Developers should realize that std::is_\ntrivially_copyable is a syntax check only and it does not test the semantics. Using \npmem::obj::persistent_ptr<> in this context leads to undefined behavior. There is no \nsingle solution to the problem. At the time of writing this book, the C++ standard does \nnot yet fully support persistent memory programming, so developers must ensure that \ncopying pmem::obj::persistent_ptr<> is safe to use in each case.\n\u0007Limitations Summary\nC++11 provides several very useful type traits for persistent memory programming. \nThese are\n•\t\ntemplate <typename T>\nstruct std::is_pod;\n•\t\ntemplate <typename T>\nstruct std::is_trivial;\n•\t\ntemplate <typename T>\nstruct std::is_trivially_copyable;\n•\t\ntemplate <typename T>\nstruct std::is_standard_layout;\nThey are correlated with each other. The most general and restrictive is the definition \nof a POD type shown in Figure 8-1.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "657.22 sec"
            },
            {
              "page_number": 152,
              "text": "126\nWe mentioned previously that a persistent memory resident class must satisfy the \nfollowing requirements:\n•\t\nstd::is_trivially_copyable\n•\t\nstd::is_standard_layout\nPersistent memory developers are free to use more restrictive type traits if required. \nIf we want to use persistent pointers, however, we cannot rely on type traits, and we \nmust be aware of all problems related to copying objects with memcpy() and the layout \nrepresentation of objects. For persistent memory programming, a format description or \nstandardization of the aforementioned concepts and features needs to take place within \nthe C++ standards body group such that it can be officially designed and implemented. \nUntil then, developers must be aware of the restrictions and limitations to manage \nundefined object-lifetime behavior.\n\u0007Persistence Simplified\nConsider a simple queue implementation, presented in Listing 8-6, which stores \nelements in volatile DRAM.\nListing 8-6.  An implementation of a volatile queue\n    33    #include <cstdio>\n    34    #include <cstdlib>\n    35    #include <iostream>\n    36    #include <string>\n    37\nFigure 8-1.  Correlation between persistent memory–related C++ type traits\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page152_img1.jpeg"
              ],
              "img_summary_files": [],
              "img_vision_files": [
                "output\\images\\img_vision\\Programming_Persistent_Memory_medium_457_page152_img1_vision.json"
              ],
              "summary": "Summary not available due to an error.",
              "time_taken": "661.52 sec"
            },
            {
              "page_number": 153,
              "text": "127\n    38    struct queue_node {\n    39        int value;\n    40        struct queue_node *next;\n    41    };\n    42\n    43    struct queue {\n    44        void\n    45        push(int value)\n    46        {\n    47            auto node = new queue_node;\n    48            node->value = value;\n    49            node->next = nullptr;\n    50\n    51            if (head == nullptr) {\n    52                head = tail = node;\n    53            } else {\n    54                tail->next = node;\n    55                tail = node;\n    56            }\n    57        }\n    58\n    59        int\n    60        pop()\n    61        {\n    62            if (head == nullptr)\n    63                throw std::out_of_range(\"no elements\");\n    64\n    65            auto head_ptr = head;\n    66            auto value = head->value;\n    67\n    68            head = head->next;\n    69            delete head_ptr;\n    70\n    71            if (head == nullptr)\n    72                tail = nullptr;\n    73\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "664.28 sec"
            },
            {
              "page_number": 154,
              "text": "128\n    74            return value;\n    75        }\n    76\n    77        void\n    78        show()\n    79        {\n    80            auto node = head;\n    81            while (node != nullptr) {\n    82                std::cout << \"show: \" << node->value << std::endl;\n    83                node = node->next;\n    84            }\n    85\n    86            std::cout << std::endl;\n    87        }\n    88\n    89    private:\n    90        queue_node *head = nullptr;\n    91        queue_node *tail = nullptr;\n    92    };\n•\t\nLines 38-40: We declare layout of the queue_node structure. It stores \nan integer value and a pointer to the next node in the list.\n•\t\nLines 44-57: We implement push() method which allocates new \nnode and sets its value.\n•\t\nLines 59-75: We implement pop() method which deletes the first \nelement in the queue.\n•\t\nLines 77-87: The show() method walks the list and prints the contents \nof each node to standard out.\nThe preceding queue implementation stores values of type int in a linked list and \nprovides three basic methods: push(), pop(), and show().\nIn this section, we will demonstrate how to modify your volatile structure to store \nelements in persistent memory with libpmemobj-cpp bindings. All the modifier methods \nshould provide atomicity and consistency properties which will be guaranteed by the \nuse of transactions.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "669.91 sec"
            },
            {
              "page_number": 155,
              "text": "129\nChanging a volatile application to start taking advantage of persistent memory \nshould rely on modifying structures and classes with only slight modifications to \nfunctions. We will begin by modifying the queue_node structure by changing its layout as \nshown in Listing 8-7.\nListing 8-7.  A persistent queue implementation – modifying the queue_node struct\n    38    #include <libpmemobj++/make_persistent.hpp>\n    39    #include <libpmemobj++/p.hpp>\n    40    #include <libpmemobj++/persistent_ptr.hpp>\n    41    #include <libpmemobj++/pool.hpp>\n    42    #include <libpmemobj++/transaction.hpp>\n    43\n    44    struct queue_node {\n    45        pmem::obj::p<int> value;\n    46        pmem::obj::persistent_ptr<queue_node> next;\n    47    };\n    48\n    49    struct queue {\n   ...\n   100    private:\n   101        pmem::obj::persistent_ptr<queue_node> head = nullptr;\n   102        pmem::obj::persistent_ptr<queue_node> tail = nullptr;\n   103    };\nAs you can see, all the modifications are limited to replace the volatile pointers with \npmem:obj::persistent_ptr and to start using the p<> property.\nNext, we modify a push() method, shown in Listing 8-8.\nListing 8-8.  A persistent queue implementation – a persistent push() method\n    50        void\n    51        push(pmem::obj::pool_base &pop, int value)\n    52        {\n    53            pmem::obj::transaction::run(pop, [&]{\n    54                auto node = pmem::obj::make_persistent<queue_node>();\n    55                node->value = value;\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "674.52 sec"
            },
            {
              "page_number": 156,
              "text": "130\n    56                node->next = nullptr;\n    57\n    58                if (head == nullptr) {\n    59                    head = tail = node;\n    60                } else {\n    61                    tail->next = node;\n    62                    tail = node;\n    63                }\n    64            });\n    65        }\nAll the modifiers methods must be aware on which persistent memory pool they \nshould operate on. For a single memory pool, this is trivial, but if the application \nmemory maps files from different file systems, we need to keep track of which pool has \nwhat data. We introduce an additional argument of type pmem::obj::pool_base to solve \nthis problem. Inside the method definition, we are wrapping the code with a transaction \nby using a C++ lambda expression, [&], to guarantee atomicity and consistency of \nmodifications. Instead of allocating a new node on the stack, we call pmem::obj::make_\npersistent<>() to transactionally allocate it on persistent memory.\nListing 8-9 shows the modification of the pop() method.\nListing 8-9.  A persistent queue implementation – a persistent pop() method\n    67        int\n    68        pop(pmem::obj::pool_base &pop)\n    69        {\n    70            int value;\n    71            pmem::obj::transaction::run(pop, [&]{\n    72                if (head == nullptr)\n    73                    throw std::out_of_range(\"no elements\");\n    74\n    75                auto head_ptr = head;\n    76                value = head->value;\n    77\n    78                head = head->next;\n    79                pmem::obj::delete_persistent<queue_node>(head_ptr);\n    80\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "681.69 sec"
            },
            {
              "page_number": 157,
              "text": "131\n    81                if (head == nullptr)\n    82                    tail = nullptr;\n    83            });\n    84\n    85            return value;\n    86        }\nThe logic of pop() is wrapped within a libpmemobj-cpp transaction. The only \nadditional modification is to exchange call to volatile delete with transactional \npmem::obj::delete_persistent<>().\nThe show() method does not modify anything on either volatile DRAM or persistent \nmemory, so we do not need to make any changes to it since the pmem:obj::persistent_\nptr implementation provides operator->.\nTo start using the persistent version of this queue example, our application can \nassociate it with a root object. Listing 8-10 presents an example application that uses our \npersistent queue.\nListing 8-10.  Example of application that uses a persistent queue\n    39    #include \"persistent_queue.hpp\"\n    40\n    41    enum queue_op {\n    42        PUSH,\n    43        POP,\n    44        SHOW,\n    45        EXIT,\n    46        MAX_OPS,\n    47    };\n    48\n    49    const char *ops_str[MAX_OPS] = {\"push\", \"pop\", \"show\", \"exit\"};\n    50\n    51    queue_op\n    52    parse_queue_ops(const std::string &ops)\n    53    {\n    54        for (int i = 0; i < MAX_OPS; i++) {\n    55            if (ops == ops_str[i]) {\n    56                return (queue_op)i;\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "685.28 sec"
            },
            {
              "page_number": 158,
              "text": "132\n    57            }\n    58        }\n    59        return MAX_OPS;\n    60    }\n    61\n    62    int\n    63    main(int argc, char *argv[])\n    64    {\n    65        if (argc < 2) {\n    66            \u0007std::cerr << \"Usage: \" << argv[0] << \" path_to_pool\"  \n<< std::endl;\n    67            return 1;\n    68        }\n    69\n    70        auto path = argv[1];\n    71        pmem::obj::pool<queue> pool;\n    72\n    73        try {\n    74            pool = pmem::obj::pool<queue>::open(path, \"queue\");\n    75        } catch(pmem::pool_error &e) {\n    76            std::cerr << e.what() << std::endl;\n    77            \u0007std::cerr << \"To create pool run: pmempool create obj \n--layout=queue -s 100M path_to_pool\" << std::endl;\n    78        }\n    79\n    80        auto q = pool.root();\n    81\n    82        while (1) {\n    83            std::cout << \"[push value|pop|show|exit]\" << std::endl;\n    84\n    85            std::string command;\n    86            std::cin >> command;\n    87\n    88            // parse string\n    89            auto ops = parse_queue_ops(std::string(command));\n    90\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "690.08 sec"
            },
            {
              "page_number": 159,
              "text": "133\n    91            switch (ops) {\n    92                case PUSH: {\n    93                    int value;\n    94                    std::cin >> value;\n    95\n    96                    q->push(pool, value);\n    97\n    98                    break;\n    99                }\n   100                case POP: {\n   101                    std::cout << q->pop(pool) << std::endl;\n   102                    break;\n   103                }\n   104                case SHOW: {\n   105                    q->show();\n   106                    break;\n   107                }\n   108                case EXIT: {\n   109                    exit(0);\n   110                }\n   111                default: {\n   112                    std::cerr << \"unknown ops\" << std::endl;\n   113                    exit(0);\n   114                }\n   115            }\n   116        }\n   117    }\n\u0007The Ecosystem\nThe overall goal for the libpmemobj C++ bindings was to create a friendly and less \nerror-prone API for persistent memory programming. Even with persistent memory \npool allocators, a convenient interface for creating and managing transactions,  \nauto-­snapshotting class templates and smart persistent pointers, and designing  \nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "694.67 sec"
            },
            {
              "page_number": 160,
              "text": "134\nan application with persistent memory usage may still prove challenging without \na lot of niceties that the C++ programmers are used to. The natural step forward to \nmake persistent programming easier was to provide programmers with efficient and \nuseful containers.\n\u0007Persistent Containers\nThe C++ standard library containers collection is something that persistent memory \nprogrammers may want to use. Containers manage the lifetime of held objects \nthrough allocation/creation and deallocation/destruction with the use of allocators. \nImplementing custom persistent allocator for C++ STL (Standard Template Library) \ncontainers has two main downsides:\n•\t\nImplementation details:\n•\t\nSTL containers do not use algorithms optimal for a persistent \nmemory programming point of view.\n•\t\nPersistent memory containers should have durability and \nconsistency properties, while not every STL method guarantees \nstrong exception safety.\n•\t\nPersistent memory containers should be designed with an \nawareness of fragmentation limitations.\n•\t\nMemory layout:\n•\t\nThe STL does not guarantee that the container layout will remain \nunchanged in new library versions.\nDue to these obstacles, the libpmemobj-cpp contains the set of custom, \nimplemented-from-scratch, containers with optimized on-media layouts and \nalgorithms to fully exploit the potential and features of persistent memory. These \nmethods guarantee atomicity, consistency, and durability. Besides specific internal \nimplementation details, libpmemobj-cpp persistent memory containers have a well-­\nknown STL-like interface, and they work with STL algorithms.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "699.30 sec"
            },
            {
              "page_number": 161,
              "text": "135\n\u0007Examples of Persistent Containers\nSince the main goal for the libpmemobj-cpp design is to focus modifications to volatile \nprograms on data structures and not on the code, the use of libpmemobj-cpp persistent \ncontainers is almost the same as for their STL counterparts. Listing 8-11 shows a \npersistent vector example to showcase this.\nListing 8-11.  Allocating a vector transactionally using persistent containers\n    33    #include <libpmemobj++/make_persistent.hpp>\n    34    #include <libpmemobj++/transaction.hpp>\n    35    #include <libpmemobj++/persistent_ptr.hpp>\n    36    #include <libpmemobj++/pool.hpp>\n    37    #include \"libpmemobj++/vector.hpp\"\n    38\n    39    using vector_type = pmem::obj::experimental::vector<int>;\n    40\n    41    struct root {\n    42            pmem::obj::persistent_ptr<vector_type> vec_p;\n    43    };\n    44\n              ...\n    63\n    64        /* creating pmem::obj::vector in transaction */\n    65        pmem::obj::transaction::run(pool, [&] {\n    66            \u0007root->vec_p = pmem::obj::make_persistent<vector_type> \n(/* optional constructor arguments */);\n    67        });\n    68\n    69        vector_type &pvector = *(root->vec_p);\nListing 8-11 shows that a pmem::obj::vector must be created and allocated in \npersistent memory using transaction to avoid an exception being thrown. The vector \ntype constructor may construct an object by internally opening another transaction. \nIn this case, an inner transaction will be flattened to an outer one. The interface and \nsemantics of pmem::obj::vector are similar to that of std::vector, as Listing 8-12 \ndemonstrates.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "703.91 sec"
            },
            {
              "page_number": 162,
              "text": "136\nListing 8-12.  Using persistent containers\n    71        pvector.reserve(10);\n    72        assert(pvector.size() == 0);\n    73        assert(pvector.capacity() == 10);\n    74\n    75        pvector = {0, 1, 2, 3, 4};\n    76        assert(pvector.size() == 5);\n    77        assert(pvector.capacity() == 10);\n    78\n    79        pvector.shrink_to_fit();\n    80        assert(pvector.size() == 5);\n    81        assert(pvector.capacity() == 5);\n    82\n    83        for (unsigned i = 0; i < pvector.size(); ++i)\n    84            assert(pvector.const_at(i) == static_cast<int>(i));\n    85\n    86        pvector.push_back(5);\n    87        assert(pvector.const_at(5) == 5);\n    88        assert(pvector.size() == 6);\n    89\n    90        pvector.emplace(pvector.cbegin(), pvector.back());\n    91        assert(pvector.const_at(0) == 5);\n    92        for (unsigned i = 1; i < pvector.size(); ++i)\n    93            assert(pvector.const_at(i) == static_cast<int>(i - 1));\nEvery method that modifies persistent memory containers does so inside an implicit \ntransaction to guarantee full exception safety. If any of these methods are called inside \nthe scope of another transaction, the operation is performed in the context of that \ntransaction; otherwise, it is atomic in its own scope.\nIterating over pmem::obj::vector works exactly the same as std::vector. We can \nuse the range-based indexing operator for loops or iterators. The pmem::obj::vector \ncan also be processed using std::algorithms, as shown in Listing 8-13.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "709.54 sec"
            },
            {
              "page_number": 163,
              "text": "137\nListing 8-13.  Iterating over persistent container and compatibility with STD \nalgorithms\n    95        std::vector<int> stdvector = {5, 4, 3, 2, 1};\n    96        pvector = stdvector;\n    97\n    98        try {\n    99            pmem::obj::transaction::run(pool, [&] {\n   100                for (auto &e : pvector)\n   101                    e++;\n   102                /* 6, 5, 4, 3, 2 */\n   103\n   104                \u0007for (auto it = pvector.begin();  \nit != pvector.end(); it++)\n   105                    *it += 2;\n   106                /* 8, 7, 6, 5, 4 */\n   107\n   108                for (unsigned i = 0; i < pvector.size(); i++)\n   109                    pvector[i]--;\n   110                /* 7, 6, 5, 4, 3 */\n   111\n   112                std::sort(pvector.begin(), pvector.end());\n   113                for (unsigned i = 0; i < pvector.size(); ++i)\n   114                    \u0007assert(pvector.const_at(i) == static_cast<int> \n(i + 3));\n   115\n   116                pmem::obj::transaction::abort(0);\n   117            });\n   118        } catch (pmem::manual_tx_abort &) {\n   119            /* expected transaction abort */\n   120        } catch (std::exception &e) {\n   121            std::cerr << e.what() << std::endl;\n   122        }\n   123\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "715.18 sec"
            },
            {
              "page_number": 164,
              "text": "138\n   124        \u0007assert(pvector == stdvector); /* pvector element's value was \nrolled back */\n   125\n   126        try {\n   127            pmem::obj::delete_persistent<vector_type>(&pvector);\n   128        } catch (std::exception &e) {\n   129        }\nIf an active transaction exists, elements accessed using any of the preceding methods \nare snapshotted. When iterators are returned by begin() and end(), snapshotting \nhappens during the iterator dereferencing phase. Note that snapshotting is done only \nfor mutable elements. In the case of constant iterators or constant versions of indexing \noperator, nothing is added to the transaction. That is why it is essential to use const \nqualified function overloads such as cbegin() or cend() whenever possible. If an object \nsnapshot occurs in the current transaction, a second snapshot of the same memory \naddress will not be performed and thus will not have performance overhead. This will \nreduce the number of snapshots and can significantly reduce the performance impact \nof transactions. Note also that pmem::obj::vector does define convenient constructors \nand compare operators that take std::vector as an argument.\n\u0007Summary\nThis chapter describes the libpmemobj-cpp library. It makes creating applications less \nerror prone, and its similarity to standard C++ API makes it easier to modify existing \nvolatile programs to use persistent memory. We also list the limitations of this library \nand the problems you must consider during development.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "719.78 sec"
            },
            {
              "page_number": 165,
              "text": "139\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter's \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter's Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 8  libpmemobj-cpp: The Adaptable Language - C++ and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "722.44 sec"
            },
            {
              "page_number": 166,
              "text": "141\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_9\nCHAPTER 9\npmemkv: A Persistent In-­\nMemory Key-Value Store\nProgramming persistent memory is not easy. In several chapters we have described \nthat applications that take advantage of persistent memory must take responsibility \nfor atomicity of operations and consistency of data structures. PMDK libraries like \nlibpmemobj are designed with flexibility and simplicity in mind. Usually, these are \nconflicting requirements, and one has to be sacrificed for the sake of the other. The truth \nis that in most cases, an API’s flexibility increases its complexity.\nIn the current cloud computing ecosystem, there is an unpredictable demand \nfor data. Consumers expect web services to provide data with predicable low-latency \nreliability. Persistent memory’s byte addressability and huge capacity characteristics \nmake this technology a perfect fit for the broadly defined cloud environment.\nToday, as greater numbers of devices with greater levels of intelligence are \nconnected to various networks, businesses and consumers are finding the cloud to \nbe an increasingly attractive option that enables fast, ubiquitous access to their data. \nIncreasingly, consumers are fine with lower storage capacity on endpoint devices in \nfavor of using the cloud. By 2020, IDC predicts that more bytes will be stored in the \npublic cloud than in consumer devices (Figure 9-1).\nFigure 9-1.  Where is data stored? Source: IDC White Paper – #US44413318",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page166_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page166_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "726.34 sec"
            },
            {
              "page_number": 167,
              "text": "142\nThe cloud ecosystem, its modularity, and variety of service modes define \nprogramming and application deployment as we know it. We call it cloud-native \ncomputing, and its popularity results in a growing number of high-level languages, \nframeworks, and abstraction layers. Figure 9-2 shows the 15 most popular languages on \nGitHub based on pull requests.\nIn cloud environments, the platform is typically virtualized, and applications are \nheavily abstracted as to not make explicit assumptions about low-level hardware details. \nThe question is: how to make programming persistent memory easier in cloud-native \nenvironment given the physical devices are local only to a specific server?\nOne of the answers is a key-value store. This data storage paradigm designed for \nstoring, retrieving, and managing associative arrays with straightforward API can easily \nutilize the advantages of persistent memory. This is why pmemkv was created.\nFigure 9-2.  The 15 most popular languages on GitHub by opened pull request \n(2017). Source: https://octoverse.github.com/2017/\nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page167_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page167_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "730.33 sec"
            },
            {
              "page_number": 168,
              "text": "143\n\u0007pmemkv Architecture\nThere are many key-value data stores available on the market. They have different \nfeatures and licenses and their APIs are targeting different use cases. However, their core \nAPI remains the same. All of them provide methods like put, get, remove, exists, open, \nand close. At the time we published this book, the most popular key-value data store \nis Redis. It is available in open source (https://redis.io/) and enterprise (https://\nredislabs.com) versions. DB-Engines (https://db-engines.com) shows that Redis has \na significantly higher rank than any of its competitors in this sector.\nPmemkv was created as a separate project not only to complement PMDK’s set \nof libraries with cloud-native support but also to provide a key-value API built for \npersistent memory. One of the main goals for pmemkv developers was to create friendly \nenvironment for open source community to develop new engines with the help of \nPMDK and to integrate it with other programming languages. Pmemkv uses the same \nBSD 3-Clause permissive license as PMDK. The native API of pmemkv is C and C++. \nOther programming language bindings are available such as JavaScript, Java, and Ruby. \nAdditional languages can easily be added.\nFigure 9-3.  DB-Engines ranking of key-value stores (July 2019). Scoring method: \nhttps://db-engines.com/en/ranking_definition. Source: https://db-­\nengines.com/en/ranking/key-value+store\nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page168_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page168_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "734.42 sec"
            },
            {
              "page_number": 169,
              "text": "144\nThe pmemkv API is similar to most key-value databases. Several storage engines \nare available for flexibility and functionality. Each engine has different performance \ncharacteristics and aims to solve different problems. Because of that, the functionality \nprovided by each engine differs. They can be described by the following characteristics:\n•\t\nPersistence: Persistent engines guarantee that modifications are \nretained and power-fail safe, while volatile ones keep its content only \nfor the application lifetime.\n•\t\nConcurrency: Concurrent engines guarantee that some methods \nsuch as get()/put()/remove() are thread-safe.\n•\t\nKeys’ ordering: \"Sorted\" engines provide range query methods (like \nget_above()).\nWhat makes pmemkv different from other key-value databases is that it provides \ndirect access to the data. This means reading data from persistent memory does not \nrequire a copy into DRAM. This was already mentioned in Chapter 1 and is presented \nagain in Figure 9-5.\nFigure 9-4.  The architecture of pmemkv and programming languages support\nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page169_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page169_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "738.32 sec"
            },
            {
              "page_number": 170,
              "text": "145\nHaving direct access to the data significantly speeds up the application. This benefit \nis most noticeable in situations where the program is only interested in a part of the \ndata stored in the database. In conventional approaches, this would require copying the \nwhole data in some buffer and returning it to the application. With pmemkv, we provide \nthe application a direct pointer, and the application reads only as much as it is needed.\nTo make the API fully functional with various engine types, a flexible pmemkv_config \nstructure was introduced. It stores engine configuration options and allows you to \ntune its behavior. Every engine has documented all supported config parameters. The \npmemkv library was designed in a way that engines are pluggable and extendable \nto support the developers own requirements. Developers are free to modify existing \nengines or contribute new ones (https://github.com/pmem/pmemkv/blob/master/\nCONTRIBUTING.md#engines).\nListing 9-1 shows a basic setup of the pmemkv_config structure using the native C \nAPI. All the setup code is wrapped around the custom function, config_setup(), which \nwill be used in a phonebook example in the next section. You can see how error handling \nis solved in pmemkv – all methods, except for pmemkv_close() and pmemkv_errormsg(), \nreturn a status. We can obtain error message using the pmemkv_errormsg() function. A \ncomplete list of return values can be found in pmemkv man page.\nFigure 9-5.  Applications directly accessing data in place using pmemkv\nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page170_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page170_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "742.21 sec"
            },
            {
              "page_number": 171,
              "text": "146\nListing 9-1.  pmemkv_config.h – An example of the pmemkv_config structure \nusing the C API\n1    #include <cstdio>\n2    #include <cassert>\n3    #include <libpmemkv.h>\n4\n5    \u0007pmemkv_config* config_setup(const char* path, const uint64_t fcreate, \nconst uint64_t size) {\n6        pmemkv_config *cfg = pmemkv_config_new();\n7        assert(cfg != nullptr);\n8\n9        if (pmemkv_config_put_string(cfg, \"path\", path) != PMEMKV_STATUS_OK) {\n10            fprintf(stderr, \"%s\", pmemkv_errormsg());\n11            return NULL;\n12       }\n13\n14       \u0007if (pmemkv_config_put_uint64(cfg, \"force_create\", fcreate) != \nPMEMKV_STATUS_OK) {\n15            fprintf(stderr, \"%s\", pmemkv_errormsg());\n16            return NULL;\n17       }\n18\n19       if (pmemkv_config_put_uint64(cfg, \"size\", size) != PMEMKV_STATUS_OK) {\n20            fprintf(stderr, \"%s\", pmemkv_errormsg());\n21            return NULL;\n22       }\n23\n24       return cfg;\n25    }\n•\t\nLine 5: We define custom function to prepare config and set all \nrequired params for engine(s) to use.\n•\t\nLine 6: We create an instance of C config class. It returns nullptr on \nfailure.\nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "745.90 sec"
            },
            {
              "page_number": 172,
              "text": "147\n•\t\nLine 9-22: All params are put into config (the cfg instance) one after \nanother (using function dedicated for the type), and each is checked \nif was stored successful (PMEMKV_STATUS_OK is returned when no \nerrors occurred).\n\u0007A Phonebook Example\nListing 9-2 shows a simple phonebook example implemented using the pmemkv C++ \nAPI v0.9. One of the main intentions of pmemkv is to provide a familiar API similar to \nthe other key-value stores. This makes it very intuitive and easy to use. We will reuse the \nconfig_setup() function from Listing 9-1.\nListing 9-2.  A simple phonebook example using the pmemkv C++ API\n 37    #include <iostream>\n 38    #include <cassert>\n 39    #include <libpmemkv.hpp>\n 40    #include <string>\n 41    #include \"pmemkv_config.h\"\n 42\n 43    using namespace pmem::kv;\n 44\n 45    auto PATH = \"/daxfs/kvfile\";\n 46    const uint64_t FORCE_CREATE = 1;\n 47    const uint64_t SIZE = 1024 ∗ 1024 ∗ 1024; // 1 Gig\n 48\n 49    int main() {\n 50        // Prepare config for pmemkv database\n 51        pmemkv_config ∗cfg = config_setup(PATH, FORCE_CREATE, SIZE);\n 52        assert(cfg != nullptr);\n 53\n 54        // Create a key-value store using the \"cmap\" engine.\n 55        db kv;\n 56\n 57        if (kv.open(\"cmap\", config(cfg)) != status::OK) {\n 58            std::cerr << db::errormsg() << std::endl;\nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "751.73 sec"
            },
            {
              "page_number": 173,
              "text": "148\n 59            return 1;\n 60        }\n 61\n 62        // Add 2 entries with name and phone number\n 63        if (kv.put(\"John\", \"123-456-789\") != status::OK) {\n 64            std::cerr << db::errormsg() << std::endl;\n 65            return 1;\n 66        }\n 67        if (kv.put(\"Kate\", \"987-654-321\") != status::OK) {\n 68            std::cerr << db::errormsg() << std::endl;\n 69            return 1;\n 70        }\n 71\n 72        // Count elements\n 73        size_t cnt;\n 74        if (kv.count_all(cnt) != status::OK) {\n 75            std::cerr << db::errormsg() << std::endl;\n 76            return 1;\n 77        }\n 78        assert(cnt == 2);\n 79\n 80        // Read key back\n 81        std::string number;\n 82        if (kv.get(\"John\", &number) != status::OK) {\n 83            std::cerr << db::errormsg() << std::endl;\n 84            return 1;\n 85        }\n 86        assert(number == \"123-456-789\");\n 87\n 88        // Iterate through the phonebook\n 89        if (kv.get_all([](string_view name, string_view number) {\n 90                std::cout << \"name: \" << name.data() <<\n 91                \", number: \" << number.data() << std::endl;\n 92                return 0;\n 93                }) != status::OK) {\nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "756.34 sec"
            },
            {
              "page_number": 174,
              "text": "149\n 94            std::cerr << db::errormsg() << std::endl;\n 95            return 1;\n 96        }\n 97\n 98        // Remove one record\n 99        if (kv.remove(\"John\") != status::OK) {\n100            std::cerr << db::errormsg() << std::endl;\n101            return 1;\n102        }\n103\n104        // Look for removed record\n105        assert(kv.exists(\"John\") == status::NOT_FOUND);\n106\n107        // Try to use one of methods of ordered engines\n108        \u0007assert(kv.get_above(\"John\", [](string_view key, string_view \nvalue) {\n109            \u0007std::cout << \"This callback should never be called\" << \nstd::endl;\n110            return 1;\n111        }) == status::NOT_SUPPORTED);\n112\n113        // Close database (optional)\n114        kv.close();\n115\n116        return 0;\n117    }\n•\t\nLine 51: We set the pmemkv_config structure by calling config_\nsetup() function introduced in previous section and listing \n(imported with #include \"pmemkv_config.h\").\n•\t\nLine 55: Creates a volatile object instance of the class pmem::kv::db \nwhich provides interface for managing persistent database.\n•\t\nLine 57: Here, we open the key-value database backed by the cmap \nengine using the config parameters. The cmap engine is a persistent \nconcurrent hash map engine, implemented in libpmemobj-cpp. \nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "762.07 sec"
            },
            {
              "page_number": 175,
              "text": "150\nYou can read more about cmap engine internal algorithms and data \nstructures in Chapter 13.\n•\t\nLine 58: The pmem::kv::db class provides a static errormsg() method \nfor extended error messages. In this example, we use the errormsg() \nfunction as a part of the error-handling routine.\n•\t\nLine 63 and 67: The put() method inserts a key-value pair into the \ndatabase. This function is guaranteed to be implemented by all \nengines. In this example, we are inserting two key-value pairs into \ndatabase and compare returned statuses with status::OK. It’s a \nrecommended way to check if function succeeded.\n•\t\nLine 74: The count_all() has a single argument of type size_t. The \nmethod returns the number of elements (phonebook entries) stored \nin the database by the argument variable (cnt).\n•\t\nLine 82: Here, we use the get() method to return the value of the \n“John” key. The value is copied into the user-provided number \nvariable. The get() function returns status::OK on success or an \nerror on failure. This function is guaranteed to be implemented by all \nengines.\n•\t\nLine 86: For this example, the expected value of variable number for \n“John” is “123-456-789”. If we do not get this value, an assertion error \nis thrown.\n•\t\nLine 89: The get_all() method used in this example gives the \napplication direct, read-only access to the data. Both key and value \nvariables are references to data stored in persistent memory. In this \nexample, we simply print the name and the number of every visited pair.\n•\t\nLine 99: Here, we are removing “John” and his phone number from \nthe database by calling the remove() method. It is guaranteed to be \nimplemented by all engines.\n•\t\nLine 105: After removal of the pair “John, 123-456-789”, we verify if \nthe pair still exists in database. The API method exists() checks \nthe existence of an element with given key. If the element is present, \nstatus::OK is returned; otherwise status::NOT_FOUND is returned.\nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "768.12 sec"
            },
            {
              "page_number": 176,
              "text": "151\n•\t\nLine 108: Not every engine provides implementations of all the \navailable API methods. In this example, we used the cmap engine, \nwhich is unordered engine type. This is why cmap does not \nsupport the get_above() function (and similarly: get_below(), \nget_between(), count_above(), count_below(), count_between()). \nCalling these functions will return status::NOT_SUPPORTED.\n•\t\nLine 114: Finally, we are calling the close() method to close \ndatabase. Calling this function is optional because kv was \nallocated on the stack and all necessary destructors will be called \nautomatically, just like for the other variables residing on stack.\n\u0007Bringing Persistent Memory Closer to the Cloud\nWe will rewrite the phonebook example using the JavaScript language bindings. There \nare several language bindings available for pmemkv – JavaScript, Java, Ruby, and Python. \nHowever, not all provide the same API functionally equivalent to the native C and C++ \ncounterparts. Listing 9-3 shows an implementation of the phonebook application \nwritten using JavaScript language bindings API.\nListing 9-3.  A simple phonebook example written using the JavaScript bindings \nfor pmemkv v0.8\n    1    const Database = require('./lib/all');\n    2\n    3    function assert(condition) {\n    4        if (!condition) throw new Error('Assert failed');\n    5    }\n    6\n    7    console.log('Create a key-value store using the \"cmap\" engine');\n    8    \u0007const db = new Database('cmap', '{\"path\":\"/daxfs/\nkvfile\",\"size\":1073741824, \"force_create\":1}');\n    9\n    10    console.log('Add 2 entries with name and phone number');\n    11    db.put('John', '123-456-789');\n    12    db.put('Kate', '987-654-321');\n    13\nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "772.72 sec"
            },
            {
              "page_number": 177,
              "text": "152\n    14    console.log('Count elements');\n    15    assert(db.count_all == 2);\n    16\n    17    console.log('Read key back');\n    18    assert(db.get('John') === '123-456-789');\n    19\n    20    console.log('Iterate through the phonebook');\n    21    db.get_all((k, v) => console.log(`  name: ${k}, number: ${v}`));\n    22\n    23    console.log('Remove one record');\n    24    db.remove('John');\n    25\n    26    console.log('Lookup of removed record');\n    27    assert(!db.exists('John'));\n    28\n    29    console.log('Stopping engine');\n    30    db.stop();\nThe goal of higher-level pmemkv language bindings is to make programming \npersistent memory even easier and to provide a convenient tool for developers of cloud \nsoftware.\n\u0007Summary\nIn this chapter, we have shown how a familiar key-value data store is an easy way for the \nbroader cloud software developer audience to use persistent memory and directly access \nthe data in place. The modular design, flexible engine API, and integration with many \nof the most popular cloud programming languages make pmemkv an intuitive choice \nfor cloud-native software developers. As an open source and lightweight library, it can \neasily be integrated into existing applications to immediately start taking advantage of \npersistent memory.\nSome of the pmemkv engines are implemented using libpmemobj-cpp that we \ndescribed in Chapter 8. The implementation of such engines provides real-world \nexamples for developers to understand how to use PMDK (and related libraries) in \napplications.\nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "777.33 sec"
            },
            {
              "page_number": 178,
              "text": "153\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 9  pmemkv: A Persistent In-­Memory Key-Value Store",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "781.12 sec"
            },
            {
              "page_number": 179,
              "text": "155\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_10\nCHAPTER 10\nVolatile Use of Persistent \nMemory\n\u0007Introduction\nThis chapter discusses how applications that require a large quantity of volatile memory \ncan leverage high-capacity persistent memory as a complementary solution to dynamic \nrandom-access memory (DRAM).\nApplications that work with large data sets, like in-memory databases, caching \nsystems, and scientific simulations, are often limited by the amount of volatile \nmemory capacity available in the system or the cost of the DRAM required to load a \ncomplete data set. Persistent memory provides a high capacity memory tier to solve \nthese memory-hungry application problems. \nIn the memory-storage hierarchy (described in Chapter 1), data is stored in tiers with \nfrequently accessed data placed in DRAM for low-latency access, and less frequently \naccessed data is placed in larger capacity, higher latency storage devices. Examples of \nsuch solutions include Redis on Flash (https://redislabs.com/redis-enterprise/\ntechnology/redis-on-flash/) and Extstore for Memcached (https://memcached.org/\nblog/extstore-cloud/).\nFor memory-hungy applications that do not require persistence, using the larger \ncapacity persistent memory as volatile memory provides new opportunities and \nsolutions.\nUsing persistent memory as a volatile memory solution is advantageous when an \napplication: \n•\t\nHas control over data placement between DRAM and other storage \ntiers within the system\n•\t\nDoes not need to persist data",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "784.70 sec"
            },
            {
              "page_number": 180,
              "text": "156\n•\t\nCan use the native latencies of persistent memory, which may be \nslower than DRAM but are faster than non-volatile memory express \n(NVMe) solid-state drives (SSDs).\n\u0007Background\nApplications manage different kinds of data structures such as user data, key-value \nstores, metadata, and working buffers. Architecting a solution that uses tiered memory \nand storage may enhance application performance, for example, placing objects that \nare accessed frequently and require low-latency access in DRAM while storing objects \nthat require larger allocations that are not as latency-sensitive on persistent memory. \nTraditional storage devices are used to provide persistence. \n\u0007Memory Allocation\nAs described in Chapters 1 through 3, persistent memory is exposed to the application \nusing memory-mapped files on a persistent memory-aware file system that provides \ndirect access to the application. Since malloc() and free() do not operate on different \ntypes of memory or memory-mapped files, an interface is needed that provides malloc() \nand free() semantics for multiple memory types. This interface is implemented as the \nmemkind library (http://memkind.github.io/memkind/).\n\u0007How it Works\nThe memkind library is a user-extensible heap manager built on top of jemalloc, which \nenables partitioning of the heap between multiple kinds of memory. Memkind was \ncreated to support different kinds of memory when high bandwidth memory (HBM) was \nintroduced. A PMEM kind was introduced to support persistent memory.\nDifferent “kinds” of memory are defined by the operating system memory policies \nthat are applied to virtual address ranges. Memory characteristics supported by \nmemkind without user extension include the control of non-uniform memory access \n(NUMA) and page sizes. Figure 10-1 shows an overview of libmemkind components and \nhardware support.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "790.54 sec"
            },
            {
              "page_number": 181,
              "text": "157\nThe memkind library serves as a wrapper that redirects memory allocation requests \nfrom an application to an allocator that manages the heap. At the time of publication, \nonly the jemalloc allocator is supported. Future versions may introduce and support \nmultiple allocators. Memkind provides jemalloc with different kinds of memory: A static \nkind is created automatically, whereas a dynamic kind is created by an application using \nmemkind_create_kind().\n\u0007Supported “Kinds” of Memory\nThe dynamic PMEM kind is best used with memory-addressable persistent storage \nthrough a DAX-enabled file system that supports load/store operations that are \nnot paged via the system page cache. For the PMEM kind, the memkind library supports \nthe traditional malloc/free-like interfaces on a memory-mapped file. When an \napplication calls memkind_create_kind() with PMEM, a temporary file (tmpfile(3)) \nis created on a mounted DAX file system and is memory-mapped into the application’s \nvirtual address space. This temporary file is deleted automatically when the program \nterminates, giving the perception of volatility. \nFigure 10-2 shows memory mappings from two memory sources: DRAM  \n(MEMKIND_DEFAULT) and persistent memory (PMEM_KIND).\nFor allocations from DRAM, rather than using the common malloc(), the \napplication can call memkind_malloc() with the kind argument set to MEMKIND_DEFAULT. \nMEMKIND_DEFAULT is a static kind that uses the operating system’s default page size for \nallocations. Refer to the memkind documentation for large and huge page support.\nFigure 10-1.  An overview of the memkind components and hardware support\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page181_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page181_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "794.54 sec"
            },
            {
              "page_number": 182,
              "text": "158\nWhen using libmemkind with DRAM and persistent memory, the key points to \nunderstand are: \n•\t\nTwo pools of memory are available to the application, one from \nDRAM and another from persistent memory.\n•\t\nBoth pools of memory can be accessed simultaneously by setting \nthe kind type to PMEM_KIND to use persistent memory and MEMKIND_\nDEFAULT to use DRAM.\n•\t\njemalloc is the single memory allocator used to manage all kinds of \nmemory.\n•\t\nThe memkind library is a wrapper around jemalloc that provides a \nunified API for allocations from different kinds of memory.\n•\t\nPMEM_KIND memory allocations are provided by a temporary file \n(tmpfile(3)) created on a persistent memory-aware file system. \nThe file is destroyed when the application exits. Allocations are not \npersistent.\n•\t\nUsing libmemkind for persistent memory requires simple \nmodifications to the application.\nFigure 10-2.  An application using different “kinds” of memory\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page182_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page182_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "797.71 sec"
            },
            {
              "page_number": 183,
              "text": "159\n\u0007The memkind API\nThe memkind API functions related to persistent memory programming are shown in \nListing 10-1 and described in this section. The complete memkind API is available in the \nmemkind man pages (http://memkind.github.io/memkind/man_pages/memkind.html).\nListing 10-1.  Persistent memory-related memkind API functions\nKIND CREATION MANAGEMENT:\nint memkind_create_pmem(const char *dir, size_t max_size, memkind_t *kind);\nint memkind_create_pmem_with_config(struct memkind_config *cfg, memkind_t \n*kind);\nmemkind_t memkind_detect_kind(void *ptr);\nint memkind_destroy_kind(memkind_t kind);\nKIND HEAP MANAGEMENT:\nvoid *memkind_malloc(memkind_t kind, size_t size);\nvoid *memkind_calloc(memkind_t kind, size_t num, size_t size);\nvoid *memkind_realloc(memkind_t kind, void *ptr, size_t size);\nvoid memkind_free(memkind_t kind, void *ptr);\nsize_t memkind_malloc_usable_size(memkind_t kind, void *ptr);\nmemkind_t memkind_detect_kind(void *ptr);\nKIND CONFIGURATION MANAGEMENT:\nstruct memkind_config *memkind_config_new();\nvoid memkind_config_delete(struct memkind_config *cfg);\nvoid memkind_config_set_path(struct memkind_config *cfg, const char  \n*pmem_dir);\nvoid memkind_config_set_size(struct memkind_config *cfg, size_t pmem_size);\nvoid memkind_config_set_memory_usage_policy(struct memkind_config *cfg, \nmemkind_mem_usage_policy policy);\n\u0007Kind Management API\nThe memkind library supports a plug-in architecture to incorporate new memory kinds, \nwhich are referred to as dynamic kinds. The memkind library provides the API to create \nand manage the heap for the dynamic kinds.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "802.32 sec"
            },
            {
              "page_number": 184,
              "text": "160\n\u0007Kind Creation\nUse the memkind_create_pmem() function to create a PMEM kind of memory from a \nfile-backed source. This file is created as a tmpfile(3) in a specified directory (PMEM_DIR) \nand is unlinked, so the file name is not listed under the directory. The temporary file is \nautomatically removed when the program terminates.\nUse memkind_create_pmem() to create a fixed or dynamic heap size depending on \nthe application requirement. Additionally, configurations can be created and supplied \nrather than passing in configuration options to the *_create_* function.\nCreating a Fixed-Size Heap\nApplications that require a fixed amount of memory can specify a nonzero value for the \nPMEM_MAX_SIZE argument to memkind_create_pmem(), shown below. This defines the \nsize of the memory pool to be created for the specified kind of memory. The value of \nPMEM_MAX_SIZE should be less than the available capacity of the file system specified in \nPMEM_DIR to avoid ENOMEM or ENOSPC errors. An internal data structure struct memkind is \npopulated internally by the library and used by the memory management functions.\nint memkind_create_pmem(PMEM_DIR, PMEM_MAX_SIZE, &pmem_kind)\nThe arguments to memkind_create_pmem() are\n•\t\nPMEM_DIR is the directory where the temp file is created.\n•\t\nPMEM_MAX_SIZE is the size, in bytes, of the memory region to be \npassed to jemalloc.\n•\t\n&pmem_kind is the address of a memkind data structure.\nIf successful, memkind_create_pmem() returns zero. On failure, an error number is \nreturned that memkind_error_message() can convert to an error message string.  \nListing 10-2 shows how a 32MiB PMEM kind is created on a /daxfs file system. Included in \nthis listing is the definition of memkind_fatal() to print a memkind error message and exit. \nThe rest of the examples in this chapter assume this routine is defined as shown below. \nListing 10-2.  Creating a 32MiB PMEM kind\nvoid memkind_fatal(int err)\n{\n    char error_message[MEMKIND_ERROR_MESSAGE_SIZE];\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "809.38 sec"
            },
            {
              "page_number": 185,
              "text": "161\n    memkind_error_message(err, error_message,\n        MEMKIND_ERROR_MESSAGE_SIZE);\n    fprintf(stderr, \"%s\\n\", error_message);\n    exit(1);\n}\n/* ... in main() ... */\n#define PMEM_MAX_SIZE (1024 * 1024 * 32)\nstruct memkind *pmem_kind;\nint err;\n// Create PMEM memory pool with specific size\nerr = memkind_create_pmem(\"/daxfs\",PMEM_MAX_SIZE, &pmem_kind);\nif (err) {\n    memkind_fatal(err);\n}\nYou can also create a heap with a specific configuration using the function memkind_\ncreate_pmem_with_config(). This function uses a memkind_config structure with \noptional parameters such as size, file path, and memory usage policy. Listing 10-3  \nshows how to build a test_cfg using memkind_config_new(), then passing that \nconfiguration to memkind_create_pmem_with_config() to create a PMEM kind. We use \nthe same path and size parameters from the Listing 10-2 example for comparison.\nListing 10-3.  Creating PMEM kind with configuration\nstruct memkind_config *test_cfg = memkind_config_new();\nmemkind_config_set_path(test_cfg, \"/daxfs\");\nmemkind_config_set_size(test_cfg, 1024 * 1024 * 32);\nmemkind_config_set_memory_usage_policy(test_cfg, MEMKIND_MEM_USAGE_POLICY_\nCONSERVATIVE);\n// create a PMEM partition with specific configuration\nerr = memkind_create_pmem_with_config(test_cfg, &pmem_kind);\nif (err) {\n    memkind_fatal(err);\n}\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "813.18 sec"
            },
            {
              "page_number": 186,
              "text": "162\nCreating a Variable Size Heap\nWhen PMEM_MAX_SIZE is set to zero, as shown below, allocations are satisfied as long as \nthe temporary file can grow. The maximum heap size growth is limited by the capacity of \nthe file system mounted under the PMEM_DIR argument.\nmemkind_create_pmem(PMEM_DIR, 0, &pmem_kind)\nThe arguments to memkind_create_pmem() are: \n•\t\nPMEM_DIR is the directory where the temp file is created.\n•\t\nPMEM_MAX_SIZE is 0.\n•\t\n&pmem_kind is the address of a memkind data structure.\nIf the PMEM kind is created successfully, memkind_create_pmem() returns zero. On \nfailure, memkind_error_message() can be used to convert an error number returned by \nmemkind_create_pmem() to an error message string, as shown in the memkind_fatal() \nroutine in Listing 10-2.\nListing 10-4 shows how to create a PMEM kind with variable size.\nListing 10-4.  Creating a PMEM kind with variable size\nstruct memkind *pmem_kind;\nint err;\nerr = memkind_create_pmem(\"/daxfs\",0,&pmem_kind);\nif (err) {\n    memkind_fatal(err);\n}\n\u0007Detecting the Memory Kind\nMemkind supports both automatic detection of the kind as well as a function to detect \nthe kind associated with a memory referenced by a pointer.\nAutomatic Kind Detection\nAutomatically detecting the kind of memory is supported to simplify code changes when \nusing libmemkind. Thus, the memkind library will automatically retrieve the kind of \nmemory pool the allocation was made from, so the heap management functions listed in \nTable 10-1 can be called without specifying the kind.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "817.89 sec"
            },
            {
              "page_number": 187,
              "text": "163\nThe memkind library internally tracks the kind of a given object from the allocator \nmetadata. However, to get this information, some of the operations may need to \nacquire a lock to prevent accesses from other threads, which may negatively affect the \nperformance in a multithreaded environment.\nMemory Kind Detection \nMemkind also provides the memkind_detect_kind() function, shown below, to query \nand return the kind of memory referenced by the pointer passed into the function. \nIf the input pointer argument is NULL, the function returns NULL. The input pointer \nargument passed into memkind_detect_kind() must have been returned by a previous \ncall to memkind_malloc(), memkind_calloc(), memkind_realloc(), or memkind_posix_\nmemalign().\nmemkind_t memkind_detect_kind(void *ptr)\nSimilar to the automatic detection approach, this function has nontrivial \nperformance overhead. Listing 10-5 shows how to detect the kind type.\nListing 10-5.  pmem_detect_kind.c – how to automatically detect the ‘kind’ type\n    73  err = memkind_create_pmem(path, 0, &pmem_kind);\n    74  if (err) {\n    75      memkind_fatal(err);\n    76  }\n    77\nTable 10-1.  Automatic kind detection functions and their equivalent specified \nkind functions and operations\nOperation\nMemkind API with Kind\nMemkind API Using Automatic Detection\nfree\nmemkind_free(kind, ptr)\nmemkind_free(NULL, ptr)\nrealloc\nmemkind_realloc(kind, ptr, size)\nmemkind_realloc(NULL, ptr, size)\nGet size of allocated \nmemory\nmemkind_malloc_usable_ \nsize(kind, ptr)\nmemkind_malloc_usable_size(NULL, ptr)\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "823.62 sec"
            },
            {
              "page_number": 188,
              "text": "164\n    78  /* do some allocations... */\n    79  buf0 = memkind_malloc(pmem_kind, 1000);\n    80  buf1 = memkind_malloc(MEMKIND_DEFAULT, 1000);\n    81\n    82  /* look up the kind of an allocation */\n    83  if (memkind_detect_kind(buf0) == MEMKIND_DEFAULT) {\n    84      printf(\"buf0 is DRAM\\n\");\n    85  } else {\n    86      printf(\"buf0 is pmem\\n\");\n    87  }\n\u0007Destroying Kind Objects\nUse the memkind_destroy_kind() function, shown below, to delete the kind object that \nwas previously created using the memkind_create_pmem() or memkind_create_pmem_\nwith_config() function.\nint memkind_destroy_kind(memkind_t kind);\nUsing the same pmem_detect_kind.c code from Listing 10-5, Listing 10-6 shows how \nthe kind is destroyed before the program exits.\nListing 10-6.  Destroying a kind object\n    89     err = memkind_destroy_kind(pmem_kind);\n    90     if (err) {\n    91         memkind_fatal(err);\n    92     }\nWhen the kind returned by memkind_create_pmem() or memkind_create_pmem_with_\nconfig() is successfully destroyed, all the allocated memory for the kind object is freed.\n\u0007Heap Management API\nThe heap management functions described in this section have an interface modeled on \nthe ISO C standard API, with an additional “kind” parameter to specify the memory type \nused for allocation.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "827.40 sec"
            },
            {
              "page_number": 189,
              "text": "165\n\u0007Allocating Memory\nThe memkind library provides memkind_malloc(), memkind_calloc(), and memkind_\nrealloc() functions for allocating memory, defined as follows:\nvoid *memkind_malloc(memkind_t kind, size_t size);\nvoid *memkind_calloc(memkind_t kind, size_t num, size_t size);\nvoid *memkind_realloc(memkind_t kind, void *ptr, size_t size);\nmemkind_malloc() allocates size bytes of uninitialized memory of the specified kind. \nThe allocated space is suitably aligned (after possible pointer coercion) for storage of any \nobject type. If size is 0, then memkind_malloc() returns NULL.\nmemkind_calloc() allocates space for num objects, each is size bytes in length. The \nresult is identical to calling memkind_malloc() with an argument of num * size. The \nexception is that the allocated memory is explicitly initialized to zero bytes. If num or size \nis 0, then memkind_calloc() returns NULL.\nmemkind_realloc() changes the size of the previously allocated memory \nreferenced by ptr to size bytes of the specified kind. The contents of the memory \nremain unchanged, up to the lesser of the new and old sizes. If the new size is larger, \nthe contents of the newly allocated portion of the memory are undefined. If successful, \nthe memory referenced by ptr is freed, and a pointer to the newly allocated memory is \nreturned.\nThe code example in Listing 10-7 shows how to allocate memory from DRAM and \npersistent memory (pmem_kind) using memkind_malloc(). Rather than using the \ncommon C library malloc() for DRAM and memkind_malloc() for persistent memory, \nwe recommend using a single library to simplify the code.\nListing 10-7.  An example of allocating memory from both DRAM and persistent \nmemory\n/*\n * Allocates 100 bytes using appropriate \"kind\"\n * of volatile memory\n */\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "833.24 sec"
            },
            {
              "page_number": 190,
              "text": "166\n// Create a PMEM memory pool with a specific size\n  err = memkind_create_pmem(path, PMEM_MAX_SIZE, &pmem_kind);\n  if (err) {\n      memkind_fatal(err);\n  }\n  char *pstring = memkind_malloc(pmem_kind, 100);\n  char *dstring = memkind_malloc(MEMKIND_DEFAULT, 100);\n\u0007Freeing Allocated Memory\nTo avoid memory leaks, allocated memory can be freed using the memkind_free() \nfunction, defined as: \nvoid memkind_free(memkind_t kind, void *ptr);\nmemkind_free() causes the allocated memory referenced by ptr to be made \navailable for future allocations. This pointer must be returned by a previous call to \nmemkind_malloc(), memkind_calloc(), memkind_realloc(), or memkind_posix_\nmemalign(). Otherwise, if memkind_free(kind, ptr) was previously called, undefined \nbehavior occurs. If ptr is NULL, no operation is performed. In cases where the kind is \nunknown in the context of the call to memkind_free(), NULL can be given as the kind \nspecified to memkind_free(), but this will require an internal lookup for the correct kind. \nAlways specify the correct kind because the lookup for kind could result in a serious \nperformance penalty.\nListing 10-8 shows four examples of memkind_free() being used. The first two specify \nthe kind, and the second two use NULL to detect the kind automatically.\nListing 10-8.  Examples of memkind_free() usage\n/* Free the memory by specifying the kind */\nmemkind_free(MEMKIND_DEFAULT, dstring);\nmemkind_free(PMEM_KIND, pstring);\n/* Free the memory using automatic kind detection */\nmemkind_free(NULL, dstring);\nmemkind_free(NULL, pstring);\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "838.05 sec"
            },
            {
              "page_number": 191,
              "text": "167\n\u0007Kind Configuration Management\nYou can also create a heap with a specific configuration using the function memkind_\ncreate_pmem_with_config(). This function requires completing a memkind_config \nstructure with optional parameters such as size, path to file, and memory usage policy.\n\u0007Memory Usage Policy\nIn jemalloc, a runtime option called dirty_decay_ms determines how fast it returns \nunused memory back to the operating system. A shorter decay time purges unused \nmemory pages faster, but the purging costs CPU cycles. Trade-offs between memory and \nCPU cycles needed for this operation should be carefully thought out before using this \nparameter.\nThe memkind library supports two policies related to this feature:\n\t 1.\t MEMKIND_MEM_USAGE_POLICY_DEFAULT\n\t 2.\t MEMKIND_MEM_USAGE_POLICY_CONSERVATIVE\nThe minimum and maximum values for dirty_decay_ms using the MEMKIND_MEM_\nUSAGE_POLICY_DEFAULT are 0ms to 10,000ms for arenas assigned to a PMEM kind. \nSetting MEMKIND_MEM_USAGE_POLICY_CONSERVATIVE sets shorter decay times to purge \nunused memory faster, reducing memory usage. To define the memory usage policy, use \nmemkind_config_set_memory_usage_policy(), shown below:\nvoid memkind_config_set_memory_usage_policy (struct memkind_config *cfg, \nmemkind_mem_usage_policy policy );\n•\t\nMEMKIND_MEM_USAGE_POLICY_DEFAULT is the default memory usage \npolicy.\n•\t\nMEMKIND_MEM_USAGE_POLICY_CONSERVATIVE allows changing the \ndirty_decay_ms parameter.\nListing 10-9 shows how to use memkind_config_set_memory_usage_policy() with a \ncustom configuration.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "842.66 sec"
            },
            {
              "page_number": 192,
              "text": "168\nListing 10-9.  An example of a custom configuration and memory policy use\n    73  struct memkind_config *test_cfg =\n    74      memkind_config_new();\n    75  if (test_cfg == NULL) {\n    76      fprintf(stderr,\n    77          \"memkind_config_new: out of memory\\n\");\n    78      exit(1);\n    79  }\n    80\n    81  memkind_config_set_path(test_cfg, path);\n    82  memkind_config_set_size(test_cfg, PMEM_MAX_SIZE);\n    83  memkind_config_set_memory_usage_policy(test_cfg,\n    84      MEMKIND_MEM_USAGE_POLICY_CONSERVATIVE);\n    85\n    86  // Create PMEM partition with the configuration\n    87  err = memkind_create_pmem_with_config(test_cfg,\n    88      &pmem_kind);\n    89  if (err) {\n    90      memkind_fatal(err);\n    91  }\n\u0007Additional memkind Code Examples\nThe memkind source tree contains many additional code examples, available on GitHub \nat https://github.com/memkind/memkind/tree/master/examples.\n\u0007C++ Allocator for PMEM Kind\nA new pmem::allocator class template is created to support allocations from persistent \nmemory, which conforms to C++11 allocator requirements. It can be used with C++ \ncompliant data structures from:\n•\t\nStandard Template Library (STL)\n•\t\nIntel® Threading Building Blocks (Intel® TBB) library\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "847.27 sec"
            },
            {
              "page_number": 193,
              "text": "169\nThe pmem::allocator class template uses the memkind_create_pmem() function \ndescribed previously. This allocator is stateful and has no default constructor.\n\u0007pmem::allocator methods\npmem::allocator(const char *dir, size_t max_size);\npmem::allocator(const std::string& dir, size_t max_size) ;\ntemplate <typename U> pmem::allocator<T>::allocator(const \npmem::allocator<U>&);\ntemplate <typename U> pmem::allocator(allocator<U>&& other);\npmem::allocator<T>::~allocator();\nT* pmem::allocator<T>::allocate(std::size_t n) const;\nvoid pmem::allocator<T>::deallocate(T* p, std::size_t n) const ;\ntemplate <class U, class... Args> void pmem::allocator<T>::construct(U* p, \nArgs... args) const;\nvoid pmem::allocator<T>::destroy(T* p) const;\nFor more information about the pmem::allocator class template, refer to the pmem \nallocator(3) man page.\n\u0007Nested Containers\nMultilevel containers such as a vector of lists, tuples, maps, strings, and so on pose \nchallenges in handling the nested objects.\nImagine you need to create a vector of strings and store it in persistent memory. The \nchallenges – and their solutions – for this task include: \n\t 1.\t Challenge: The std::string cannot be used for this purpose because \nit is an alias of the std::basic_string. The std::allocator requires a \nnew alias that uses pmem:allocator. \nSolution: A new alias called pmem_string is defined as a typedef \nof std::basic_string when created with pmem::allocator.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "854.34 sec"
            },
            {
              "page_number": 194,
              "text": "170\n\t 2.\t Challenge: How to ensure that an outermost vector will properly \nconstruct nested pmem_string with a proper instance of \npmem::allocator.\nSolution: From C++11 and later, the std::scoped_allocator_\nadaptor class template can be used with multilevel containers. \nThe purpose of this adaptor is to correctly initialize stateful \nallocators in nested containers, such as when all levels of a nested \ncontainer must be placed in the same memory segment.\n\u0007C++ Examples\nThis section presents several full-code examples demonstrating the use of libmemkind \nusing C and C++.\n\u0007Using the pmem::allocator\nAs mentioned earlier, you can use pmem::allocator with any STL-like data structure. \nThe code sample in Listing 10-10 includes a pmem_allocator.h header file to use \npmem::allocator.\nListing 10-10.  pmem_allocator.cpp: using pmem::allocator with std:vector\n    37  #include <pmem_allocator.h>\n    38  #include <vector>\n    39  #include <cassert>\n    40\n    41  int main(int argc, char *argv[]) {\n    42      const size_t pmem_max_size = 64 * 1024 * 1024; //64 MB\n    43      const std::string pmem_dir(\"/daxfs\");\n    44\n    45      // Create allocator object\n    46      libmemkind::pmem::allocator<int>\n    47          alc(pmem_dir, pmem_max_size);\n    48\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "858.90 sec"
            },
            {
              "page_number": 195,
              "text": "171\n    49      // Create std::vector with our allocator.\n    50      std::vector<int,\n    51          libmemkind::pmem::allocator<int>> v(alc);\n    52\n    53      for (int i = 0; i < 100; ++i)\n    54          v.push_back(i);\n    55\n    56      for (int i = 0; i < 100; ++i)\n    57          assert(v[i] == i);\n•\t\nLine 43: We define a persistent memory pool of 64MiB.\n•\t\nLines 46-47: We create an allocator object alc of type \npmem::allocator<int>.\n•\t\nLine 50: We create a vector object v of type std::vector<int, \npmem::allocator<int> > and pass in the alc from line 47 object as \nan argument. The pmem::allocator is stateful and has no default \nconstructor. This requires passing the allocator object to the vector \nconstructor; otherwise, a compilation error occurs if the default \nconstructor of std::vector<int, pmem::allocator<int> > is called \nbecause the vector constructor will try to call the default constructor \nof pmem::allocator, which does not exist yet.\n\u0007Creating a Vector of Strings\nListing 10-11 shows how to create a vector of strings that resides in persistent memory. \nWe define pmem_string as a typedef of std::basic_string with pmem::allocator. \nIn this example, std::scoped_allocator_adaptor allows the vector to propagate the \npmem::allocator instance to all pmem_string objects stored in the vector object.\nListing 10-11.  vector_of_strings.cpp: creating a vector of strings\n    37  #include <pmem_allocator.h>\n    38  #include <vector>\n    39  #include <string>\n    40  #include <scoped_allocator>\n    41  #include <cassert>\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "861.71 sec"
            },
            {
              "page_number": 196,
              "text": "172\n    42  #include <iostream>\n    43\n    44  typedef libmemkind::pmem::allocator<char> str_alloc_type;\n    45\n    46  \u0007typedef std::basic_string<char, std::char_traits<char>,  \nstr_alloc_type> pmem_string;\n    47\n    48  typedef libmemkind::pmem::allocator<pmem_string> vec_alloc_type;\n    49\n    50  \u0007typedef std::vector<pmem_string, std::scoped_allocator_adaptor \n<vec_alloc_type> > vector_type;\n    51\n    52  int main(int argc, char *argv[]) {\n    53      const size_t pmem_max_size = 64 * 1024 * 1024; //64 MB\n    54      const std::string pmem_dir(\"/daxfs\");\n    55\n    56      // Create allocator object\n    57      vec_alloc_type alc(pmem_dir, pmem_max_size);\n    58      // Create std::vector with our allocator.\n    59      vector_type v(alc);\n    60\n    61      v.emplace_back(\"Foo\");\n    62      v.emplace_back(\"Bar\");\n    63\n    64      for (auto str : v) {\n    65              std::cout << str << std::endl;\n    66      }\n•\t\nLine 46: We define pmem_string as a typedef of std::basic_string.\n•\t\nLine 48: We define the pmem::allocator using the pmem_string type.\n•\t\nLine 50: Using std::scoped_allocator_adaptor allows the vector to \npropagate the pmem::allocator instance to all pmem_string objects \nstored in the vector object.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "866.62 sec"
            },
            {
              "page_number": 197,
              "text": "173\n\u0007Expanding Volatile Memory Using  \nPersistent Memory\nPersistent memory is treated by the kernel as a device. In a typical use-case, a persistent \nmemory-aware file system is created and mounted with the –o dax option, and files are \nmemory-mapped into the virtual address space of a process to give the application direct \nload/store access to persistent memory regions.\nA new feature was added to the Linux kernel v5.1 such that persistent memory \ncan be used more broadly as volatile memory. This is done by binding a persistent \nmemory device to the kernel, and the kernel manages it as an extension to DRAM. Since \npersistent memory has different characteristics than DRAM, memory provided by this \ndevice is visible as a separate NUMA node on its corresponding socket.\nTo use the MEMKIND_DAX_KMEM kind, you need pmem to be available using device \nDAX, which exposes pmem as devices with names like /dev/dax*. If you have an existing \ndax device and want to migrate the device model type to use DEV_DAX_KMEM, use:\n$ sudo daxctl migrate-device-model\nTo create a new dax device using all available capacity on the first available region \n(NUMA node), use:\n$ sudo ndctl create-namespace --mode=devdax --map=mem\nTo create a new dax device specifying the region and capacity, use:\n$ sudo ndctl create-namespace --mode=devdax --map=mem --region=region0 \n--size=32g\nTo display a list of namespaces, use:\n$ ndctl list\nIf you have already created a namespace in another mode, such as the default fsdax, \nyou can reconfigure the device using the following where namespace0.0 is the existing \nnamespace you want to reconfigure:\n$ sudo ndctl create-namespace --mode=devdax --map=mem --force -e namespace0.0\nFor more details about creating new namespace read https://docs.pmem.io/\nndctl-users-guide/managing-namespaces#creating-namespaces.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "872.33 sec"
            },
            {
              "page_number": 198,
              "text": "174\nDAX devices must be converted to use the system-ram mode. Converting a dax \ndevice to a NUMA node suitable for use with system memory can be performed using \nfollowing command:\n$ sudo daxctl reconfigure-device dax2.0 --mode=system-ram\nThis will migrate the device from using the device_dax driver to the dax_pmem \ndriver. The following shows an example output with dax1.0 configured as the default \ndevdax type and dax2.0 is system-ram:\n$ daxctl list\n    [\n      {\n        \"chardev\":\"dax1.0\",\n        \"size\":263182090240,\n        \"target_node\":3,\n        \"mode\":\"devdax\"\n      },\n      {\n        \"chardev\":\"dax2.0\",\n        \"size\":263182090240,\n        \"target_node\":4,\n        \"mode\":\"system-ram\"\n      }\n    ]\nYou can now use numactl -H to show the hardware NUMA configuration.  \nThe following example output is collected from a 2-socket system and shows node 4  \nis a new system-ram backed NUMA node created from persistent memory:\n$ numactl -H\n    available: 3 nodes (0-1,4)\n    node 0 cpus: \u00070 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \n23 24 25 26 27 56 57 58 59 60 61 62 63 64 65 66 67 68 69 \n70 71 72 73 74 75 76 77 78 79 80 81 82 83\n    node 0 size: 192112 MB\n    node 0 free: 185575 MB\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "876.09 sec"
            },
            {
              "page_number": 199,
              "text": "175\n    node 1 cpus: \u000728 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \n47 48 49 50 51 52 53 54 55 84 85 86 87 88 89 90 91 92 93 \n94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 \n110 111\n    node 1 size: 193522 MB\n    node 1 free: 193107 MB\n    node 4 cpus:\n    node 4 size: 250880 MB\n    node 4 free: 250879 MB\n    node distances:\n    node   0   1   4\n      0:  10  21  17\n      1:  21  10  28\n      4:  17  28  10\nTo online the NUMA node and have the Kernel manage the new memory, use:\n$ sudo daxctl online-memory dax0.1\ndax0.1: 5 sections already online\ndax0.1: 0 new sections onlined\nonlined memory for 1 device\nAt this point, the kernel will use the new capacity for normal operation. The new \nmemory shows itself in tools such lsmem example shown below where we see an additional \n10GiB of system-ram in the 0x0000003380000000-0x00000035ffffffff address range:\n$ lsmem\nRANGE                                  SIZE  STATE REMOVABLE   BLOCK\n0x0000000000000000-0x000000007fffffff    2G online        no       0\n0x0000000100000000-0x000000277fffffff  154G online       yes    2-78\n0x0000002780000000-0x000000297fffffff    8G online        no   79-82\n0x0000002980000000-0x0000002effffffff   22G online       yes   83-93\n0x0000002f00000000-0x0000002fffffffff    4G online        no   94-95\n0x0000003380000000-0x00000035ffffffff   10G online       yes 103-107\n0x000001aa80000000-0x000001d0ffffffff  154G online       yes 853-929\n0x000001d100000000-0x000001d37fffffff   10G online        no 930-934\n0x000001d380000000-0x000001d8ffffffff   22G online       yes 935-945\n0x000001d900000000-0x000001d9ffffffff    4G online        no 946-947\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "883.73 sec"
            },
            {
              "page_number": 200,
              "text": "176\nMemory block size:         2G\nTotal online memory:     390G\nTotal offline memory:      0B\nTo programmatically allocate memory from a NUMA node created using persistent \nmemory, a new static kind, called MEMKIND_DAX_KMEM, was added to libmemkind \nthat uses the system-ram DAX device.\nUsing MEMKIND_DAX_KMEM as the first argument to memkind_malloc(), shown below, \nyou can use persistent memory from separate NUMA nodes in a single application. \nThe persistent memory is still physically connected to a CPU socket, so the application \nshould take care to ensure CPU affinity for optimal performance.\nmemkind_malloc(MEMKIND_DAX_KMEM, size_t size)\nFigure 10-3 shows an application that created two static kind objects: MEMKIND_\nDEFAULT and MEMKIND_DAX_KMEM.\nThe difference between the PMEM_KIND described earlier and MEMKIND_DAX_\nKMEM is that the MEMKIND_DAX_KMEM is a static kind and uses mmap() with the \nMAP_PRIVATE flag, while the dynamic PMEM_KIND is created with memkind_create_\npmem() and uses the MAP_SHARED flag when memory-mapping files on a DAX-\nenabled file system.\nFigure 10-3.  An application that created two kind objects from different types of \nmemory\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page200_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page200_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "892.12 sec"
            },
            {
              "page_number": 201,
              "text": "177\nChild processes created using the fork(2) system call inherit the MAP_PRIVATE \nmappings from the parent process. When memory pages are modified by the parent \nprocess, a copy-on-write mechanism is triggered by the kernel to create an unmodified \ncopy for the child process. These pages are allocated on the same NUMA node as the \noriginal page.\n\u0007libvmemcache: An Efficient Volatile Key-Value \nCache for Large-Capacity Persistent Memory\nSome existing in-memory databases (IMDB) rely on manual dynamic memory allocations \n(malloc, jemalloc, tcmalloc), which can exhibit external and internal memory \nfragmentation when run for a long period of time, leaving large amounts of memory  \nun-­allocatable. Internal and external fragmentation is briefly explained as follows:\n•\t\nInternal fragmentation occurs when more memory is allocated \nthan is required, and the unused memory is contained within the \nallocated region. For example, if the requested allocation size is 200 \nbytes, a chunk of 256 bytes is allocated.\n•\t\nExternal fragmentation occurs when variable memory sizes are \nallocated dynamically, resulting in a failure to allocate a contiguous \nchunk of memory, although the requested chunk of memory remains \navailable in the system. This problem is more pronounced when large \ncapacities of persistent memory are being used as volatile memory. \nApplications with substantially long runtimes need to solve this \nproblem, especially if the allocated sizes have considerable variation. \nApplications and runtime environments handle this problem in \ndifferent ways, for example:\n•\t\nJava and .NET use compacting garbage collection\n•\t\nRedis and Apache Ignite* use defragmentation algorithms\n•\t\nMemcached uses a slab allocator\nEach of the above allocator mechanisms has pros and cons. Garbage collection and \ndefragmentation algorithms require processing to occur on the heap to free unused \nallocations or move data to create contiguous space. Slab allocators usually define a fixed \nset of different sized buckets at initialization without knowing how many of each bucket \nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "895.92 sec"
            },
            {
              "page_number": 202,
              "text": "178\nthe application will need. If the slab allocator depletes a certain bucket size, it allocates \nfrom larger sized buckets, which reduces the amount of free space. These mechanisms \ncan potentially block the application’s processing and reduce its performance.\n\u0007libvmemcache Overview\nlibvmemcache is an embeddable and lightweight in-memory caching solution with a \nkey-value store at its core. It is designed to take full advantage of large-capacity memory, \nsuch as persistent memory, efficiently using memory mapping in a scalable way. It \nis optimized for use with memory-addressable persistent storage through a DAX-\nenabled file system that supports load/store operations. libvmemcache has these unique \ncharacteristics:\n•\t\nThe extent-based memory allocator sidesteps the fragmentation \nproblem that affects most in-memory databases, and it allows the \ncache to achieve very high space utilization for most workloads.\n•\t\nBuffered LRU (least recently used) combines a traditional LRU \ndoubly linked list with a non-blocking ring buffer to deliver high \nscalability on modern multicore CPUs.\n•\t\nA unique indexing critnib data structure delivers high performance \nand is very space efficient.\nThe cache for libvmemcache is tuned to work optimally with relatively large value \nsizes. While the smallest possible size is 256 bytes, libvmemcache performs best if the \nexpected value sizes are above 1 kilobyte.\nlibvmemcache has more control over the allocation because it implements a custom \nmemory-allocation scheme using an extents-based approach (like that of file system \nextents). libvmemcache can, therefore, concatenate and achieve substantial space \nefficiency. Additionally, because it is a cache, it can evict data to allocate new entries in \na worst-case scenario. libvmemcache will always allocate exactly as much memory as it \nfreed, minus metadata overhead. This is not true for caches based on common memory \nallocators such as memkind. libvmemcache is designed to work with terabyte-sized  \nin-memory workloads, with very high space utilization.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "901.33 sec"
            },
            {
              "page_number": 203,
              "text": "179\nlibvmemcache works by automatically creating a temporary file on a DAX-enabled \nfile system and memory-mapping it into the application’s virtual address space. The \ntemporary file is deleted when the program terminates and gives the perception of \nvolatility. Figure 10-4 shows the application using traditional malloc() to allocate \nmemory from DRAM and using libvmemcache to memory map a temporary file residing \non a DAX-enabled file system from persistent memory.\nAlthough libmemkind supports different kinds of memory and memory consumption \npolicies, the underlying allocator is jemalloc, which uses dynamic memory allocation. \nTable 10-2 compares the implementation details of libvmemcache and libmemkind.\nFigure 10-4.  An application using libvmemcache memory-maps a temporary file \nfrom a DAX-enabled file system\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page203_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page203_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "903.28 sec"
            },
            {
              "page_number": 204,
              "text": "180\n\u0007libvmemcache Design\nlibvmemcache has two main design aspects:\n\t 1.\t Allocator design to improve/resolve fragmentation issues\n\t 2.\t A scalable and efficient LRU policy\n\u0007Extent-Based Allocator\nlibvmemcache can solve fragmentation issues when working with terabyte-sized in-­\nmemory workloads and provide high space utilization. Figure 10-5 shows a workload \nexample that creates many small objects, and over time, the allocator stops due to \nfragmentation.\nTable 10-2.  Design aspects of libmemkind and libvmemcache\nlibmemkind (PMEM)\nlibvmemcache\nAllocation \nScheme\nDynamic allocator\nExtent based (not restricted to \nsector, page, etc.)\nPurpose\nGeneral purpose\nLightweight in-memory cache\nFragmentation\nApps with random size allocations/\ndeallocations that run for a longer period\nMinimized\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "905.85 sec"
            },
            {
              "page_number": 205,
              "text": "181\nlibvmemcache uses an extent-based allocator, where an extent is a contiguous set of \nblocks allocated for storing the data in a database. Extents are typically used with large \nblocks supported by file systems (sectors, pages, etc.), but such restrictions do not apply \nwhen working with persistent memory that supports smaller block sizes (cache line). \nFigure 10-6 shows that if a single contiguous free block is not available to allocate an \nobject, multiple, noncontiguous blocks are used to satisfy the allocation request. The \nnoncontiguous allocations appear as a single allocation to the application.\nFigure 10-5.  An example of a workload that creates many small objects, and the \nallocator stops due to fragmentation\nFigure 10-6.  Using noncontiguous free blocks to fulfill a larger allocation request\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page205_img1.jpeg",
                "output\\images\\Programming_Persistent_Memory_medium_457_page205_img2.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page205_img1_summary.json",
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page205_img2_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "908.99 sec"
            },
            {
              "page_number": 206,
              "text": "182\n\u0007Scalable Replacement Policy\nAn LRU cache is traditionally implemented as a doubly linked list. When an item is \nretrieved from this list, it gets moved from the middle to the front of the list, so it is not \nevicted. In a multithreaded environment, multiple threads may contend with the front \nelement, all trying to move elements being retrieved to the front. Therefore, the front \nelement is always locked (along with other locks) before moving the element being \nretrieved, which results in lock contention. This method is not scalable and is inefficient.\nA buffer-based LRU policy creates a scalable and efficient replacement policy. A non-­\nblocking ring buffer is placed in front of the LRU linked list to track the elements being \nretrieved. When an element is retrieved, it is added to this buffer, and only when the \nbuffer is full (or the element is being evicted), the linked list is locked, and the elements \nin that buffer are processed and moved to the front of the list. This method preserves the \nLRU policy and provides a scalable LRU mechanism with minimal performance impact. \nFigure 10-7 shows a ring buffer-based design for the LRU algorithm.\nFigure 10-7.  A ring buffer-based LRU design\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page206_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page206_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "912.91 sec"
            },
            {
              "page_number": 207,
              "text": "183\n\u0007Using libvmemcache\nTable 10-3 lists the basic functions that libvmemcache provides. For a complete list, \nsee the libvmemcache man pages (https://pmem.io/vmemcache/manpages/master/\nvmemcache.3.html).\nTable 10-3.  The libvmemcache functions\nFunction Name\nDescription\nvmemcache_new\nCreates an empty unconfigured vmemcache instance with default \nvalues: Eviction_policy=VMEMCACHE_REPLACEMENT_LRU\nExtent_size = VMEMCAHE_MIN_EXTENT\nVMEMCACHE_MIN_POOL\nvmemcache_add\nAssociates the cache with a path.\nvmemcache_set_size\nSets the size of the cache.\nvmemcache_set_extent_size\nSets the block size of the cache (256 bytes minimum).\nvmemcache_set_eviction_policy\nSets the eviction policy:\n1. VMEMCACHE_REPLACEMENT_NONE\n2. VMEMCACHE_REPLACEMENT_LRU\nvmemcache_add\nAssociates the cache with a given path on a DAX-enabled file \nsystem or non-DAX-enabled file system.\nvmemcache_delete\nFrees any structures associated with the cache.\nvmemcache_get\nSearches for an entry with the given key, and if found, the entry’s \nvalue is copied to vbuf.\nvmemcache_put\nInserts the given key-value pair into the cache.\nvmemcache_evict\nRemoves the given key from the cache.\nvmemcache_callback_on_evict\nCalled when an entry is being removed from the cache.\nvmemcache_callback_on_miss\nCalled when a get query fails to provide an opportunity to insert \nthe missing key.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "916.49 sec"
            },
            {
              "page_number": 208,
              "text": "184\nTo illustrate how libvmemcache is used, Listing 10-12 shows how to create an \ninstance of vmemcache using default values. This example uses a temporary file on a \nDAX-enabled file system and shows how a callback is registered after a cache miss for a \nkey “meow.”\nListing 10-12.  vmemcache.c: An example program using libvmemcache\n    37  #include <libvmemcache.h>\n    38  #include <stdio.h>\n    39  #include <stdlib.h>\n    40  #include <string.h>\n    41\n    42  #define STR_AND_LEN(x) (x), strlen(x)\n    43\n    44  VMEMcache *cache;\n    45\n    46  void on_miss(VMEMcache *cache, const void *key,\n    47      size_t key_size, void *arg)\n    48  {\n    49      vmemcache_put(cache, STR_AND_LEN(\"meow\"),\n    50           STR_AND_LEN(\"Cthulhu fthagn\"));\n    51  }\n    52\n    53  void get(const char *key)\n    54  {\n    55      char buf[128];\n    56      ssize_t len = vmemcache_get(cache,\n    57      STR_AND_LEN(key), buf, sizeof(buf), 0, NULL);\n    58      if (len >= 0)\n    59          printf(\"%.*s\\n\", (int)len, buf);\n    60      else\n    61          printf(\"(key not found: %s)\\n\", key);\n    62  }\n    63\n    64  int main()\n    65  {\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "921.20 sec"
            },
            {
              "page_number": 209,
              "text": "185\n    66      cache = vmemcache_new();\n    67      if (vmemcache_add(cache, \"/daxfs\")) {\n    68          fprintf(stderr, \"error: vmemcache_add: %s\\n\",\n    69                  vmemcache_errormsg());\n    70              exit(1);\n    71      }\n    72\n    73      // Query a non-existent key\n    74      get(\"meow\");\n    75\n    76      // Insert then query\n    77      vmemcache_put(cache, STR_AND_LEN(\"bark\"),\n    78          STR_AND_LEN(\"Lorem ipsum\"));\n    79      get(\"bark\");\n    80\n    81      // Install an on-miss handler\n    82      vmemcache_callback_on_miss(cache, on_miss, 0);\n    83      get(\"meow\");\n    84\n    85      vmemcache_delete(cache);\n•\t\nLine 66: Creates a new instance of vmemcache with default values for \neviction_policy and extent_size.\n•\t\nLine 67: Calls the vmemcache_add() function to associate cache with a \ngiven path.\n•\t\nLine 74: Calls the get() function to query on an existing key. This \nfunction calls the vmemcache_get() function with error checking for \nsuccess/failure of the function.\n•\t\nLine 77: Calls vmemcache_put() to insert a new key.\n•\t\nLine 82: Adds an on-miss callback handler to insert the key “meow” \ninto the cache.\n•\t\nLine 83: Retrieves the key “meow” using the get() function.\n•\t\nLine 85: Deletes the vmemcache instance.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "926.84 sec"
            },
            {
              "page_number": 210,
              "text": "186\n\u0007Summary\nThis chapter showed how persistent memory’s large capacity can be used to hold volatile \napplication data. Applications can choose to allocate and access data from DRAM or \npersistent memory or both.\nmemkind is a very flexible and easy-to-use library with semantics that are similar to \nthe libc malloc/free APIs that developers frequently use.\nlibvmemcache is an embeddable and lightweight in-memory caching solution that \nallows applications to efficiently use persistent memory’s large capacity in a scalable \nway. libvmemcache is an open source project available on GitHub at ­https://github.\ncom/pmem/vmemcache.\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 10  Volatile Use of Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "931.55 sec"
            },
            {
              "page_number": 211,
              "text": "187\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_11\nCHAPTER 11\nDesigning Data Structures \nfor Persistent Memory\nTaking advantage of the unique characteristics of persistent memory, such as byte \naddressability, persistence, and update in place, allows us to build data structures that \nare much faster than any data structure requiring serialization or flushing to a disk. \nHowever, this comes at a cost. Algorithms must be carefully designed to properly persist \ndata by flushing CPU caches or using non-temporal stores and memory barriers to \nmaintain data consistency. This chapter describes how to design such data structures \nand algorithms and shows what properties they should have.\n\u0007Contiguous Data Structures and Fragmentation\nFragmentation is one of the most critical factors to consider when designing a data \nstructure for persistent memory due to the length of heap life. A persistent heap can \nlive for years with different versions of an application. In volatile use cases, the heap is \ndestroyed when the application exits. The life of the heap is usually measured in hours, \ndays, or weeks.\nUsing file-backed pages for memory allocation makes it difficult to take advantage \nof the operating system–provided mechanisms for minimizing fragmentation, such as \npresenting discontinuous physical memory as a contiguous virtual region. It is possible \nto manually manage virtual memory at a low granularity, producing a page-level \ndefragmentation mechanism for objects in user space. But this mechanism could lead to \ncomplete fragmentation of physical memory and an inability to take advantage of huge \npages. This can cause an increased number of translation lookaside buffer (TLB) misses, \nwhich significantly slows down the entire application. To make effective use of persistent \nmemory, you should design data structures in a way that minimizes fragmentation.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "936.15 sec"
            },
            {
              "page_number": 212,
              "text": "188\n\u0007Internal and External Fragmentation\nInternal fragmentation refers to space that is overprovisioned inside allocated blocks. \nAn allocator always returns memory in fixed-sized chunks or buckets. The allocator must \ndetermine what size each bucket is and how many different sized buckets it provides.  \nIf the size of the memory allocation request does not exactly match a predefined bucket \nsize, the allocator will return a larger memory bucket. For example, if the application \nrequests a memory allocation of 200KiB, but the allocator has bucket sizes of 128KiB \nand 256KiB, the request is allocated from an available 256KiB bucket. The allocator must \nusually return a memory chunk with a size divisible by 16 due to its internal alignment \nrequirements.\nExternal fragmentation occurs when free memory is scattered in small blocks. \nFor example, imagine using up the entire memory with 4KiB allocations. If we then \nfree every other allocation, we have half of the memory available; however, we cannot \nallocate more than 4KiB at once because that is the maximum size of any contiguous free \nspace. Figure 11-1 illustrates this fragmentation, where the red cells represent allocated \nspace and the white cells represent free space.\nWhen storing a sequence of elements in persistent memory, several possible data \nstructures can be used:\n•\t\nLinked list: Each node is allocated from persistent memory.\n•\t\nDynamic array (vector): A data structure that pre-allocates memory \nin bigger chunks. If there is no free space for new elements, it \nallocates a new array with bigger capacity and moves all elements \nfrom the old array to the new one.\n•\t\nSegment vector: A list of fixed-size arrays. If there is no free space left \nin any segment, a new one is allocated.\nFigure 11-1.  External fragmentation\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page212_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page212_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "941.07 sec"
            },
            {
              "page_number": 213,
              "text": "189\nConsider fragmentation for each of those data structures:\n•\t\nFor linked lists, fragmentation efficiency depends on the node size. If \nit is small enough, then high internal fragmentation can be expected. \nDuring node allocation, every allocator will return memory with a \ncertain alignment that will likely be different than the node size.\n•\t\nUsing dynamic array results in fewer memory allocations, but every \nallocation will have a different size (most implementations double \nthe previous one), which results in a higher external fragmentation.\n•\t\nUsing a segment vector, the size of a segment is fixed, so every allocation \nhas the same size. This practically eliminates external fragmentation \nbecause we can allocate a new one for each freed segment.1\n\u0007Atomicity and Consistency\nGuaranteeing consistency requires the proper ordering of stores and making sure data \nis stored persistently. To make an atomic store bigger than 8 bytes, you must use some \nadditional mechanisms. This section describes several mechanisms and discusses their \nmemory and time overheads. For the time overhead, the focus is on analyzing the number \nof flushes and memory barriers used because they have the biggest impact on performance.\n\u0007Transactions\nOne way to guarantee atomicity and consistency is to use transactions (described in \ndetail in Chapter 7). Here we focus on how to design a data structure to use transactions \nefficiently. An example data structure that uses transactions is described in the “Sorted \nArray with Versioning” section later in this chapter.\nTransactions are the simplest solution for guaranteeing consistency. While using \ntransactions can easily make most operations atomic, two items must be kept in mind. \nFirst, transactions that use logging always introduce memory and time overheads. \nSecond, in the case of undo logging, the memory overhead is proportional to the size of \ndata you modify, while the time overhead depends on the number of snapshots. Each \nsnapshot must be persisted prior to the modification of snapshotted data.\n1\u0007Using the libpmemobj allocator, it is also possible to easily lower internal fragmentation by using \nallocation classes (see Chapter 7).\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "946.91 sec"
            },
            {
              "page_number": 214,
              "text": "190\nIt is recommended to use a data-oriented approach when designing a data structure \nfor persistent memory. The idea is to store data in such a way that its processing by the \nCPU is cache friendly. Imagine having to store a sequence of 1000 records that consist of \n2 integer values. This has two approaches: Either use two arrays of integers as shown in \nListing 11-1, or use one array of pairs as shown in Listing 11-2. The first approach is SoA \n(Structure of Arrays), and the second is AoS (Array of Structures).\nListing 11-1.  SoA layout approach to store data\nstruct soa {\n    int a[1000];\n    int b[1000];\n};\nListing 11-2.  AoS layout approach to store data\nstd::pair<int, int> aos_records[1000];\nDepending on the access pattern to the data, you may prefer one solution over the \nother. If the program frequently updates both fields of an element, then the AoS solution \nis better. However, if the program only updates the first variable of all elements, then the \nSoA solution works best.\nFor applications that use volatile memory, the main concerns are usually cache \nmisses and optimizations for single instruction, multiple data (SIMD) processing. SIMD \nis a class of parallel computers in Flynn’s taxonomy,2 which describes computers with \nmultiple processing elements that simultaneously perform the same operation on \nmultiple data points. Such machines exploit data-level parallelism, but not concurrency: \nThere are simultaneous (parallel) computations but only a single process (instruction) at \na given moment.\nWhile those are still valid concerns for persistent memory, developers must consider \nsnapshotting performance when transactions are used. Snapshotting one contiguous \nmemory region is always better then snapshotting several smaller regions, mainly due to \nthe smaller overhead incurred by using less metadata. Efficient data structure layout that \ntakes these considerations into account is imperative for avoiding future problems when \nmigrating data from DRAM-based implementations to persistent memory.\n2\u0007For a full definition of SIMD, see https://en.wikipedia.org/wiki/SIMD.\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "952.54 sec"
            },
            {
              "page_number": 215,
              "text": "191\nListing 11-3 presents both approaches; in this example, we want to increase the first \ninteger by one.\nListing 11-3.  Layout and snapshotting performance\n37 struct soa {\n38   int a[1000];\n39   int b[1000];\n40 };\n41\n42 struct root {\n43   soa soa_records;\n44   std::pair<int, int aos_records[1000];\n45 };\n46\n47 int main()\n48 {\n49   try {\n50     auto pop = pmem::obj::pool<root>::create(\"/daxfs/pmpool\",\n51              \"data_oriented\", PMEMOBJ_MIN_POOL, 0666);\n52\n53   auto root = pop.root();\n54\n55   pmem::obj::transaction::run(pop, [&]{\n56     pmem::obj::transaction::snapshot(&root->soa_records);\n57     for (int i = 0; i < 1000; i++) {\n58       root->soa_records.a[i]++;\n59     }\n60\n61     for (int i = 0; i < 1000; i++) {\n62       pmem::obj::transaction::snapshot(\n63                       &root->aos_records[i].first);\n64       root->aos_records[i].first++;\n65     }\n66   });\n67\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "955.20 sec"
            },
            {
              "page_number": 216,
              "text": "192\n68   pop.close();\n69   } catch (std::exception &e) {\n70      std::cerr << e.what() << std::endl;\n71   }\n72 }\n•\t\nLines 37-45: We define two different data structures to store records \nof integers. The first one is SoA – where we store integers in two \nseparate arrays. Line 44 shows a single array of pairs – AoS.\n•\t\nLines 56-59: We take advantage of the SoA layout by snapshotting the \nentire array at once. Then we can safely modify each element.\n•\t\nLines 61-65: When using AoS, we are forced to snapshot data in every \niteration – elements we want to modify are not contiguous in memory.\nExamples of data structures that use transactions are shown in the “Hash Table with \nTransactions” and “Hash Table with Transactions and Selective Persistence” sections, \nlater in this chapter.\n\u0007Copy-on-Write and Versioning\nAnother way to maintain consistency is the copy-on-write (CoW) technique. In this \napproach, every modification creates a new version at a new location whenever you \nwant to modify some part of a persistent data structure. For example, a node in a linked \nlist can use the CoW approach as described in the following:\n\t 1.\t Create a copy of the element in the list. If a copy is dynamically \nallocated in persistent memory, you should also save the pointer \nin persistent memory to avoid a memory leak. If you fail to do \nthat and the application crashes after the allocation, then on the \napplication restart, newly allocated memory will be unreachable.\n\t 2.\t Modify the copy and persist the changes.\n\t 3.\t Atomically change the original element with the copy and persist \nthe changes, then free the original node if needed. After this \nstep successfully completes, the element is updated and is in a \nconsistent state. If a crash occurs before this step, the original \nelement is untouched.\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "961.55 sec"
            },
            {
              "page_number": 217,
              "text": "193\nAlthough using this approach compared to transactions can be faster, it is significantly \nharder to implement because you must manually persist data.\nCopy-on-write usually works well in multithreaded systems where mechanisms \nlike reference counting or garbage collection are used to free copies that are no longer \nused. Although such systems are beyond the scope of this book, Chapter 14 describes \nconcurrency in multithreaded applications.\nVersioning is a very similar concept to copy-on-write. The difference is that here \nyou hold more than one version of a data field. Each modification creates a new version \nof the field and stores information about the current one. The example presented \nin “Sorted Array with Versioning” later in this chapter shows this technique in an \nimplementation of the insert operation for a sorted array. In the preceding example, only \ntwo versions of a variable are kept, the old and current one as a two-element array. The \ninsert operations alternately write data to the first and second element of this array.\n\u0007Selective Persistence\nPersistent memory is faster than disk storage but potentially slower than DRAM. Hybrid \ndata structures, where some parts are stored in DRAM and some parts are in persistent \nmemory, can be implemented to accelerate performance. Caching previously computed \nvalues or frequently accessed parts of a data structure in DRAM can improve access \nlatency and improve overall performance.\nData does not always need to be stored in persistent memory. Instead, it can be \nrebuilt during the restart of an application to provide a performance improvement \nduring runtime given that it accesses data from DRAM and does not require \ntransactions. An example of this approach appears in “Hash Table with Transactions and \nSelective Persistence.”\n\u0007Example Data Structures\nThis section presents several data structure examples that were designed using the \npreviously described methods for guaranteeing consistency. The code is written in C++ \nand uses libpmemobj-cpp. See Chapter 8 for more information about this library.\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "967.28 sec"
            },
            {
              "page_number": 218,
              "text": "194\n\u0007Hash Table with Transactions\n We present an example of a hash table implemented using transactions and containers \nusing libpmemobj-cpp.\nAs a quick primer to some, and a refresher to other readers, a hash table is a data \nstructure that maps keys to values and guarantees O(1) lookup time. It is usually \nimplemented as an array of buckets (a bucket is a data structure that can hold one or \nmore key-value pairs). When inserting a new element to the hash table, a hash function \nis applied to the element’s key. The resulting value is treated as an index of a bucket \nto which the element is inserted. It is possible that the result of the hash function for \ndifferent keys will be the same; this is called a collision. One method for resolving \ncollisions is to use separate chaining. This approach stores multiple key-value pairs in \none bucket; the example in Listing 11-4 uses this method.\nFor simplicity, the hash table in Listing 11-4 only provides the const Value& \nget(const std::string &key) and void put(const std::string &key, const Value \n&value) methods. It also has a fixed number of buckets. Extending this data structure \nto support the remove operation and to have a dynamic number of buckets is left as an \nexercise to you.\nListing 11-4.  Implementation of a hash table using transactions\n38   #include <functional>\n39   #include <libpmemobj++/p.hpp>\n40   #include <libpmemobj++/persistent_ptr.hpp>\n41   #include <libpmemobj++/pext.hpp>\n42   #include <libpmemobj++/pool.hpp>\n43   #include <libpmemobj++/transaction.hpp>\n44   #include <libpmemobj++/utils.hpp>\n45   #include <stdexcept>\n46   #include <string>\n47\n48   #include \"libpmemobj++/array.hpp\"\n49   #include \"libpmemobj++/string.hpp\"\n50   #include \"libpmemobj++/vector.hpp\"\n51\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "971.99 sec"
            },
            {
              "page_number": 219,
              "text": "195\n52   /**\n53    * Value - type of the value stored in hashmap\n54    * N - number of buckets in hashmap\n55    */\n56   template <typename Value, std::size_t N>\n57   class simple_kv {\n58   private:\n59     using key_type = pmem::obj::string;\n60     using bucket_type = pmem::obj::vector<\n61         std::pair<key_type, std::size_t>>;\n62     using bucket_array_type = pmem::obj::array<bucket_type, N>;\n63     using value_vector = pmem::obj::vector<Value>;\n64\n65     bucket_array_type buckets;\n66     value_vector values;\n67\n68   public:\n69     simple_kv() = default;\n70\n71     const Value &\n72     get(const std::string &key) const\n73    {\n74     auto index = std::hash<std::string>{}(key) % N;\n75\n76     for (const auto &e : buckets[index]) {\n77      if (e.first == key)\n78        return values[e.second];\n79    }\n80\n81    throw std::out_of_range(\"no entry in simplekv\");\n82   }\n83\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "974.65 sec"
            },
            {
              "page_number": 220,
              "text": "196\n84   void\n85   put(const std::string &key, const Value &val)\n86   {\n87    auto index = std::hash<std::string>{}(key) % N;\n88\n89    /* get pool on which this simple_kv resides */\n90    auto pop = pmem::obj::pool_by_vptr(this);\n91\n92    /* search for element with specified key - if found\n93     * update its value in a transaction*/\n94    for (const auto &e : buckets[index]) {\n95      if (e.first == key) {\n96        pmem::obj::transaction::run(\n97          pop, [&] { values[e.second] = val; });\n98\n99        return;\n100      }\n101    }\n102\n103    /* if there is no element with specified key, insert\n104     * new value to the end of values vector and put\n105     * reference in proper bucket */\n106     pmem::obj::transaction::run(pop, [&] {\n107      values.emplace_back(val);\n108      buckets[index].emplace_back(key, values.size() - 1);\n109       });\n110     }\n111   };\n•\t\nLines 58-66: Define the layout of a hash map as a pmem::obj::array \nof buckets, where each bucket is a pmem::obj::vector of key and \nindex pairs and pmem::obj::vector contains the values. The index \nin a bucket entry always specifies a position of the actual value \nstored in a separate vector. For snapshotting optimization, the value \nis not saved next to a key in a bucket. When obtaining a non-const \nreference to an element in pmem::obj::vector, the element is always \nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "980.29 sec"
            },
            {
              "page_number": 221,
              "text": "197\nsnapshotted. To avoid snapshotting unnecessary data, for example, \nif the key is immutable, we split keys and values into separate \nvectors. This also helps in the case of updating several values in \none transaction. Recall the discussion in the “Copy-on-Write and \nVersioning” section. The result could turn out to be next to each other \nin a vector, and there could be fewer bigger regions to snapshot.\n•\t\nLine 74: Calculate hash in a table using standard library feature.\n•\t\nLines 76-79: Search for entry with specified key by iterating over \nall buckets stored in the table under index. Note that e is a const \nreference to the key-value pair. Because of the way libpmemobj-cpp \ncontainers work, this has a positive impact on performance when \ncompared to non-const reference; obtaining non-const reference \nrequires a snapshot, while a const reference does not.\n•\t\nLine 90: Get the instance of the pmemobj pool object, which is used to \nmanage the persistent memory pool where our data structure resides.\n•\t\nLines 94-95: Find the position of a value in the values vector by \niterating over all the entries in the designated bucket.\n•\t\nLines 96-98: If an element with the specified key is found, update its \nvalue using a transaction.\n•\t\nLines 106-109: If there is no element with the specified key, insert a \nvalue into the values vector, and put a reference to this value in the \nproper bucket; that is, create key, index pair. Those two operations \nmust be completed in a single atomic transaction because we want \nthem both to either succeed or fail.\n\u0007Hash Table with Transactions and Selective Persistence\nThis example shows how to modify a persistent data structure (hash table) by moving \nsome data out of persistent memory. The data structure presented in Listing 11-5 is \na modified version of the hash table in Listing 11-4 and contains the implementation \nof this hash table design. Here we store only the vector of keys and vector of values in \npersistent memory. On application startup, we build the buckets and store them in \nvolatile memory for faster processing during runtime. The most noticeable performance \ngain would be in the get() method.\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "987.26 sec"
            },
            {
              "page_number": 222,
              "text": "198\nListing 11-5.  Implementation of hash table with transactions and selective \npersistence\n40 #include <array>\n41 #include <functional>\n42 #include <libpmemobj++/p.hpp>\n43 #include <libpmemobj++/persistent_ptr.hpp>\n44 #include <libpmemobj++/pext.hpp>\n45 #include <libpmemobj++/pool.hpp>\n46 #include <libpmemobj++/transaction.hpp>\n47 #include <libpmemobj++/utils.hpp>\n48 #include <stdexcept>\n49 #include <string>\n50 #include <vector>\n51\n52 #include \"libpmemobj++/array.hpp\"\n53 #include \"libpmemobj++/string.hpp\"\n54 #include \"libpmemobj++/vector.hpp\"\n55\n56 template <typename Value, std::size_t N>\n57 struct simple_kv_persistent;\n58\n59 /**\n60  * This class is runtime wrapper for simple_kv_peristent.\n61  * Value - type of the value stored in hashmap\n62  * N - number of buckets in hashmap\n63  */\n64 template <typename Value, std::size_t N>\n65 class simple_kv_runtime {\n66 private:\n67   using volatile_key_type = std::string;\n68   using bucket_entry_type = std::pair<volatile_key_type, std::size_t>;\n69   using bucket_type = std::vector<bucket_entry_type>;\n70   using bucket_array_type = std::array<bucket_type, N>;\n71\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "989.81 sec"
            },
            {
              "page_number": 223,
              "text": "199\n72   bucket_array_type buckets;\n73   simple_kv_persistent<Value, N> *data;\n74\n75 public:\n76  simple_kv_runtime(simple_kv_persistent<Value, N> *data)\n77  {\n78   this->data = data;\n79\n80   for (std::size_t i = 0; i < data->values.size(); i++) {\n81    auto volatile_key = std::string(data->keys[i].c_str(),\n82                data->keys[i].size());\n83\n84    auto index = std::hash<std::string>{}(volatile_key)%N;\n85    buckets[index].emplace_back(\n86     bucket_entry_type{volatile_key, i});\n87    }\n88   }\n89\n90   const Value &\n91   get(const std::string &key) const\n92   {\n93    auto index = std::hash<std::string>{}(key) % N;\n94\n95    for (const auto &e : buckets[index]) {\n96     if (e.first == key)\n97      return data->values[e.second];\n98    }\n99\n100   throw std::out_of_range(\"no entry in simplekv\");\n101  }\n102\n103  void\n104  put(const std::string &key, const Value &val)\n105  {\n106   auto index = std::hash<std::string>{}(key) % N;\n107\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "994.73 sec"
            },
            {
              "page_number": 224,
              "text": "200\n108   /* get pool on which persistent data resides */\n109      auto pop = pmem::obj::pool_by_vptr(data);\n110\n111    /* search for element with specified key - if found\n112     * update its value in a transaction */\n113    for (const auto &e : buckets[index]) {\n114     if (e.first == key) {\n115       pmem::obj::transaction::run(pop, [&] {\n116         data->values[e.second] = val;\n117       });\n118\n119      return;\n120     }\n121    }\n122\n123   /* if there is no element with specified key, insert new value\n124    * to the end of values vector and key to keys vector\n125    * in a transaction */\n126    pmem::obj::transaction::run(pop, [&] {\n127     data->values.emplace_back(val);\n128     data->keys.emplace_back(key);\n129    });\n130\n131    buckets[index].emplace_back(key, data->values.size() - 1);\n132  }\n133 };\n134\n135 /**\n136  * Class which is stored on persistent memory.\n137  * Value - type of the value stored in hashmap\n138  * N - number of buckets in hashmap\n139  */\n140 template <typename Value, std::size_t N>\n141 struct simple_kv_persistent {\n142  using key_type = pmem::obj::string;\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "999.34 sec"
            },
            {
              "page_number": 225,
              "text": "201\n143  using value_vector = pmem::obj::vector<Value>;\n144  using key_vector = pmem::obj::vector<key_type>;\n145\n146 /* values and keys are stored in separate vectors to optimize\n147  * snapshotting. If they were stored as a pair in single vector\n148  * entire pair would have to be snapshotted in case of value update */\n149  value_vector values;\n150  key_vector keys;\n151\n152  simple_kv_runtime<Value, N>\n153  get_runtime()\n154  {\n155   return simple_kv_runtime<Value, N>(this);\n156  }\n157 };\n•\t\nLine 67: We define the data types residing in volatile memory. These \nare very similar to the types used in the persistent version in “Hash \nTable with Transactions.” The only difference is that here we use std \ncontainers instead of pmem::obj.\n•\t\nLine 72: We declare the volatile buckets array.\n•\t\nLine 73: We declare the pointer to persistent data (simple_kv_\npersistent structure).\n•\t\nLines 75-88: In the simple_kv_runtime constructor, we rebuild the \nbucket’s array by iterating over keys and values in persistent memory. \nIn volatile memory, we store both the keys, which are a copy of the \npersistent data and the index for the values vector in persistent \nmemory.\n•\t\nLines 90-101: The get() function looks for an element reference in \nthe volatile buckets array. There is only one reference to persistent \nmemory when we read the actual value on line 97.\n•\t\nLines 113-121: Similar to the get() function, we search for an \nelement using the volatile data structure and, when found, update \nthe value in a transaction.\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1004.04 sec"
            },
            {
              "page_number": 226,
              "text": "202\n•\t\nLines 126-129: When there is no element with the specified key in the \nhash table, we insert both a value and a key to their respective vectors \nin persistent memory in a transaction.\n•\t\nLine 131: After inserting data to persistent memory, we update the \nstate of the volatile data structure. Note that this operation does not \nhave to be atomic. If a program crashes, the bucket array will be \nrebuilt on startup.\n•\t\nLines 149-150: We define the layout of the persistent data. Key and \nvalues are stored in separate pmem::obj::vector.\n•\t\nLines 153-156: We define a function that returns the runtime object of \nthis hash table.\n\u0007Sorted Array with Versioning\nThis section presents an overview of an algorithm for inserting elements into a sorted \narray and preserving the order of elements. This algorithm guarantees data consistency \nusing the versioning technique.\nFirst, we describe the layout of our sorted array. Figure 11-2 and Listing 11-6 show \nthat there are two arrays of elements and two size fields. Additionally, one current field \nstores information about which array and size variable is currently used.\nFigure 11-2.  Sorted array layout\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page226_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page226_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1007.83 sec"
            },
            {
              "page_number": 227,
              "text": "203\nListing 11-6.  Sorted array layout\n41  template <typename Value, uint64_t slots>\n42  struct entries_t {\n43    Value entries[slots];\n44    size_t size;\n45  };\n46\n47  template <typename Value, uint64_t slots>\n48  class array {\n49  public:\n50   void insert(pmem::obj::pool_base &pop, const Value &);\n51   void insert_element(pmem::obj::pool_base &pop, const Value&);\n52\n53   entries_t<Value, slots> v[2];\n54   uint32_t current;\n55  };\n•\t\nLines 41-45: We define the helper structure, which consists of an \narray of indexes and a size.\n•\t\nLine 53: We define two elements array of entries_t structures. \nentries_t holds an array of elements (entries array) and the number \nof elements in the node as the size variable.\n•\t\nLine 54: This variable determines which entries_t structure from \nline 53 is used. It can be only 0 or 1. Figure 11-2 shows the situation \nwhere the current is equal to 0 and points to the first element of the v \narray.\nTo understand why we need two versions of the entries_t structure and a current \nfield, Figure 11-3 shows how the insert operation works, and the corresponding \npseudocode appears in Listing 11-7.\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1011.73 sec"
            },
            {
              "page_number": 228,
              "text": "204\nListing 11-7.  Pseudocode of a sorted tree insert operation\n57  template <typename Value, uint64_t slots>\n58  void array<Value, slots>::insert_element(pmem::obj::pool_base &pop,\n59                     const Value &entry) {\n60    auto &working_copy = v[1 - current];\n61    auto &consistent_copy = v[current];\n62\n63    auto consistent_insert_position = std::lower_bound(\n64     std::begin(consistent_copy.entries),\n65     std::begin(consistent_copy.entries) +\n66                consistent_copy.size, entry);\n67     auto working_insert_position =\n68         std::begin(working_copy.entries) + \n           std::distance(std::begin(consistent_copy.entries),\n69         consistent_insert_position);\n70\n71          std::copy(std::begin(consistent_copy.entries),\n72                    consistent_insert_position,\n73                    std::begin(working_copy.entries));\n74\n75          *working_insert_position = entry;\n76\n77          std::copy(consistent_insert_position,\n78                    std::begin(consistent_copy.entries) + \n                         consistent_copy.size,\n79                    working_insert_position + 1);\nFigure 11-3.  Overview of a sorted tree insert operation\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page228_img1.jpeg"
              ],
              "img_summary_files": [],
              "img_vision_files": [
                "output\\images\\img_vision\\Programming_Persistent_Memory_medium_457_page228_img1_vision.json"
              ],
              "summary": "Summary not available due to an error.",
              "time_taken": "1016.64 sec"
            },
            {
              "page_number": 229,
              "text": "205\n80\n81          working_copy.size = consistent_copy.size + 1;\n82  }\n83\n84  template <typename V, uint64_t s>\n85  void array<V,s>::insert(pmem::obj::pool_base &pop,\n86                                   const Value &entry){\n87   insert_element(pop, entry);\n88   pop.persist(&(v[1 - current]), sizeof(entries_t<Value, slots>));\n89\n90   current = 1 - current;\n91   pop.persist(&current, sizeof(current));\n92 }\n•\t\nLines 60-61: We define references to the current version of entries \narray and to the working version.\n•\t\nLine 63: We find the position in the current array where an entry \nshould be inserted.\n•\t\nLine 67: We create iterator to the working array.\n•\t\nLine 71: We copy part of the current array to the working array (range \nfrom beginning of the current array to the place where a new element \nshould be inserted).\n•\t\nLine 75: We insert an entry to the working array.\n•\t\nLine 77: We copy remaining elements from the current array to the \nworking array after the element we just inserted.\n•\t\nLine 81: We update the size of the working array to the size of the \ncurrent array plus one, for the element inserted.\n•\t\nLines 87-88: We insert an element and persist the entire v[1-current] \nelement.\n•\t\nLines 90-91: We update the current value and save it.\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1022.89 sec"
            },
            {
              "page_number": 230,
              "text": "206\nLet’s analyze whether this approach guarantees data consistency. In the first step, \nwe copy elements from the original array to a currently unused one, insert the new \nelement, and persist it to make sure data goes to the persistence domain. The persist \ncall also ensures that the next operation (updating the current value) is not reordered \nbefore any of the previous stores. Because of this, any interruption before or after issuing \nthe instruction to update the current field would not corrupt data because the current \nvariable always points to a valid version.\nThe memory overhead of using versioning for the insert operation is equal to a size \nof the entries array and the current field. In terms of time overhead, we issued only two \npersist operations.\n\u0007Summary\nThis chapter shows how to design data structures for persistent memory, considering its \ncharacteristics and capabilities. We discuss fragmentation and why it is problematic in \nthe case of persistent memory. We also present a few different methods of guaranteeing \ndata consistency; using transactions is the simplest and least error-prone method. \nOther approaches, such as copy-on-write or versioning, can perform better, but they are \nsignificantly more difficult to implement correctly.\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 11  Designing Data Structures for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1026.68 sec"
            },
            {
              "page_number": 231,
              "text": "207\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_12\nCHAPTER 12\nDebugging Persistent \nMemory Applications\nPersistent memory programming introduces new opportunities that allow developers to \ndirectly persist data structures without serialization and to access them in place without \ninvolving classic block I/O. As a result, you can merge your data models and avoid the \nclassic split between data in memory – which is volatile, fast, and byte addressable – with \ndata on traditional storage devices, which is non-volatile but slower.\nPersistent memory programming also brings challenges. Recall our discussion \nabout power-fail protected persistence domains in Chapter 2: When a process or system \ncrashes on an Asynchronous DRAM Refresh (ADR)-enabled platform, data residing in \nthe CPU caches that has not yet been flushed, is lost. This is not a problem with volatile \nmemory because all the memory hierarchy is volatile. With persistent memory, however, \na crash can cause permanent data corruption. How often must you flush data? Flushing \ntoo frequently yields suboptimal performance, and not flushing often enough leaves the \npotential for data loss or corruption.\nChapter 11 described several approaches to designing data structures and using \nmethods such as copy-on-write, versioning, and transactions to maintain data integrity. \nMany libraries within the Persistent Memory Development Kit (PMDK) provide \ntransactional updates of data structures and variables. These libraries provide optimal \nCPU cache flushing, when required by the platform, at precisely the right time, so you \ncan program without concern about the hardware intricacies.\nThis programming paradigm introduces new dimensions related to errors and \nperformance issues that programmers need to be aware of. The PMDK libraries reduce \nerrors in persistent memory programming, but they cannot eliminate them. This chapter",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1031.49 sec"
            },
            {
              "page_number": 232,
              "text": "208\ndescribes common persistent memory programming issues and pitfalls and how to \ncorrect them using the tools available. The first half of this chapter introduces the tools. \nThe second half presents several erroneous programming scenarios and describes how \nto use the tools to correct the mistakes before releasing your code into production.\n\u0007pmemcheck for Valgrind\npmemcheck is a Valgrind (http://www.valgrind.org/) tool developed by Intel. It is very \nsimilar to memcheck, which is the default tool in Valgrind to discover memory-related \nbugs but adapted for persistent memory. Valgrind is an instrumentation framework for \nbuilding dynamic analysis tools. Some Valgrind tools can automatically detect many \nmemory management and threading bugs and profile your programs in detail. You can \nalso use Valgrind to build new tools.\nTo run pmemcheck, you need a modified version of Valgrind supporting the new \nCLFLUSHOPT and CLWB flushing instructions. The persistent memory version of Valgrind \nincludes the pmemcheck tool and is available from https://github.com/pmem/valgrind. \nRefer to the README.md within the GitHub project for installation instructions.\nAll the libraries in PMDK are already instrumented with pmemcheck. If you use PMDK \nfor persistent memory programming, you will be able to easily check your code with \npmemcheck without any code modification.\nBefore we discuss the pmemcheck details, the following two sections demonstrate how \nit identifies errors in an out-of-bounds and a memory leak example.\n\u0007Stack Overflow Example\nAn out-of-bounds scenario is a stack/buffer overflow bug, where data is written or  \nread beyond the capacity of the stack or array. Consider the small code snippet in  \nListing 12-1.\nListing 12-1.  stackoverflow.c: Example of an out-of-bound bug\n    32  #include <stdlib.h>\n    33\n    34  int main() {\n    35          int *stack = malloc(100 * sizeof(int));\n    36          stack[100] = 1234;\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1036.10 sec"
            },
            {
              "page_number": 233,
              "text": "209\n    37          free(stack);\n    38      return 0;\n    39  }\nIn line 36, we are incorrectly assigning the value 1234 to the position 100, which is \noutside the array range of 0-99. If we compile and run this code, it may not fail. This is \nbecause, even if we only allocated 400 bytes (100 integers) for our array, the operating \nsystem provides a whole memory page, typically 4KiB. Executing the binary under \nValgrind reports an issue, shown in Listing 12-2.\nListing 12-2.  Running Valgrind with code Listing 12-1\n$ valgrind ./stackoverflow\n==4188== Memcheck, a memory error detector\n...\n==4188== Invalid write of size 4\n==4188==    at 0x400556: main (stackoverflow.c:36)\n==4188==  Address 0x51f91d0 is 0 bytes after a block of size 400 alloc'd\n==4188==    at 0x4C2EB37: malloc (vg_replace_malloc.c:299)\n==4188==    by 0x400547: main (stackoverflow.c:35)\n...\n==4188== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)\nBecause Valgrind can produce long reports, we show only the relevant “Invalid write” \nerror part of the report. When compiling code with symbol information (gcc -g), it is \neasy to see the exact place in the code where the error is detected. In this case, Valgrind \nhighlights line 36 of the stackoverflow.c file. With the issue identified in the code, we \nknow where to fix it.\n\u0007Memory Leak Example\nMemory leaks are another common issue. Consider the code in Listing 12-3.\nListing 12-3.  leak.c: Example of a memory leak\n    32  #include <stdlib.h>\n    33\n    34  void func(void) {\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1040.81 sec"
            },
            {
              "page_number": 234,
              "text": "210\n    35      int *stack = malloc(100 * sizeof(int));\n    36  }\n    37\n    38  int main(void) {\n    39      func();\n    40      return 0;\n    41  }\nThe memory allocation is moved to the function func(). A memory leak occurs \nbecause the pointer to the newly allocated memory is a local variable on line 35, which is \nlost when the function returns. Executing this program under Valgrind shows the results \nin Listing 12-4.\nListing 12-4.  Running Valgrind with code Listing 12-3\n$ valgrind --leak-check=yes ./leak\n==4413== Memcheck, a memory error detector\n...\n==4413== 400 bytes in 1 blocks are definitely lost in loss record 1 of 1\n==4413==    at 0x4C2EB37: malloc (vg_replace_malloc.c:299)\n==4413==    by 0x4004F7: func (leak.c:35)\n==4413==    by 0x400507: main (leak.c:39)\n==4413==\n==4413== LEAK SUMMARY:\n...\n==4413== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)\nValgrind shows a loss of 400 bytes of memory allocated at leak.c:35. To learn more, \nplease visit the official Valgrind documentation (http://www.valgrind.org/docs/\nmanual/index.html).\n\u0007Intel Inspector – Persistence Inspector\nIntel Inspector – Persistence Inspector is a runtime tool that developers use to detect \nprogramming errors in persistent memory programs. In addition to cache flush misses, \nthis tool detects\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1045.52 sec"
            },
            {
              "page_number": 235,
              "text": "211\n•\t\nRedundant cache flushes and memory fences\n•\t\nOut-of-order persistent memory stores\n•\t\nIncorrect undo logging for the PMDK\nPersistence Inspector is included as part of Intel Inspector, an easy-to-use  \nmemory and threading error debugger for C, C++, and Fortran that works with both \nWindows and Linux operating systems. It has an intuitive graphical and command-\nline interfaces, and it can be integrated with Microsoft Visual Studio. Intel Inspector \nis available as part of Intel Parallel Studio XE (https://software.intel.com/en-us/\nparallel-studio-xe) and Intel System Studio (https://software.intel.com/en-us/\nsystem-studio).\nThis section describes how the Intel Inspector tool works with the same out-of-\nbounds and memory leak examples from Listings 12-1 and 12-3.\n\u0007Stack Overflow Example\nThe Listing 12-5 example demonstrates how to use the command-line interface to \nperform the analysis and collect the data and then switches to the GUI to examine \nthe results in detail. To collect the data, we use the inspxe-cl utility with the –c=mi2 \ncollection option for detecting memory problems.\nListing 12-5.  Running Intel Inspector with code Listing 12-1\n$ inspxe-cl -c=mi2 -- ./stackoverflow\n1 new problem(s) found\n    1 Invalid memory access problem(s) detected\nIntel Inspector creates a new directory with the data and analysis results, and prints \na summary of findings to the terminal. For the stackoverflow app, it detected one invalid \nmemory access.\nAfter launching the GUI using inspxe-gui, we open the results collection through \nthe File ➤ Open ➤ Result menu and navigate to the directory created by inspxe-cli. The \ndirectory will be named r000mi2 if it is the first run. Within the directory is a file named \nr000mi2.inspxe. Once opened and processed, the GUI presents the data shown in \nFigure 12-1.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1052.38 sec"
            },
            {
              "page_number": 236,
              "text": "212\nThe GUI defaults to the Summary tab to provide an overview of the analysis. Since \nwe compiled the program with symbols, the Code Locations panel at the bottom shows \nthe exact place in the code where the problem was detected. Intel Inspector identified \nthe same error on line 36 that Valgrind found.\nIf Intel Inspector detects multiple problems within the program, those issues are \nlisted in the Problems section in the upper left area of the window. You can select each \nproblem and see the information relating to it in the other sections of the window.\n\u0007Memory Leak Example\nThe Listing 12-6 example runs Intel Inspector using the leak.c code from Listing 12-2 \nand uses the same arguments from the stackoverflow program to detect memory issues.\nListing 12-6.  Running Intel Inspector with code Listing 12-2\n$ inspxe-cl -c=mi2 -- ./leak\n1 new problem(s) found\n    1 Memory leak problem(s) detected\nFigure 12-1.  GUI of Intel Inspector showing results for Listing 12-1\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page236_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page236_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1055.21 sec"
            },
            {
              "page_number": 237,
              "text": "213\nThe Intel Inspector output is shown in Figure 12-2 and explains that a memory leak \nproblem was detected. When we open the r001mi2/r001mi2.inspxe result file in the \nGUI, we get something similar to what is shown in the lower left section of Figure 12-2.\nThe information related to the leaked object is shown above the code listing:\n•\t\nAllocation site (source, function name, and module)\n•\t\nObject size (400 bytes)\n•\t\nThe variable name that caused the leak\nThe right side of the Code panel shows the call stack that led to the bug (call stacks \nare read from bottom to top). We see the call to func() in the main() function on line 39 \n(leak.c:39), then the memory allocation occurs within func() on line 35 (leak.c:35).\nThe Intel Inspector offers much more than what we presented here. To learn \nmore, please visit the documentation (https://software.intel.com/en-us/intel-\ninspector-support/documentation).\nFigure 12-2.  GUI of Intel Inspector showing results for Listing 12-2\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page237_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page237_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1058.11 sec"
            },
            {
              "page_number": 238,
              "text": "214\n\u0007Common Persistent Memory Programming \nProblems\nThis section reviews several coding and performance problems you are likely to \nencounter, how to catch them using the pmemcheck and Intel Inspector tools, and how to \nresolve the issues.\nThe tools we use highlight deliberately added issues in our code that can cause \nbugs, data corruption, or other problems. For pmemcheck, we show how to bypass data \nsections that should not be checked by the tool and use macros to assist the tool in better \nunderstanding our intent.\n\u0007Nonpersistent Stores\nNonpersistent stores refer to data written to persistent memory but not flushed explicitly. \nIt is understood that if the program writes to persistent memory, it wishes for those \nwrites to be persistent. If the program ends without explicitly flushing writes, there is an \nopen possibility for data corruption. When a program exits gracefully, all the pending \nwrites in the CPU caches are flushed automatically. However, if the program were to \ncrash unexpectedly, writes still residing in the CPU caches could be lost.\nConsider the code in Listing 12-7 that writes data to a persistent memory device \nmounted to /mnt/pmem without flushing the data.\nListing 12-7.  Example of writing to persistent memory without flushing\n    32  #include <stdio.h>\n    33  #include <sys/mman.h>\n    34  #include <fcntl.h>\n    35\n    36  int main(int argc, char *argv[]) {\n    37      int fd, *data;\n    38      fd = open(\"/mnt/pmem/file\", O_CREAT|O_RDWR, 0666);\n    39      posix_fallocate(fd, 0, sizeof(int));\n    40      data = (int *) mmap(NULL, sizeof(int), PROT_READ |\n    41                      PROT_WRITE, MAP_SHARED_VALIDATE |\n    42                      MAP_SYNC, fd, 0);\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1063.75 sec"
            },
            {
              "page_number": 239,
              "text": "215\n    43      *data = 1234;\n    44      munmap(data, sizeof(int));\n    45      return 0;\n    46  }\n•\t\nLine 38: We open /mnt/pmem/file.\n•\t\nLine 39: We make sure there is enough space in the file to allocate an \ninteger by calling posix_fallocate().\n•\t\nLine 40: We memory map /mnt/pmem/file.\n•\t\nLine 43: We write 1234 to the memory.\n•\t\nLine 44: We unmap the memory.\nIf we run pmemcheck with Listing 12-7, we will not get any useful information \nbecause pmemcheck has no way to know which memory addresses are persistent and \nwhich ones are volatile. This may change in future versions. To run pmemcheck, we pass \n--tool=pmemcheck argument to valgrind as shown in Listing 12-8. The result shows no \nissues were detected.\nListing 12-8.  Running pmemcheck with code Listing 12-7\n$ valgrind --tool=pmemcheck ./listing_12-7\n==116951== pmemcheck-1.0, a simple persistent store checker\n==116951== Copyright (c) 2014-2016, Intel Corporation\n==116951== \u0007Using Valgrind-3.14.0 and LibVEX; rerun with -h for copyright \ninfo\n==116951== Command: ./listing_12-9\n==116951==\n==116951==\n==116951== Number of stores not made persistent: 0\n==116951== ERROR SUMMARY: 0 errors\nWe can inform pmemcheck which memory regions are persistent using a VALGRIND_\nPMC_REGISTER_PMEM_MAPPING macro shown on line 52 in Listing 12-9. We must include \nthe valgrind/pmemcheck.h header for pmemcheck, line 36, which defines the VALGRIND_\nPMC_REGISTER_PMEM_MAPPING macro and others.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1068.35 sec"
            },
            {
              "page_number": 240,
              "text": "216\nListing 12-9.  Example of writing to persistent memory using Valgrind macros \nwithout flushing\n    33  #include <stdio.h>\n    34  #include <sys/mman.h>\n    35  #include <fcntl.h>\n    36  #include <valgrind/pmemcheck.h>\n    37\n    38  int main(int argc, char *argv[]) {\n    39      int fd, *data;\n    40\n    41      // open the file and allocate enough space for an\n    42      // integer\n    43      fd = open(\"/mnt/pmem/file\", O_CREAT|O_RDWR, 0666);\n    44      posix_fallocate(fd, 0, sizeof(int));\n    45\n    46      // memory map the file and register the mapped\n    47      // memory with VALGRIND\n    48      data = (int *) mmap(NULL, sizeof(int),\n    49              PROT_READ|PROT_WRITE,\n    50              MAP_SHARED_VALIDATE | MAP_SYNC,\n    51              fd, 0);\n    52      VALGRIND_PMC_REGISTER_PMEM_MAPPING(data,\n    53                                 sizeof(int));\n    54\n    55      // write to pmem\n    56      *data = 1234;\n    57\n    58      // unmap the memory and un-register it with\n    59      // VALGRIND\n    60      munmap(data, sizeof(int));\n    61      VALGRIND_PMC_REMOVE_PMEM_MAPPING(data,\n    62                                       sizeof(int));\n    63      return 0;\n    64  }\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1074.09 sec"
            },
            {
              "page_number": 241,
              "text": "217\nWe remove persistent memory mapping identification from pmemcheck using the \nVALGRIND_PMC_REMOVE_PMEM_MAPPING macro. As mentioned earlier, this is useful when \nyou want to exclude parts of persistent memory from the analysis. Listing 12-10 shows \nexecuting pmemcheck with the modified code in Listing 12-9, which now reports a \nproblem.\nListing 12-10.  Running pmemcheck with code Listing 12-9\n$ valgrind --tool=pmemcheck ./listing_12-9\n==8904== pmemcheck-1.0, a simple persistent store checker\n...\n==8904== Number of stores not made persistent: 1\n==8904== Stores not made persistent properly:\n==8904== [0]    at 0x4008B4: main (listing_12-9.c:56)\n==8904==        Address: 0x4027000      size: 4 state: DIRTY\n==8904== Total memory not made persistent: 4\n==8904== ERROR SUMMARY: 1 errors\nSee that pmemcheck detected that data is not being flushed after a write in \nlisting_12-9.c, line 56. To fix this, we create a new flush() function, accepting an \naddress and size, to flush all the CPU cache lines storing any part of the data using the \nCLFLUSH machine instruction (__mm_clflush()). Listing 12-11 shows the modified \ncode.\nListing 12-11.  Example of writing to persistent memory using Valgrind with \nflushing\n    33  #include <emmintrin.h>\n    34  #include <stdint.h>\n    35  #include <stdio.h>\n    36  #include <sys/mman.h>\n    37  #include <fcntl.h>\n    38  #include <valgrind/pmemcheck.h>\n    39\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1078.90 sec"
            },
            {
              "page_number": 242,
              "text": "218\n    40  // flushing from user space\n    41  void flush(const void *addr, size_t len) {\n    42      uintptr_t flush_align = 64, uptr;\n    43      for (uptr = (uintptr_t)addr & ~(flush_align - 1);\n    44               uptr < (uintptr_t)addr + len;\n    45               uptr += flush_align)\n    46          _mm_clflush((char *)uptr);\n    47  }\n    48\n    49  int main(int argc, char *argv[]) {\n    50      int fd, *data;\n    51\n    52      // open the file and allocate space for one\n    53      // integer\n    54      fd = open(\"/mnt/pmem/file\", O_CREAT|O_RDWR, 0666);\n    55      posix_fallocate(fd, 0, sizeof(int));\n    56\n    57      // map the file and register it with VALGRIND\n    58      data = (int *)mmap(NULL, sizeof(int),\n    59              PROT_READ | PROT_WRITE,\n    60              MAP_SHARED_VALIDATE | MAP_SYNC, fd, 0);\n    61      VALGRIND_PMC_REGISTER_PMEM_MAPPING(data,\n    62                                         sizeof(int));\n    63\n    64      // write and flush\n    65      *data = 1234;\n    66      flush((void *)data, sizeof(int));\n    67\n    68      // unmap and un-register\n    69      munmap(data, sizeof(int));\n    70      VALGRIND_PMC_REMOVE_PMEM_MAPPING(data,\n    71                                       sizeof(int));\n    72      return 0;\n    73  }\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1084.53 sec"
            },
            {
              "page_number": 243,
              "text": "219\nRunning the modified code through pmemcheck reports no issues, as shown in  \nListing 12-12.\nListing 12-12.  Running pmemcheck with code Listing 12-11\n$ valgrind --tool=pmemcheck ./listing_12-11\n==9710== pmemcheck-1.0, a simple persistent store checker\n...\n==9710== Number of stores not made persistent: 0\n==9710== ERROR SUMMARY: 0 errors\nBecause Intel Inspector – Persistence Inspector does not consider an unflushed write a \nproblem unless there is a write dependency with other variables, we need to show a more \ncomplex example than writing a single variable in Listing 12-7. You need to understand \nhow programs writing to persistent memory are designed to know which parts of the data \nwritten to the persistent media are valid and which parts are not. Remember that recent \nwrites may still be sitting on the CPU caches if they are not explicitly flushed.\nTransactions solve the problem of half-written data by using logs to either roll back \nor apply uncommitted changes; thus, programs reading the data back can be assured \nthat everything written is valid. In the absence of transactions, it is impossible to know \nwhether or not the data written on persistent memory is valid, especially if the program \ncrashes.\nA writer can inform a reader that data is properly written in one of two ways, either \nby setting a “valid” flag or by using a watermark variable with the address (or the index, \nin the case of an array) of the last valid written memory position.\nListing 12-13 shows pseudocode for how the “valid” flag approach could be \nimplemented.\nListing 12-13.  Pseudocode showcasing write dependency of var1 with var1_valid\n    1  writer() {\n    2          var1 = \"This is a persistent Hello World\n    3                  written to persistent memory!\";\n    4          flush (var1);\n    5          var1_valid = True;\n    6          flush (var1_valid);\n    7  }\n    8\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1090.27 sec"
            },
            {
              "page_number": 244,
              "text": "220\n    9  reader() {\n    10          if (var1_valid == True) {\n    11                  print (var1);\n    12          }\n    14  }\nThe reader() will read the data in var1 if the var1_valid flag is set to True (line 10), \nand var1_valid can only be True if var1 has been flushed (lines 4 and 5).\nWe can now modify the code from Listing 12-7 to introduce this “valid” flag. In \nListing 12-14, we separate the code into writer and reader programs and map two \nintegers instead of one (to accommodate for the flag). Listing 12-15 shows the reading to \npersistent memory example.\nListing 12-14.  Example of writing to persistent memory with a write \ndependency; the code does not flush\n    33  #include <stdio.h>\n    34  #include <sys/mman.h>\n    35  #include <fcntl.h>\n    36  #include <string.h>\n    37\n    38  int main(int argc, char *argv[]) {\n    39      int fd, *ptr, *data, *flag;\n    40\n    41      fd = open(\"/mnt/pmem/file\", O_CREAT|O_RDWR, 0666);\n    42      posix_fallocate(fd, 0, sizeof(int)*2);\n    43\n    44      ptr = (int *) mmap(NULL, sizeof(int)*2,\n    45                         PROT_READ | PROT_WRITE,\n    46                         MAP_SHARED_VALIDATE | MAP_SYNC,\n    47                         fd, 0);\n    48\n    49      data = &(ptr[1]);\n    50      flag = &(ptr[0]);\n    51      *data = 1234;\n    52      *flag = 1;\n    53\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1095.18 sec"
            },
            {
              "page_number": 245,
              "text": "221\n    54      munmap(ptr, 2 * sizeof(int));\n    55      return 0;\n    56  }\nListing 12-15.  Example of reading from persistent memory with a write \ndependency\n    33  #include <stdio.h>\n    34  #include <sys/mman.h>\n    35  #include <fcntl.h>\n    36\n    37  int main(int argc, char *argv[]) {\n    38      int fd, *ptr, *data, *flag;\n    39\n    40      fd = open(\"/mnt/pmem/file\", O_CREAT|O_RDWR, 0666);\n    41      posix_fallocate(fd, 0, 2 * sizeof(int));\n    42\n    43      ptr = (int *) mmap(NULL, 2 * sizeof(int),\n    44                         PROT_READ | PROT_WRITE,\n    45                         MAP_SHARED_VALIDATE | MAP_SYNC,\n    46                         fd, 0);\n    47\n    48      data = &(ptr[1]);\n    49      flag = &(ptr[0]);\n    50      if (*flag == 1)\n    51          printf(\"data = %d\\n\", *data);\n    52\n    53      munmap(ptr, 2 * sizeof(int));\n    54      return 0;\n    55  }\nChecking our code with Persistence Inspector is done in three steps.\nStep 1: We must run the before-unfortunate-event phase analysis (see Listing 12-16), \nwhich corresponds to the writer code in Listing 12-14.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1099.99 sec"
            },
            {
              "page_number": 246,
              "text": "222\nListing 12-16.  Running Intel Inspector – Persistence Inspector with code  \nListing 12-14 for before-unfortunate-event phase analysis\n$ pmeminsp cb -pmem-file /mnt/pmem/file -- ./listing_12-14\n++ Analysis starts\n++ Analysis completes\n++ Data is stored in folder \"/data/.pmeminspdata/data/listing_12-14\"\nThe parameter cb is an abbreviation of check-before-unfortunate-event, which \nspecifies the type of analysis. We must also pass the persistent memory file that will be \nused by the application so that Persistence Inspector knows which memory accesses \ncorrespond to persistent memory. By default, the output of the analysis is stored in \na local directory under the .pmeminspdata directory. (You can also specify a custom \ndirectory; run pmeminsp -help for information on the available options.)\nStep 2: We run the after-unfortunate-event phase analysis (see Listing 12-17). This \ncorresponds to the code that will read the data after an unfortunate event happens, such \nas a process crash.\nListing 12-17.  Running Intel Inspector – Persistence Inspector with code Listing \n12-15 for after-unfortunate-event phase analysis\n$ pmeminsp ca -pmem-file /mnt/pmem/file -- ./listing_12-15\n++ Analysis starts\ndata = 1234\n++ Analysis completes\n++ Data is stored in folder \"/data/.pmeminspdata/data/listing_12-15\"\nThe parameter ca is an abbreviation of check-after-unfortunate-event. Again, the \noutput of the analysis is stored in .pmeminspdata within the current working directory.\nStep 3: We generate the final report. For this, we pass the option rp (abbreviation for \nreport) along with the name of both programs, as shown in Listing 12-18.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1104.60 sec"
            },
            {
              "page_number": 247,
              "text": "223\nListing 12-18.  Generating a final report with Intel Inspector – Persistence \nInspector from the analysis done in Listings 12-16 and 12-17\n$ pmeminsp rp -- listing_12-16 listing_12-17\n#=============================================================\n# Diagnostic # 1: Missing cache flush\n#-------------------\n  The first memory store\n    of size 4 at address 0x7F9C68893004 (offset 0x4 in /mnt/pmem/file)\n    in /data/listing_12-16!main at listing_12-16.c:51 - 0x67D\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_\nline> - 0x223D3\n    in /data/listing_12-16!_start at <unknown_file>:<unknown_line> - 0x534\n  is not flushed before\n  the second memory store\n    of size 4 at address 0x7F9C68893000 (offset 0x0 in /mnt/pmem/file)\n    in /data/listing_12-16!main at listing_12-16.c:52 - 0x687\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_\nline> - 0x223D3\n    in /data/listing_12-16!_start at <unknown_file>:<unknown_line> - 0x534\n  while\n  memory load from the location of the first store\n    in /data/listing_12-17!main at listing_12-17.c:51 - 0x6C8\n  depends on\n  memory load from the location of the second store\n    in /data/listing_12-17!main at listing_12-17.c:50 - 0x6BD\n#=============================================================\n# Diagnostic # 2: Missing cache flush\n#-------------------\n  Memory store\n    of size 4 at address 0x7F9C68893000 (offset 0x0 in /mnt/pmem/file)\n    in /data/listing_12-16!main at listing_12-16.c:52 - 0x687\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1109.21 sec"
            },
            {
              "page_number": 248,
              "text": "224\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_\nline> - 0x223D3\n    in /data/listing_12-16!_start at <unknown_file>:<unknown_line> - 0x534\n  is not flushed before\n  memory is unmapped\n    in /data/listing_12-16!main at listing_12-16.c:54 - 0x699\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_\nline> - 0x223D3\n    in /data/listing_12-16!_start at <unknown_file>:<unknown_line> - 0x534\nAnalysis complete. 2 diagnostic(s) reported.\nThe output is very verbose, but it is easy to follow. We get two missing cache flushes \n(diagnostics 1 and 2) corresponding to lines 51 and 52 of listing_12-16.c. We do these \nwrites to the locations in the mapped persistent memory pointed by variables flag \nand data. The first diagnostic says that the first memory store is not flushed before the \nsecond store, while, at the same time, there is a load dependency of the first store to the \nsecond. This is exactly what we intended.\nThe second diagnostic says that the second store (to the flag) itself is never actually \nflushed before ending. Even if we flush the first store correctly before we write the flag, \nwe must still flush the flag to make sure the dependency works.\nTo open the results in the Intel Inspector GUI, you can use the -insp option when \ngenerating the report, for example:\n$ pmeminsp rp -insp -- listing_12-16 listing_12-17\nThis generates a directory called r000pmem inside the analysis directory \n(.pmeminspdata by default). Launch the GUI running inspxe-gui and open the result \nfile by going to File ➤ Open ➤ Result and selecting the file r000pmem/r000pmem.inspxe. \nYou should see something similar to what is shown in Figure 12-3.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1116.69 sec"
            },
            {
              "page_number": 249,
              "text": "225\nThe GUI shows the same information as the command-line analysis but in a more \nreadable way by highlighting the errors directly on our source code. As Figure 12-3 \nshows, the modification of the flag is called “primary store.”\nIn Figure 12-4, the second diagnosis is selected in the Problems pane, showing the \nmissing flush for the flag itself.\nFigure 12-3.  GUI of Intel Inspector showing results for Listing 12-18 (diagnostic 1)\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page249_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page249_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1118.73 sec"
            },
            {
              "page_number": 250,
              "text": "226\nTo conclude this section, we fix the code and rerun the analysis with Persistence \nInspector. The code in Listing 12-19 adds the necessary flushes to Listing 12-14.\nListing 12-19.  Example of writing to persistent memory with a write \ndependency. The code flushes both writes\n    33  #include <emmintrin.h>\n    34  #include <stdint.h>\n    35  #include <stdio.h>\n    36  #include <sys/mman.h>\n    37  #include <fcntl.h>\n    38  #include <string.h>\n    39\n    40  void flush(const void *addr, size_t len) {\n    41      uintptr_t flush_align = 64, uptr;\n    42      for (uptr = (uintptr_t)addr & ~(flush_align - 1);\n    43              uptr < (uintptr_t)addr + len;\n    44              uptr += flush_align)\nFigure 12-4.  GUI of Intel Inspector showing results for Listing 12-20 (diagnostic #2)\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page250_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page250_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1121.70 sec"
            },
            {
              "page_number": 251,
              "text": "227\n    45          _mm_clflush((char *)uptr);\n    46  }\n    47\n    48  int main(int argc, char *argv[]) {\n    49      int fd, *ptr, *data, *flag;\n    50\n    51      fd = open(\"/mnt/pmem/file\", O_CREAT|O_RDWR, 0666);\n    52      posix_fallocate(fd, 0, sizeof(int) * 2);\n    53\n    54      ptr = (int *) mmap(NULL, sizeof(int) * 2,\n    55                         PROT_READ | PROT_WRITE,\n    56                         MAP_SHARED_VALIDATE | MAP_SYNC,\n    57                         fd, 0);\n    58\n    59      data = &(ptr[1]);\n    60      flag = &(ptr[0]);\n    61      *data = 1234;\n    62      flush((void *) data, sizeof(int));\n    63      *flag = 1;\n    64      flush((void *) flag, sizeof(int));\n    65\n    66      munmap(ptr, 2 * sizeof(int));\n    67      return 0;\n    68  }\nListing 12-20 executes Persistence Inspector against the modified code from  \nListing 12-19, then the reader code from Listing 12-15, and finally running the report, \nwhich says that no problems were detected.\nListing 12-20.  Running full analysis with Intel Inspector – Persistence Inspector \nwith code Listings 12-19 and 12-15\n$ pmeminsp cb -pmem-file /mnt/pmem/file -- ./listing_12-19\n++ Analysis starts\n++ Analysis completes\n++ Data is stored in folder \"/data/.pmeminspdata/data/listing_12-19\"\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1126.42 sec"
            },
            {
              "page_number": 252,
              "text": "228\n$ pmeminsp ca -pmem-file /mnt/pmem/file -- ./listing_12-15\n++ Analysis starts\ndata = 1234\n++ Analysis completes\n++ Data is stored in folder \"/data/.pmeminspdata/data/listing_12-15\"\n$ pmeminsp rp -- listing_12-19 listing_12-15\nAnalysis complete. No problems detected.\n\u0007Stores Not Added into a Transaction\nWhen working within a transaction block, it is assumed that all the modified persistent \nmemory addresses were added to it at the beginning, which also implies that their \nprevious values are copied to an undo log. This allows the transaction to implicitly flush \nadded memory addresses at the end of the block or roll back to the old values in the \nevent of an unexpected failure. A modification within a transaction to an address that is \nnot added to the transaction is a bug that you must be aware of.\nConsider the code in Listing 12-21 that uses the libpmemobj library from PMDK. It \nshows an example of writing within a transaction using a memory address that is not \nexplicitly tracked by the transaction.\nListing 12-21.  Example of writing within a transaction with a memory address \nnot added to the transaction\n    33  #include <libpmemobj.h>\n    34\n    35  struct my_root {\n    36      int value;\n    37      int is_odd;\n    38  };\n    39\n    40  // registering type 'my_root' in the layout\n    41  POBJ_LAYOUT_BEGIN(example);\n    42  POBJ_LAYOUT_ROOT(example, struct my_root);\n    43  POBJ_LAYOUT_END(example);\n    44\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1132.15 sec"
            },
            {
              "page_number": 253,
              "text": "229\n    45  int main(int argc, char *argv[]) {\n    46      // creating the pool\n    47      PMEMobjpool *pop= pmemobj_create(\"/mnt/pmem/pool\",\n    48                        POBJ_LAYOUT_NAME(example),\n    49                        (1024 * 1024 * 100), 0666);\n    50\n    51      // transation\n    52      TX_BEGIN(pop) {\n    53          TOID(struct my_root) root\n    54              = POBJ_ROOT(pop, struct my_root);\n    55\n    56          // adding root.value to the transaction\n    57          TX_ADD_FIELD(root, value);\n    58\n    59          D_RW(root)->value = 4;\n    60          D_RW(root)->is_odd = D_RO(root)->value % 2;\n    61      } TX_END\n    62\n    63      return 0;\n    64  }\nNote  For a refresh on the definitions of a layout, root object, or macros used in \nListing 12-21, see Chapter 7 where we introduce libpmemobj.\nIn lines 35-38, we create a my_root data structure, which has two integer members: \nvalue and is_odd. These integers are modified inside a transaction (lines 52-61), \nsetting value=4 and is_odd=0. On line 57, we are only adding the value variable to the \ntransaction, leaving is_odd out. Given that persistent memory is not natively supported \nin C, there is no way for the compiler to warn you about this. The compiler cannot \ndistinguish between pointers to volatile memory vs. those to persistent memory.\nListing 12-22 shows the response from running the code through pmemcheck.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1136.86 sec"
            },
            {
              "page_number": 254,
              "text": "230\nListing 12-22.  Running pmemcheck with code Listing 12-21\n$ valgrind --tool=pmemcheck ./listing_12-21\n==48660== pmemcheck-1.0, a simple persistent store checker\n==48660== Copyright (c) 2014-2016, Intel Corporation\n==48660== Using Valgrind-3.14.0 and LibVEX; rerun with -h for copyright info\n==48660== Command: ./listing_12-21\n==48660==\n==48660==\n==48660== Number of stores not made persistent: 1\n==48660== Stores not made persistent properly:\n==48660== [0]    at 0x400C2D: main (listing_12-25.c:60)\n==48660==       Address: 0x7dc0554      size: 4 state: DIRTY\n==48660== Total memory not made persistent: 4\n==48660==\n==48660== Number of stores made without adding to transaction: 1\n==48660== Stores made without adding to transactions:\n==48660== [0]    at 0x400C2D: main (listing_12-25.c:60)\n==48660==       Address: 0x7dc0554      size: 4\n==48660== ERROR SUMMARY: 2 errors\nAlthough they are both related to the same root cause, pmemcheck identified two \nissues. One is the error we expected; that is, we have a store inside a transaction that \nwas not added to it. The other error says that we are not flushing the store. Since \ntransactional stores are flushed automatically when the program exits the transaction, \nfinding two errors per store to a location not included within a transaction should be \ncommon in pmemcheck.\nPersistence Inspector has a more user-friendly output, as shown in Listing 12-23.\nListing 12-23.  Generating a report with Intel Inspector – Persistence Inspector \nfor code Listing 12-21\n$ pmeminsp cb -pmem-file /mnt/pmem/pool -- ./listing_12-21\n++ Analysis starts\n++ Analysis completes\n++ Data is stored in folder \"/data/.pmeminspdata/data/listing_12-21\"\n$\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1142.79 sec"
            },
            {
              "page_number": 255,
              "text": "231\n$ pmeminsp rp -- ./listing_12-21\n#=============================================================\n# Diagnostic # 1: Store without undo log\n#-------------------\n  Memory store\n    of size 4 at address 0x7FAA84DC0554 (offset 0x3C0554 in /mnt/pmem/pool)\n    in /data/listing_12-21!main at listing_12-21.c:60 - 0xC2D\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_\nline> - 0x223D3\n    in /data/listing_12-21!_start at <unknown_file>:<unknown_line> - 0x954\n  is not undo logged in\n  transaction\n    in /data/listing_12-21!main at listing_12-21.c:52 - 0xB67\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_\nline> - 0x223D3\n    in /data/listing_12-21!_start at <unknown_file>:<unknown_line> - 0x954\nAnalysis complete. 1 diagnostic(s) reported.\nWe do not perform an after-unfortunate-event phase analysis here because we are \nonly concerned about transactions.\nWe can fix the problem reported in Listing 12-23 by adding the whole root object to \nthe transaction using TX_ADD(root), as shown on line 53 in Listing 12-24.\nListing 12-24.  Example of adding an object and writing it within a transaction\n    32  #include <libpmemobj.h>\n    33\n    34  struct my_root {\n    35      int value;\n    36      int is_odd;\n    37  };\n    38\n    39  POBJ_LAYOUT_BEGIN(example);\n    40  POBJ_LAYOUT_ROOT(example, struct my_root);\n    41  POBJ_LAYOUT_END(example);\n    42\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1147.51 sec"
            },
            {
              "page_number": 256,
              "text": "232\n    43  int main(int argc, char *argv[]) {\n    44      PMEMobjpool *pop= pmemobj_create(\"/mnt/pmem/pool\",\n    45                        POBJ_LAYOUT_NAME(example),\n    46                        (1024 * 1024 * 100), 0666);\n    47\n    48      TX_BEGIN(pop) {\n    49          TOID(struct my_root) root\n    50              = POBJ_ROOT(pop, struct my_root);\n    51\n    52          // adding full root to the transaction\n    53          TX_ADD(root);\n    54\n    55          D_RW(root)->value = 4;\n    56          D_RW(root)->is_odd = D_RO(root)->value % 2;\n    57      } TX_END\n    58\n    59      return 0;\n    60  }\nIf we run the code through pmemcheck, as shown in Listing 12-25, no issues are \nreported.\nListing 12-25.  Running pmemcheck with code Listing 12-24\n$ valgrind --tool=pmemcheck ./listing_12-24\n==80721== pmemcheck-1.0, a simple persistent store checker\n==80721== Copyright (c) 2014-2016, Intel Corporation\n==80721== \u0007Using Valgrind-3.14.0 and LibVEX; rerun with -h for copyright \ninfo\n==80721== Command: ./listing_12-24\n==80721==\n==80721==\n==80721== Number of stores not made persistent: 0\n==80721== ERROR SUMMARY: 0 errors\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1152.32 sec"
            },
            {
              "page_number": 257,
              "text": "233\nSimilarly, no issues are reported by Persistence Inspector in Listing 12-26.\nListing 12-26.  Generating report with Intel Inspector – Persistence Inspector for \ncode Listing 12-24\n$ pmeminsp cb -pmem-file /mnt/pmem/pool -- ./listing_12-24\n++ Analysis starts\n++ Analysis completes\n++ Data is stored in folder \"/data/.pmeminspdata/data/listing_12-24\"\n$\n$ pmeminsp rp -- ./listing_12-24\nAnalysis complete. No problems detected.\nAfter properly adding all the memory that will be modified to the transaction, both \ntools report that no problems were found.\n\u0007Memory Added to Two Different Transactions\nIn the case where one program can work with multiple transactions simultaneously, \nadding the same memory object to multiple transactions can potentially corrupt data. \nThis can occur in PMDK, for example, where the library maintains a different transaction \nper thread. If two threads write to the same object within different transactions, after an \napplication crash, a thread might overwrite modifications made by another thread in a \ndifferent transaction. In database systems, this problem is known as dirty reads. Dirty \nreads violate the isolation requirement of the ACID (atomicity, consistency, isolation, \ndurability) properties, as shown in Figure 12-5.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1156.93 sec"
            },
            {
              "page_number": 258,
              "text": "234\nIn Figure 12-5, time is shown in the y axis with time progressing downward. These \noperations occur in the following order:\n•\t\nAssume X=0 when the application starts.\n•\t\nA main() function creates two threads: Thread 1 and Thread 2. Both \nthreads are intended to start their own transactions and acquire the \nlock to modify X.\n•\t\nSince Thread 1 runs first, it acquires the lock on X first. It then \nadds the X variable to the transaction before incrementing X by 5. \nTransparent to the program, the value of X (X=0) is added to the undo \nlog when X was added to the transaction. Since the transaction is not \nyet complete, the application has not yet explicitly flushed the value.\n•\t\nThread 2 starts, begins its own transaction, acquires the lock, reads \nthe value of X (which is now 5), adds X=5 to the undo log, and \nincrements it by 5. The transaction completes successfully, and \nThread 2 flushes the CPU caches. Now, x=10.\nFigure 12-5.  The rollback mechanism for the unfinished transaction in Thread 1 \nis also overriding the changes made by Thread 2, even though the transaction for \nThread 2 finishes correctly\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page258_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page258_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1160.00 sec"
            },
            {
              "page_number": 259,
              "text": "235\n•\t\nUnfortunately, the program crashes after Thread 2 successfully \ncompletes its transaction but before Thread 1 was able to finish its \ntransaction and flush its value.\nThis scenario leaves the application with an invalid, but consistent, value of x=10. \nSince transactions are atomic, all changes done within them are not valid until they \nsuccessfully complete.\nWhen the application starts, it knows it must perform a recovery operation due \nto the previous crash and will replay the undo logs to rewind the partial update made \nby Thread 1. The undo log restores the value of X=0, which was correct when Thread 1 \nadded its entry. The expected value of X should be X=5 in this situation, but the undo log \nputs X=0. You can probably see the huge potential for data corruption that this situation \ncan produce.\nWe describe concurrency for multithreaded applications in Chapter 14. Using \nlibpmemobj-cpp, the C++ language binding library to libpmemobj, concurrency issues \nare very easy to resolve because the API allows us to pass a list of locks using lambda \nfunctions when transactions are created. Chapter 8 discusses libpmemobj-cpp and \nlambda functions in more detail.\nListing 12-27 shows how you can use a single mutex to lock a whole transaction. This \nmutex can either be a standard mutex (std::mutex) if the mutex object resides in volatile \nmemory or a pmem mutex (pmem::obj::mutex) if the mutex object resides in persistent \nmemory.\nListing 12-27.  Example of a libpmemobj++ transaction whose writes are both \natomic – with respect to persistent memory – and isolated – in a multithreaded \nscenario. The mutex is passed to the transaction as a parameter\ntransaction::run (pop, [&] {\n     ...\n     // all writes here are atomic and thread safe\n     ...\n }, mutex);\nConsider the code in Listing 12-28 that simultaneously adds the same memory \nregion to two different transactions.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1165.74 sec"
            },
            {
              "page_number": 260,
              "text": "236\nListing 12-28.  Example of two threads simultaneously adding the same \npersistent memory location to their respective transactions\n    33  #include <libpmemobj.h>\n    34  #include <pthread.h>\n    35\n    36  struct my_root {\n    37      int value;\n    38      int is_odd;\n    39  };\n    40\n    41  POBJ_LAYOUT_BEGIN(example);\n    42  POBJ_LAYOUT_ROOT(example, struct my_root);\n    43  POBJ_LAYOUT_END(example);\n    44\n    45  pthread_mutex_t lock;\n    46\n    47  // function to be run by extra thread\n    48  void *func(void *args) {\n    49      PMEMobjpool *pop = (PMEMobjpool *) args;\n    50\n    51      TX_BEGIN(pop) {\n    52          pthread_mutex_lock(&lock);\n    53          TOID(struct my_root) root\n    54              = POBJ_ROOT(pop, struct my_root);\n    55          TX_ADD(root);\n    56          D_RW(root)->value = D_RO(root)->value + 3;\n    57          pthread_mutex_unlock(&lock);\n    58      } TX_END\n    59  }\n    60\n    61  int main(int argc, char *argv[]) {\n    62      PMEMobjpool *pop= pmemobj_create(\"/mnt/pmem/pool\",\n    63                        POBJ_LAYOUT_NAME(example),\n    64                        (1024 * 1024 * 10), 0666);\n    65\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1170.75 sec"
            },
            {
              "page_number": 261,
              "text": "237\n    66      pthread_t thread;\n    67      pthread_mutex_init(&lock, NULL);\n    68\n    69      TX_BEGIN(pop) {\n    70          pthread_mutex_lock(&lock);\n    71          TOID(struct my_root) root\n    72              = POBJ_ROOT(pop, struct my_root);\n    73          TX_ADD(root);\n    74          pthread_create(&thread, NULL,\n    75                         func, (void *) pop);\n    76          D_RW(root)->value = D_RO(root)->value + 4;\n    77          D_RW(root)->is_odd = D_RO(root)->value % 2;\n    78          pthread_mutex_unlock(&lock);\n    79          // wait to make sure other thread finishes 1st\n    80          pthread_join(thread, NULL);\n    81      } TX_END\n    82\n    83      pthread_mutex_destroy(&lock);\n    84      return 0;\n    85  }\n•\t\nLine 69: The main thread starts a transaction and adds the root data \nstructure to it (line 73).\n•\t\nLine 74: We create a new thread by calling pthread_create() and \nhave it execute the func() function. This function also starts a \ntransaction (line 51) and adds the root data structure to it (line 55).\n•\t\nBoth threads will simultaneously modify all or part of the same data \nbefore finishing their transactions. We force the second thread to \nfinish first by making the main thread wait on pthread_join().\nListing 12-29 shows code execution with pmemcheck, and the result warns us that we \nhave overlapping regions registered in different transactions.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1175.37 sec"
            },
            {
              "page_number": 262,
              "text": "238\nListing 12-29.  Running pmemcheck with Listing 12-28\n$ valgrind --tool=pmemcheck ./listing_12-28\n==97301== pmemcheck-1.0, a simple persistent store checker\n==97301== Copyright (c) 2014-2016, Intel Corporation\n==97301== Using Valgrind-3.14.0 and LibVEX; rerun with -h for copyright info\n==97301== Command: ./listing_12-28\n==97301==\n==97301==\n==97301== Number of stores not made persistent: 0\n==97301==\n==97301== \u0007Number of overlapping regions registered in different \ntransactions: 1\n==97301== Overlapping regions:\n==97301== [0]    \u0007at 0x4E6B0BC: pmemobj_tx_add_snapshot (in /usr/lib64/\nlibpmemobj.so.1.0.0)\n==97301==    \u0007by 0x4E6B5F8: pmemobj_tx_add_common.constprop.18 (in /usr/\nlib64/libpmemobj.so.1.0.0)\n==97301==    \u0007by 0x4E6C62F: pmemobj_tx_add_range (in /usr/lib64/libpmemobj.\nso.1.0.0)\n==97301==    by 0x400DAC: func (listing_12-28.c:55)\n==97301==    by 0x4C2DDD4: start_thread (in /usr/lib64/libpthread-2.17.so)\n==97301==    by 0x5180EAC: clone (in /usr/lib64/libc-2.17.so)\n==97301==     Address: 0x7dc0550    size: 8    tx_id: 2\n==97301==    First registered here:\n==97301== [0]'   \u0007at 0x4E6B0BC: pmemobj_tx_add_snapshot (in /usr/lib64/\nlibpmemobj.so.1.0.0)\n==97301==    \u0007by 0x4E6B5F8: pmemobj_tx_add_common.constprop.18 (in /usr/\nlib64/libpmemobj.so.1.0.0)\n==97301==    \u0007by 0x4E6C62F: pmemobj_tx_add_range (in /usr/lib64/libpmemobj.\nso.1.0.0)\n==97301==    by 0x400F23: main (listing_12-28.c:73)\n==97301==    Address: 0x7dc0550    size: 8    tx_id: 1\n==97301== ERROR SUMMARY: 1 errors\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1180.07 sec"
            },
            {
              "page_number": 263,
              "text": "239\nListing 12-30 shows the same code run with Persistence Inspector, which also reports \n“Overlapping regions registered in different transactions” in diagnostic 25. The first 24 \ndiagnostic results were related to stores not added to our transactions corresponding \nwith the locking and unlocking of our volatile mutex; these can be ignored.\nListing 12-30.  Generating a report with Intel Inspector – Persistence Inspector \nfor code Listing 12-28\n$ pmeminsp rp -- ./listing_12-28\n...\n#=============================================================\n# Diagnostic # 25: Overlapping regions registered in different transactions\n#-------------------\n  transaction\n    in /data/listing_12-28!main at listing_12-28.c:69 - 0xEB6\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_line> \n- 0x223D3\n    in /data/listing_12-28!_start at <unknown_file>:<unknown_line> - 0xB44\n  protects\n  memory region\n    in /data/listing_12-28!main at listing_12-28.c:73 - 0xF1F\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_line> \n- 0x223D3\n    in /data/listing_12-28!_start at <unknown_file>:<unknown_line> - 0xB44\n  overlaps with\n  memory region\n    in /data/listing_12-28!func at listing_12-28.c:55 - 0xDA8\n    \u0007in /lib64/libpthread.so.0!start_thread at <unknown_file>:<unknown_line> \n- 0x7DCD\n    in /lib64/libc.so.6!__clone at <unknown_file>:<unknown_line> - 0xFDEAB\nAnalysis complete. 25 diagnostic(s) reported.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1187.96 sec"
            },
            {
              "page_number": 264,
              "text": "240\n\u0007Memory Overwrites\nWhen multiple modifications to the same persistent memory location occur before \nthe location is made persistent (that is, flushed), a memory overwrite occurs. This is \na potential data corruption source if a program crashes because the final value of the \npersistent variable can be any of the values written between the last flush and the crash. \nIt is important to know that this may not be an issue if it is in the code by design. We \nrecommend using volatile variables for short-lived data and only write to persistent \nvariables when you want to persist data.\nConsider the code in Listing 12-31, which writes twice to the data variable inside the \nmain() function (lines 62 and 63) before we call flush() on line 64.\nListing 12-31.  Example of persistent memory overwriting – variable data – \nbefore flushing\n    33  #include <emmintrin.h>\n    34  #include <stdint.h>\n    35  #include <stdio.h>\n    36  #include <sys/mman.h>\n    37  #include <fcntl.h>\n    38  #include <valgrind/pmemcheck.h>\n    39\n    40  void flush(const void *addr, size_t len) {\n    41      uintptr_t flush_align = 64, uptr;\n    42      for (uptr = (uintptr_t)addr & ~(flush_align - 1);\n    43              uptr < (uintptr_t)addr + len;\n    44              uptr += flush_align)\n    45          _mm_clflush((char *)uptr);\n    46  }\n    47\n    48  int main(int argc, char *argv[]) {\n    49      int fd, *data;\n    50\n    51      fd = open(\"/mnt/pmem/file\", O_CREAT|O_RDWR, 0666);\n    52      posix_fallocate(fd, 0, sizeof(int));\n    53\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1192.56 sec"
            },
            {
              "page_number": 265,
              "text": "241\n    54      data = (int *)mmap(NULL, sizeof(int),\n    55              PROT_READ | PROT_WRITE,\n    56              MAP_SHARED_VALIDATE | MAP_SYNC,\n    57              fd, 0);\n    58      VALGRIND_PMC_REGISTER_PMEM_MAPPING(data,\n    59                                         sizeof(int));\n    60\n    61      // writing twice before flushing\n    62      *data = 1234;\n    63      *data = 4321;\n    64      flush((void *)data, sizeof(int));\n    65\n    66      munmap(data, sizeof(int));\n    67      VALGRIND_PMC_REMOVE_PMEM_MAPPING(data,\n    68                                       sizeof(int));\n    69      return 0;\n    70  }\nListing 12-32 shows the report from pmemcheck with the code from Listing 12-31.  \nTo make pmemcheck look for overwrites, we must use the --mult-stores=yes option.\nListing 12-32.  Running pmemcheck with Listing 12-31\n$ valgrind --tool=pmemcheck --mult-stores=yes ./listing_12-31\n==25609== pmemcheck-1.0, a simple persistent store checker\n==25609== Copyright (c) 2014-2016, Intel Corporation\n==25609== Using Valgrind-3.14.0 and LibVEX; rerun with -h for copyright info\n==25609== Command: ./listing_12-31\n==25609==\n==25609==\n==25609== Number of stores not made persistent: 0\n==25609==\n==25609== Number of overwritten stores: 1\n==25609== Overwritten stores before they were made persistent:\n==25609== [0]    at 0x400962: main (listing_12-31.c:62)\n==25609==       Address: 0x4023000      size: 4 state: DIRTY\n==25609== ERROR SUMMARY: 1 errors\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1198.15 sec"
            },
            {
              "page_number": 266,
              "text": "242\npmemcheck reports that we have overwritten stores. We can fix this problem by either \ninserting a flushing instruction between both writes, if we forgot to flush, or by moving \none of the stores to volatile data if that store corresponds to short-lived data.\nAt the time of publication, Persistence Inspector does not support checking for \noverwritten stores. As you have seen, Persistence Inspector does not consider a missing \nflush an issue unless there is a write dependency. In addition, it does not consider this a \nperformance problem because writing to the same variable in a short time span is likely \nto hit the CPU caches anyway, rendering the latency differences between DRAM and \npersistent memory irrelevant.\n\u0007Unnecessary Flushes\nFlushing should be done carefully. Detecting unnecessary flushes, such as redundant \nones, can help improve code performance. The code in Listing 12-33 shows a redundant \ncall to the flush() function on line 64.\nListing 12-33.  Example of redundant flushing of a persistent memory variable\n    33  #include <emmintrin.h>\n    34  #include <stdint.h>\n    35  #include <stdio.h>\n    36  #include <sys/mman.h>\n    37  #include <fcntl.h>\n    38  #include <valgrind/pmemcheck.h>\n    39\n    40  void flush(const void *addr, size_t len) {\n    41      uintptr_t flush_align = 64, uptr;\n    42      for (uptr = (uintptr_t)addr & ~(flush_align - 1);\n    43              uptr < (uintptr_t)addr + len;\n    44              uptr += flush_align)\n    45          _mm_clflush((char *)uptr);\n    46  }\n    47\n    48  int main(int argc, char *argv[]) {\n    49      int fd, *data;\n    50\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1204.95 sec"
            },
            {
              "page_number": 267,
              "text": "243\n    51      fd = open(\"/mnt/pmem/file\", O_CREAT|O_RDWR, 0666);\n    52      posix_fallocate(fd, 0, sizeof(int));\n    53\n    54      data = (int *)mmap(NULL, sizeof(int),\n    55              PROT_READ | PROT_WRITE,\n    56              MAP_SHARED_VALIDATE | MAP_SYNC,\n    57              fd, 0);\n    58\n    59      VALGRIND_PMC_REGISTER_PMEM_MAPPING(data,\n    60                                         sizeof(int));\n    61\n    62      *data = 1234;\n    63      flush((void *)data, sizeof(int));\n    64      flush((void *)data, sizeof(int)); // extra flush\n    65\n    66      munmap(data, sizeof(int));\n    67      VALGRIND_PMC_REMOVE_PMEM_MAPPING(data,\n    68                                       sizeof(int));\n    69      return 0;\n    70  }\nWe can use pmemcheck to detect redundant flushes using --flush-check=yes option, \nas shown in Listing 12-34.\nListing 12-34.  Running pmemcheck with Listing 12-33\n$ valgrind --tool=pmemcheck --flush-check=yes ./listing_12-33\n==104125== pmemcheck-1.0, a simple persistent store checker\n==104125== Copyright (c) 2014-2016, Intel Corporation\n==104125== Using Valgrind-3.14.0 and LibVEX; rerun with -h for copyright info\n==104125== Command: ./listing_12-33\n==104125==\n==104125==\n==104125== Number of stores not made persistent: 0\n==104125==\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1208.54 sec"
            },
            {
              "page_number": 268,
              "text": "244\n==104125== Number of unnecessary flushes: 1\n==104125== [0]    at 0x400868: flush (emmintrin.h:1459)\n==104125==    by 0x400989: main (listing_12-33.c:64)\n==104125==      Address: 0x4023000      size: 64\n==104125== ERROR SUMMARY: 1 errors\nTo showcase Persistence Inspector, Listing 12-35 has code with a write dependency, \nsimilar to what we did for Listing 12-11 in Listing 12-19. The extra flush occurs on line 65.\nListing 12-35.  Example of writing to persistent memory with a write \ndependency. The code does an extra flush for the flag\n    33  #include <emmintrin.h>\n    34  #include <stdint.h>\n    35  #include <stdio.h>\n    36  #include <sys/mman.h>\n    37  #include <fcntl.h>\n    38  #include <string.h>\n    39\n    40  void flush(const void *addr, size_t len) {\n    41      uintptr_t flush_align = 64, uptr;\n    42      for (uptr = (uintptr_t)addr & ~(flush_align - 1);\n    43              uptr < (uintptr_t)addr + len;\n    44              uptr += flush_align)\n    45          _mm_clflush((char *)uptr);\n    46  }\n    47\n    48  int main(int argc, char *argv[]) {\n    49      int fd, *ptr, *data, *flag;\n    50\n    51      fd = open(\"/mnt/pmem/file\", O_CREAT|O_RDWR, 0666);\n    52      posix_fallocate(fd, 0, sizeof(int) * 2);\n    53\n    54      ptr = (int *) mmap(NULL, sizeof(int) * 2,\n    55              PROT_READ | PROT_WRITE,\n    56              MAP_SHARED_VALIDATE | MAP_SYNC,\n    57              fd, 0);\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1214.27 sec"
            },
            {
              "page_number": 269,
              "text": "245\n    58      data = &(ptr[1]);\n    59      flag = &(ptr[0]);\n    60\n    61      *data = 1234;\n    62      flush((void *) data, sizeof(int));\n    63      *flag = 1;\n    64      flush((void *) flag, sizeof(int));\n    65      flush((void *) flag, sizeof(int)); // extra flush\n    66\n    67      munmap(ptr, 2 * sizeof(int));\n    68      return 0;\n    69  }\nListing 12-36 uses the same reader program from Listing 12-15 to show the analysis \nfrom Persistence Inspector. As before, we first collect data from the writer program, \nthen the reader program, and finally run the report to identify any issues.\nListing 12-36.  Running Intel Inspector – Persistence Inspector with Listing 12-35 \n(writer) and Listing 12-15 (reader)\n$ pmeminsp cb -pmem-file /mnt/pmem/file -- ./listing_12-35\n++ Analysis starts\n++ Analysis completes\n++ Data is stored in folder \"/data/.pmeminspdata/data/listing_12-35\"\n$ pmeminsp ca -pmem-file /mnt/pmem/file -- ./listing_12-15\n++ Analysis starts\ndata = 1234\n++ Analysis completes\n++ Data is stored in folder \"/data/.pmeminspdata/data/listing_12-15\"\n$ pmeminsp rp -- ./listing_12-35 ./listing_12-15\n#=============================================================\n# Diagnostic # 1: Redundant cache flush\n#-------------------\n  Cache flush\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1219.09 sec"
            },
            {
              "page_number": 270,
              "text": "246\n    of size 64 at address 0x7F3220C55000 (offset 0x0 in /mnt/pmem/file)\n    in /data/listing_12-35!flush at listing_12-35.c:45 - 0x674\n    in /data/listing_12-35!main at listing_12-35.c:64 - 0x73F\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_line> \n- 0x223D3\n    in /data/listing_12-35!_start at <unknown_file>:<unknown_line> - 0x574\n  is redundant with regard to\n   cache flush\n    of size 64 at address 0x7F3220C55000 (offset 0x0 in /mnt/pmem/file)\n    in /data/listing_12-35!flush at listing_12-35.c:45 - 0x674\n    in /data/listing_12-35!main at listing_12-35.c:65 - 0x750\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_line> \n- 0x223D3\n    in /data/listing_12-35!_start at <unknown_file>:<unknown_line> - 0x574\n  of\n  memory store\n    of size 4 at address 0x7F3220C55000 (offset 0x0 in /mnt/pmem/file)\n    in /data/listing_12-35!main at listing_12-35.c:63 - 0x72D\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_line> \n- 0x223D3\n    in /data/listing_12-35!_start at <unknown_file>:<unknown_line> - 0x574\nThe Persistence Inspector report warns about the redundant cache flush within \nthe main() function on line 65 of the listing_12-35.c program file – “main at \nlisting_12-35.c:65”. Solving these issues is as easy as deleting all the unnecessary \nflushes, and the result will improve the application’s performance.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1223.80 sec"
            },
            {
              "page_number": 271,
              "text": "247\n\u0007Out-of-Order Writes\nWhen developing software for persistent memory, remember that even if a cache line is \nnot explicitly flushed, that does not mean the data is still in the CPU caches. For example, \nthe CPU could have evicted it due to cache pressure or other reasons. Furthermore, the \nsame way that writes that are not flushed properly may produce bugs in the event of an \nunexpected application crash, so do automatically evicted dirty cache lines if they violate \nsome expected order of writes that the applications rely on.\nTo better understand this problem, explore how flushing works in the x86_64 \nand AMD64 architectures. From the user space, we can issue any of the following \ninstructions to ensure our writes reach the persistent media:\n•\t\nCLFLUSH\n•\t\nCLFLUSHOPT (needs SFENCE)\n•\t\nCLWB (needs SFENCE)\n•\t\nNon-temporal stores (needs SFENCE)\nThe only instruction that ensures each flush is issued in order is CLFUSH because \neach CLFLUSH instruction always does an implicit fence instruction (SFENCE). The other \ninstructions are asynchronous and can be issued in parallel and in any order. The CPU \ncan only guarantee that all flushes issued since the previous SFENCE have completed \nwhen a new SFENCE instruction is explicitly executed. Think of SFENCE instructions as \nsynchronization points (see Figure 12-6). For more information about these instructions, \nrefer to the Intel software developer manuals and the AMD software developer manuals.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1230.86 sec"
            },
            {
              "page_number": 272,
              "text": "248\nAs Figure 12-6 shows, we cannot guarantee the order with respect to how A and B \nwould be finally written to persistent memory. This happens because stores and flushes \nto A and B are done between synchronization points. The case of C is different. Using \nthe SFENCE instruction, we can be assured that C will always go after A and B have been \nflushed.\nKnowing this, you can now imagine how out-of-order writes could be a problem in \na program crash. If assumptions are made with respect to the order of writes between \nsynchronization points, or if you forget to add synchronization points between writes \nand flushes where strict order is essential (think of a “valid flag” for a variable write, \nwhere the variable needs to be written before the flag is set to valid), you may encounter \ndata consistency issues. Consider the pseudocode in Listing 12-37.\nFigure 12-6.  Example of how asynchronous flushing works. The SFENCE \ninstruction ensures a synchronization point between the writes to A and B on one \nside and to C on the other side\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page272_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page272_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1233.93 sec"
            },
            {
              "page_number": 273,
              "text": "249\nListing 12-37.  Pseudocode showcasing an out-of-order issue\n 1  writer () {\n 2          pcounter = 0;\n 3          flush (pcounter);\n 4          for (i=0; i<max; i++) {\n 5                  pcounter++;\n 6                  if (rand () % 2 == 0) {\n 7                          pcells[i].data = data ();\n 8                          flush (pcells[i].data);\n 9                          pcells[i].valid = True;\n10                  } else {\n11                          pcells[i].valid = False;\n12                  }\n13                  flush (pcells[i].valid);\n14          }\n15          flush (pcounter);\n16  }\n17\n18  reader () {\n19          for (i=0; i<pcounter; i++) {\n20                  if (pcells[i].valid == True) {\n21                          print (pcells[i].data);\n22                  }\n23          }\n24  }\nFor simplicity, assume that all flushes in Listing 12-37 are also synchronization \npoints; that is, flush() uses CLFLUSH. The logic of the program is very simple. There are \ntwo persistent memory variables: pcells and pcounter. The first is an array of tuples \n{data, valid} where data holds the data and valid is a flag indicating if data is valid \nor not. The second variable is a counter indicating how many elements in the array have \nbeen written correctly to persistent memory. In this case, the valid flag is not the one \nindicating whether or not the array position was written correctly to persistent memory. \nIn this case, the flag’s meaning only indicates if the function data() was called, that is, \nwhether or not data has meaningful data.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1238.54 sec"
            },
            {
              "page_number": 274,
              "text": "250\nAt first glance, the program appears correct. With every new iteration of the loop, \nthe counter is incremented, and then the array position is written and flushed. However, \npcounter is incremented before we write to the array, thus creating a discrepancy \nbetween pcounter and the actual number of committed entries in the array. Although it \nis true that pcounter is not flushed until after the loop, the program is only correct after \na crash if we assume that the changes to pcounter stay in the CPU caches (in that case, a \nprogram crash in the middle of the loop would simply leave the counter to zero).\nAs mentioned at the beginning of this section, we cannot make that assumption. A \ncache line can be evicted at any time. In the pseudocode example in Listing 12-37, we \ncould run into a bug where pcounter indicates that the array is longer than it really is, \nmaking the reader() read uninitialized memory.\nThe code in Listings 12-38 and 12-39 provide a C++ implementation of the \npseudocode from Listing 12-37. Both use libpmemobj-cpp from the PMDK. Listing 12-38 \nis the writer program, and Listing 12-39 is the reader.\nListing 12-38.  Example of writing to persistent memory with an out-of-order \nwrite bug\n    33  #include <emmintrin.h>\n    34  #include <unistd.h>\n    35  #include <stdio.h>\n    36  #include <string.h>\n    37  #include <stdint.h>\n    38  #include <libpmemobj++/persistent_ptr.hpp>\n    39  #include <libpmemobj++/make_persistent.hpp>\n    40  #include <libpmemobj++/make_persistent_array.hpp>\n    41  #include <libpmemobj++/transaction.hpp>\n    42  #include <valgrind/pmemcheck.h>\n    43\n    44  using namespace std;\n    45  namespace pobj = pmem::obj;\n    46\n    47  struct header_t {\n    48      uint32_t counter;\n    49      uint8_t reserved[60];\n    50  };\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1244.17 sec"
            },
            {
              "page_number": 275,
              "text": "251\n    51  struct record_t {\n    52      char name[63];\n    53      char valid;\n    54  };\n    55  struct root {\n    56      pobj::persistent_ptr<header_t> header;\n    57      pobj::persistent_ptr<record_t[]> records;\n    58  };\n    59\n    60  pobj::pool<root> pop;\n    61\n    62  int main(int argc, char *argv[]) {\n    63\n    64      // everything between BEGIN and END can be\n    65      // assigned a particular engine in pmreorder\n    66      VALGRIND_PMC_EMIT_LOG(\"PMREORDER_TAG.BEGIN\");\n    67\n    68      pop = pobj::pool<root>::open(\"/mnt/pmem/file\",\n    69                                   \"RECORDS\");\n    70      auto proot = pop.root();\n    71\n    72      // allocation of memory and initialization to zero\n    73      pobj::transaction::run(pop, [&] {\n    74          proot->header\n    75              = pobj::make_persistent<header_t>();\n    76          proot->header->counter = 0;\n    77          proot->records\n    78              = pobj::make_persistent<record_t[]>(10);\n    79          proot->records[0].valid = 0;\n    80      });\n    81\n    82      pobj::persistent_ptr<header_t> header\n    83          = proot->header;\n    84      pobj::persistent_ptr<record_t[]> records\n    85          = proot->records;\n    86\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1248.78 sec"
            },
            {
              "page_number": 276,
              "text": "252\n    87      VALGRIND_PMC_EMIT_LOG(\"PMREORDER_TAG.END\");\n    88\n    89      header->counter = 0;\n    90      for (uint8_t i = 0; i < 10; i++) {\n    91          header->counter++;\n    92          if (rand() % 2 == 0) {\n    93              snprintf(records[i].name, 63,\n    94                       \"record #%u\", i + 1);\n    95              pop.persist(records[i].name, 63); // flush\n    96              records[i].valid = 2;\n    97          } else\n    98              records[i].valid = 1;\n    99          pop.persist(&(records[i].valid), 1); // flush\n   100      }\n   101      pop.persist(&(header->counter), 4); // flush\n   102\n   103      pop.close();\n   104      return 0;\n   105  }\nListing 12-39.  Reading the data structure written by Listing 12-38 to persistent \nmemory\n    33  #include <stdio.h>\n    34  #include <stdint.h>\n    35  #include <libpmemobj++/persistent_ptr.hpp>\n    36\n    37  using namespace std;\n    38  namespace pobj = pmem::obj;\n    39\n    40  struct header_t {\n    41      uint32_t counter;\n    42      uint8_t reserved[60];\n    43  };\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1253.59 sec"
            },
            {
              "page_number": 277,
              "text": "253\n    44  struct record_t {\n    45      char name[63];\n    46      char valid;\n    47  };\n    48  struct root {\n    49      pobj::persistent_ptr<header_t> header;\n    50      pobj::persistent_ptr<record_t[]> records;\n    51  };\n    52\n    53  pobj::pool<root> pop;\n    54\n    55  int main(int argc, char *argv[]) {\n    56\n    57      pop = pobj::pool<root>::open(\"/mnt/pmem/file\",\n    58                                   \"RECORDS\");\n    59      auto proot = pop.root();\n    60      pobj::persistent_ptr<header_t> header\n    61          = proot->header;\n    62      pobj::persistent_ptr<record_t[]> records\n    63          = proot->records;\n    64\n    65      for (uint8_t i = 0; i < header->counter; i++) {\n    66          if (records[i].valid == 2) {\n    67              printf(\"found valid record\\n\");\n    68              printf(\"  name   = %s\\n\",\n    69                            records[i].name);\n    70          }\n    71      }\n    72\n    73      pop.close();\n    74      return 0;\n    75  }\nListing 12-38 (writer) uses the VALGRIND_PMC_EMIT_LOG macro to emit a pmreorder \nmessage when we get to lines 66 and 87. This will make sense later when we introduce \nout-of-order analysis using pmemcheck.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1258.20 sec"
            },
            {
              "page_number": 278,
              "text": "254\nNow we will run Persistence Inspector first. To perform out-of-order analysis, we \nmust use the -check-out-of-order-store option to the report phase. Listing 12-40 \nshows collecting the before and after data and then running the report.\nListing 12-40.  Running Intel Inspector – Persistence Inspector with Listing 12-38 \n(writer) and Listing 12-39 (reader)\n$ pmempool create obj --size=100M --layout=RECORDS /mnt/pmem/file\n$ pmeminsp cb -pmem-file /mnt/pmem/file -- ./listing_12-38\n++ Analysis starts\n++ Analysis completes\n++ Data is stored in folder \"/data/.pmeminspdata/data/listing_12-38\"\n$ pmeminsp ca -pmem-file /mnt/pmem/file -- ./listing_12-39\n++ Analysis starts\nfound valid record\n  name   = record #2\nfound valid record\n  name   = record #7\nfound valid record\n  name   = record #8\n++ Analysis completes\n++ Data is stored in folder \"/data/.pmeminspdata/data/listing_12-39\"\n$ pmeminsp rp -check-out-of-order-store -- ./listing_12-38 ./listing_12-39\n#=============================================================\n# Diagnostic # 1: Out-of-order stores\n#-------------------\n  Memory store\n    of size 4 at address 0x7FD7BEBC05D0 (offset 0x3C05D0 in /mnt/pmem/file)\n    in /data/listing_12-38!main at listing_12-38.cpp:91 - 0x1D0C\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_line> \n- 0x223D3\n    in /data/listing_12-38!_start at <unknown_file>:<unknown_line> - 0x1624\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1262.91 sec"
            },
            {
              "page_number": 279,
              "text": "255\n  is out of order with respect to\n  memory store\n    of size 1 at address 0x7FD7BEBC068F (offset 0x3C068F in /mnt/pmem/file)\n    in /data/listing_12-38!main at listing_12-38.cpp:98 - 0x1DAF\n    \u0007in /lib64/libc.so.6!__libc_start_main at <unknown_file>:<unknown_line> \n- 0x223D3\n    in /data/listing_12-38!_start at <unknown_file>:<unknown_line> - 0x1624\nThe Persistence Inspector report identifies an out-of-order store issue. The tool \nsays that incrementing the counter in line 91 (main at listing_12-38.cpp:91) is \nout of order with respect to writing the valid flag inside a record in line 98 (main at \nlisting_12-38.cpp:98).\nTo perform out-of-order analysis with pmemcheck, we must introduce a new tool \ncalled pmreorder. The pmreorder tool is included in PMDK from version 1.5 onward. \nThis stand-­alone Python tool performs a consistency check of persistent programs \nusing a store reordering mechanism. The pmemcheck tool cannot do this type of analysis, \nalthough it is still used to generate a detailed log of all the stores and flushes issued by an \napplication that pmreorder can parse. For example, consider Listing 12-41.\nListing 12-41.  Running pmemcheck to generate a detailed log of all the stores \nand flushes issued by Listing 12-38\n$ valgrind --tool=pmemcheck -q --log-stores=yes --log-stores-\nstacktraces=yes\n  --log-stores-stacktraces-depth=2 --print-summary=yes\n  --log-file=store_log.log ./listing_12-38\nThe meaning of each parameter is as follows:\n•\t\n-q silences unnecessary pmemcheck logs that pmreorder cannot parse.\n•\t\n--log-stores=yes tells pmemcheck to log all stores.\n•\t\n--log-stores-stacktraces=yes dumps stacktrace with each logged \nstore. This helps locate issues in your source code.\n•\t\n--log-stores-stacktraces-depth=2 is the depth of logged \nstacktraces. Adjust according to the level of information you need.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1269.98 sec"
            },
            {
              "page_number": 280,
              "text": "256\n•\t\n--print-summary=yes prints a summary on program exit. Why not?\n•\t\n--log-file=store_log.log logs everything to store_log.log.\nThe pmreorder tool works with the concept of “engines.” For example, the ReorderFull \nengine checks consistency for all the possible combinations of reorders of stores and \nflushes. This engine can be extremely slow for some programs, so you can use other \nengines such as ReorderPartial or NoReorderDoCheck. For more information, refer to the \npmreorder page, which has links to the man pages (https://pmem.io/pmdk/pmreorder/).\nBefore we run pmreorder, we need a program that can walk the list of records \ncontained within the memory pool and return 0 when the data structure is consistent, or \n1 otherwise. This program is similar to the reader shown in Listing 12-42.\nListing 12-42.  Checking the consistency of the data structure written in  \nListing 12-38\n    33  #include <stdio.h>\n    34  #include <stdint.h>\n    35  #include <libpmemobj++/persistent_ptr.hpp>\n    36\n    37  using namespace std;\n    38  namespace pobj = pmem::obj;\n    39\n    40  struct header_t {\n    41      uint32_t counter;\n    42      uint8_t reserved[60];\n    43  };\n    44  struct record_t {\n    45      char name[63];\n    46      char valid;\n    47  };\n    48  struct root {\n    49      pobj::persistent_ptr<header_t> header;\n    50      pobj::persistent_ptr<record_t[]> records;\n    51  };\n    52\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1274.61 sec"
            },
            {
              "page_number": 281,
              "text": "257\n    53  pobj::pool<root> pop;\n    54\n    55  int main(int argc, char *argv[]) {\n    56\n    57      pop = pobj::pool<root>::open(\"/mnt/pmem/file\",\n    58                                   \"RECORDS\");\n    59      auto proot = pop.root();\n    60      pobj::persistent_ptr<header_t> header\n    61          = proot->header;\n    62      pobj::persistent_ptr<record_t[]> records\n    63          = proot->records;\n    64\n    65      for (uint8_t i = 0; i < header->counter; i++) {\n    66          if (records[i].valid < 1 or\n    67                              records[i].valid > 2)\n    68              return 1; // data struc. corrupted\n    69      }\n    70\n    71      pop.close();\n    72      return 0; // everything ok\n    73  }\nThe program in Listing 12-42 iterates over all the records that we expect should have \nbeen written correctly to persistent memory (lines 65-69). It checks the valid flag for \neach record, which should be either 1 or 2 for the record to be correct (line 66). If an \nissue is detected, the checker will return 1 indicating data corruption.\nListing 12-43 shows a three-step process for analyzing the program:\n\t 1.\t Create an object type persistent memory pool, known as a \nmemory-mapped file, on /mnt/pmem/file of size 100MiB, and \nname the internal layout “RECORDS.”\n\t 2.\t Use the pmemcheck Valgrind tool to record data and call stacks \nwhile the program is running.\n\t 3.\t The pmreorder utility processes the store.log output file from \npmemcheck using the ReorderFull engine to produce a final report.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1280.25 sec"
            },
            {
              "page_number": 282,
              "text": "258\nListing 12-43.  First, a pool is created for Listing 12-38. Then, pmemcheck is run \nto get a detailed log of all the stores and flushes issued by Listing 12-38. Finally, \npmreorder is run with engine ReorderFull\n$ pmempool create obj --size=100M --layout=RECORDS /mnt/pmem/file\n$ valgrind --tool=pmemcheck -q --log-stores=yes --log-stores-\nstacktraces=yes --log-stores-stacktraces-depth=2 --print-summary=yes  \n--log-file=store.log ./listing_12-38\n$ pmreorder -l store.log -o output_file.log -x PMREORDER_\nTAG=NoReorderNoCheck -r ReorderFull -c prog -p ./listing_12-38\nThe meaning of each pmreorder option is as follows:\n•\t\n-l store_log.log is the input file generated by pmemcheck with all \nthe stores and flushes issued by the application.\n•\t\n-o output_file.log is the output file with the out-of-order analysis \nresults.\n•\t\n-x PMREORDER_TAG=NoReorderNoCheck assigns the engine \nNoReorderNoCheck to the code enclosed by the tag PMREORDER_TAG \n(see lines 66-87 from Listing 12-38). This is done to focus the analysis \non the loop only (lines 89-105 from Listing 12-38).\n•\t\n-r ReorderFull sets the initial reorder engine. In our case, ReorderFull.\n•\t\n-c prog is the consistency checker type. It can be prog (program) or \nlib (library).\n•\t\n-p ./checker is the consistency checker.\nOpening the generated file output_file.log, you should see entries similar to those \nin Listing 12-44 that highlight detected inconsistencies and problems within the code.\nListing 12-44.  Content from “output_file.log” generated by pmreorder showing a \ndetected inconsistency during the out-of-order analysis\nWARNING:pmreorder:File /mnt/pmem/file inconsistent\nWARNING:pmreorder:Call trace:\nStore [0]:\n    by  0x401D0C: main (listing_12-38.cpp:91)\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1287.90 sec"
            },
            {
              "page_number": 283,
              "text": "259\nThe report states that the problem resides at line 91 of the listing_12-38.cpp writer \nprogram. To fix listing_12-38.cpp, move the counter incrementation after all the data \nin the record has been flushed all the way to persistent media. Listing 12-45 shows the \ncorrected part of the code.\nListing 12-45.  Fix Listing 12-38 by moving the incrementation of the counter to \nthe end of the loop (line 95)\n    86      for (uint8_t i = 0; i < 10; i++) {\n    87          if (rand() % 2 == 0) {\n    88              snprintf(records[i].name, 63,\n    89                      \"record #%u\", i + 1);\n    90              pop.persist(records[i].name, 63);\n    91              records[i].valid = 2;\n    92          } else\n    93              records[i].valid = 1;\n    94          pop.persist(&(records[i].valid), 1);\n    95          header->counter++;\n    96      }\n\u0007Summary\nThis chapter provided an introduction to each tool and described how to use them. \nCatching issues early in the development cycle can save countless hours of debugging \ncomplex code later on. This chapter introduced three valuable tools – Persistence \nInspector, pmemcheck, and pmreorder – that persistent memory programmers will want \nto integrate into their development and testing cycles to detect issues. We demonstrated \nhow useful these tools are at detecting many different types of common programming \nerrors.\nThe Persistent Memory Development Kit (PMDK) uses the tools described here to \nensure each release is fully validated before it is shipped. The tools are tightly integrated \ninto the PMDK continuous integration (CI) development cycle, so you can quickly catch \nand fix issues.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1292.81 sec"
            },
            {
              "page_number": 284,
              "text": "260\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 12  Debugging Persistent Memory Applications",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1294.55 sec"
            },
            {
              "page_number": 285,
              "text": "261\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_13\nCHAPTER 13\nEnabling Persistence \nUsing a Real-World \nApplication\nThis chapter turns the theory from Chapter 4 (and other chapters) into practice. \nWe show how an application can take advantage of persistent memory by building \na persistent memory-aware database storage engine. We use MariaDB (https://\nmariadb.org/), a popular open source database, as it provides a pluggable storage \nengine model. The completed storage engine is not intended for production use and \ndoes not implement all the features a production quality storage engine should. We \nimplement only the basic functionality to demonstrate how to begin persistent memory \nprogramming using a well known database. The intent is to provide you with a more \nhands-on approach for persistent memory programming so you may enable persistent \nmemory features and functionality within your own application. Our storage engine is \nleft as an optional exercise for you to complete. Doing so would create a new persistent \nmemory storage engine for MariaDB, MySQL, Percona Server, and other derivatives. You \nmay also choose to modify an existing MySQL database storage engine to add persistent \nmemory features, or perhaps choose a different database entirely. \nWe assume that you are familiar with the preceding chapters that covered the \nfundamentals of the persistent memory programming model and Persistent Memory \nDevelopment Kit (PMDK). In this chapter, we implement our storage engine using C++ \nand libpmemobj-cpp from Chapter 8. If you are not a C++ developer, you will still find this \ninformation helpful because the fundamentals apply to other languages and applications.\nThe complete source code for the persistent memory-aware database storage engine \ncan be found on GitHub at https://github.com/pmem/pmdk-examples/tree/master/\npmem-mariadb.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1300.29 sec"
            },
            {
              "page_number": 286,
              "text": "262\n\u0007The Database Example\nA tremendous number of existing applications can be categorized in many ways. For \nthe purpose of this chapter, we explore applications from the common components \nperspective, including an interface, a business layer, and a store. The interface interacts \nwith the user, the business layer is a tier where the application’s logic is implemented, \nand the store is where data is kept and processed by the application.\nWith so many applications available today, choosing one to include in this book that \nwould satisfy all or most of our requirements was difficult. We chose to use a database as \nan example because a unified way of accessing data is a common denominator for many \napplications.\n\u0007Different Persistent Memory Enablement \nApproaches\nThe main advantages of persistent memory include: \n•\t\nIt provides access latencies that are lower than flash SSDs.\n•\t\nIt has higher throughput than NAND storage devices.\n•\t\nReal-time access to data allows ultrafast access to large datasets.\n•\t\nData persists in memory after a power interruption.\nPersistent memory can be used in a variety of ways to deliver lower latency for many \napplications:\n•\t\nIn-memory databases: In-memory databases can leverage \npersistent memory’s larger capacities and significantly reduce restart \ntimes. Once the database memory maps the index, tables, and \nother files, the data is immediately accessible. This avoids lengthy \nstartup times where the data is traditionally read from disk and paged \nin to memory before it can be accessed or processed. \n•\t\nFraud detection: Financial institutions and insurance companies \ncan perform real-time data analytics on millions of records to detect \nfraudulent transactions.\n•\t\nCyber threat analysis: Companies can quickly detect and defend \nagainst increasing cyber threats.\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1305.10 sec"
            },
            {
              "page_number": 287,
              "text": "263\n•\t\nWeb-scale personalization: Companies can tailor online user \nexperiences by returning relevant content and advertisements, \nresulting in higher user click-through rate and more e-commerce \nrevenue opportunities.\n•\t\nFinancial trading: Financial trading applications can rapidly \nprocess and execute financial transactions, allowing them to gain a \ncompetitive advantage and create a higher revenue opportunity.\n•\t\nInternet of Things (IoT): Faster data ingest and processing of huge \ndatasets in real-time reduces time to value.\n•\t\nContent delivery networks (CDN): A CDN is a highly distributed \nnetwork of edge servers strategically placed across the globe with the \npurpose of rapidly delivering digital content to users. With a memory \ncapacity, each CDN node can cache more data and reduce the total \nnumber of servers, while networks can reliably deliver low-latency \ndata to their clients. If the CDN cache is persisted, a node can restart \nwith a warm cache and sync only the data it is missed while it was out \nof the cluster. \n\u0007Developing a Persistent Memory-Aware MariaDB* \nStorage Engine\nThe storage engine developed here is not production quality and does not implement \nall the functionality expected by most database administrators. To demonstrate the \nconcepts described earlier, we kept the example simple, implementing table create(), \nopen(), and close() operations and INSERT, UPDATE, DELETE, and SELECT SQL \noperations. Because the storage engine capabilities are quite limited without indexing, \nwe include a simple indexing system using volatile memory to provide faster access to \nthe data residing in persistent memory.\nAlthough MariaDB has many storage engines to which we could add persistent \nmemory, we are building a new storage engine from scratch in this chapter. To learn \nmore about the MariaDB storage engine API and how storage engines work, we suggest \nreading the MariaDB “Storage Engine Development” documentation (https://\nmariadb.com/kb/en/library/storage-engines-storage-engine-development/).  \nSince MariaDB is based on MySQL, you can also refer to the MySQL “Writing a Custom \nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1310.84 sec"
            },
            {
              "page_number": 288,
              "text": "264\nStorage Engine” documentation (https://dev.mysql.com/doc/internals/en/custom-­\nengine.html) to find all the information for creating an engine from scratch.\n\u0007Understanding the Storage Layer\nMariaDB provides a pluggable architecture for storage engines that makes it easier \nto develop and deploy new storage engines. A pluggable storage engine architecture \nalso makes it possible to create new storage engines and add them to a running \nMariaDB server without recompiling the server itself. The storage engine provides data \nstorage and index management for MariaDB. The MariaDB server communicates with \nthe storage engines through a well-defined API.\nIn our code, we implement a prototype of a pluggable persistent memory–enabled \nstorage engine for MariaDB using the libpmemobj library from the Persistent Memory \nDevelopment Kit (PMDK).\nFigure 13-1 shows how the storage engine communicates with libpmemobj to \nmanage the data stored in persistent memory. The library is used to turn a persistent \nmemory pool into a flexible object store.\nFigure 13-1.  MariaDB storage engine architecture diagram for persistent memory\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page288_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page288_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1313.70 sec"
            },
            {
              "page_number": 289,
              "text": "265\n\u0007Creating a Storage Engine Class\nThe implementation of the storage engine described here is single-threaded to support a \nsingle session, a single user, and single table requests. A multi-threaded implementation \nwould detract from the focus of this chapter. Chapter 14 discussed concurrency in more \ndetail. The MariaDB server communicates with storage engines through a well-defined \nhandler interface that includes a handlerton, which is a singleton handler that is \nconnected to a table handler. The handlerton defines the storage engine and contains \npointers to the methods that apply to the persistent memory storage engine.\nThe first method the storage engine needs to support is to enable the call for a new \nhandler instance, shown in Listing 13-1.\nListing 13-1.  ha_pmdk.cc – Creating a new handler instance\n117  static handler *pmdk_create_handler(handlerton *hton,\n118                                       TABLE_SHARE *table,\n119                                       MEM_ROOT *mem_root);\n120\n121  handlerton *pmdk_hton;\nWhen a handler instance is created, the MariaDB server sends commands to the \nhandler to perform data storage and retrieve tasks such as opening a table, manipulating \nrows, managing indexes, and transactions. When a handler is instantiated, the first \nrequired operation is the opening of a table. Since the storage engine is a single user and \nsingle-threaded implementation, only one handler instance is created.\nVarious handler methods are also implemented; they apply to the storage engine as \na whole, as opposed to methods like create() and open() that work on a per-table basis. \nSome examples of such methods include transaction methods to handle commits and \nrollbacks, shown in Listing 13-2.\nListing 13-2.  ha_pmdk.cc – Handler methods including transactions, rollback, etc\n209  static int pmdk_init_func(void *p)\n210  {\n...\n213    pmdk_hton= (handlerton *)p;\n214    pmdk_hton->state=   SHOW_OPTION_YES;\n215    pmdk_hton->create=  pmdk_create_handler;\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1319.85 sec"
            },
            {
              "page_number": 290,
              "text": "266\n216    pmdk_hton->flags=   HTON_CAN_RECREATE;\n217    pmdk_hton->tablefile_extensions= ha_pmdk_exts;\n218\n219    pmdk_hton->commit= pmdk_commit;\n220    pmdk_hton->rollback= pmdk_rollback;\n...\n223  }\nThe abstract methods defined in the handler class are implemented to work with \npersistent memory. An internal representation of the objects in persistent memory is \ncreated using a single linked list (SLL). This internal representation is very helpful to \niterate through the records to improve performance.\nTo perform a variety of operations and gain faster and easier access to data, we used \nthe simple row structure shown in Listing 13-3 to hold the pointer to persistent memory \nand the associated field value in the buffer.\nListing 13-3.  ha_pmdk.h – A simple data structure to store data in a single  \nlinked list\n71  struct row {\n72    persistent_ptr<row> next;\n73    uchar buf[];\n74  };\n\u0007Creating a Database Table\nThe create() method is used to create the table. This method creates all necessary \nfiles in persistent memory using libpmemobj. As shown in Listing 13-4, we create a new \npmemobj type pool for each table using the pmemobj_create() method; this method \ncreates a transactional object store with the given total poolsize. The table is created in \nthe form of an .obj extension.\nListing 13-4.  Creating a table method\n1247  int ha_pmdk::create(const char *name, TABLE *table_arg,\n1248                         HA_CREATE_INFO *create_info)\n1249  {\n1250\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1324.97 sec"
            },
            {
              "page_number": 291,
              "text": "267\n1251    char path[MAX_PATH_LEN];\n1252    DBUG_ENTER(\"ha_pmdk::create\");\n1253    DBUG_PRINT(\"info\", (\"create\"));\n1254\n1255    snprintf(path, MAX_PATH_LEN, \"%s%s\", name, PMEMOBJ_EXT);\n1256    \u0007PMEMobjpool *pop = pmemobj_create(path, name,PMEMOBJ_MIN_POOL,  \nS_IRWXU);\n1257    if (pop == NULL) {\n1258      \u0007DBUG_PRINT(\"info\", (\"failed : %s error number : \n%d\",path,errCodeMap[errno]));\n1259      DBUG_RETURN(errCodeMap[errno]);\n1260    }\n1261    DBUG_PRINT(\"info\", (\"Success\"));\n1262    pmemobj_close(pop);\n1263\n1264    DBUG_RETURN(0);\n1265  }\n\u0007Opening a Database Table\nBefore any read or write operations are performed on a table, the MariaDB server calls \nthe open()method to open the data and index tables. This method opens all the named \ntables associated with the persistent memory storage engine at the time the storage \nengine starts. A new table class variable, objtab, was added to hold the PMEMobjpool. \nThe names for the tables to be opened are provided by the MariaDB server. The index \ncontainer in volatile memory is populated using the open() function call at the time of \nserver start using the loadIndexTableFromPersistentMemory() function.\nThe pmemobj_open() function from libpmemobj is used to open an existing object \nstore memory pool (see Listing 13-5). The table is also opened at the time of a table \ncreation if any read/write action is triggered.\nListing 13-5.  ha_pmdk.cc – Opening a database table\n290  int ha_pmdk::open(const char *name, int mode, uint test_if_locked)\n291  {\n...\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1328.66 sec"
            },
            {
              "page_number": 292,
              "text": "268\n302    objtab = pmemobj_open(path, name);\n303    if (objtab == NULL)\n304      DBUG_RETURN(errCodeMap[errno]);\n305\n306    proot = pmemobj_root(objtab, sizeof (root));\n307    // update the MAP when start occured\n308    loadIndexTableFromPersistentMemory();\n...\n310  }\nOnce the storage engine is up and running, we can begin to insert data into it. But we \nfirst must implement the INSERT, UPDATE, DELETE, and SELECT operations.\n\u0007Closing a Database Table\nWhen the server is finished working with a table, it calls the closeTable() method to \nclose the file using pmemobj_close() and release any other resources (see Listing 13-6). \nThe pmemobj_close() function closes the memory pool indicated by objtab and deletes \nthe memory pool handle.\nListing 13-6.  ha_pmdk.cc – Closing a database table\n376  int ha_pmdk::close(void)\n377  {\n378    DBUG_ENTER(\"ha_pmdk::close\");\n379    DBUG_PRINT(\"info\", (\"close\"));\n380\n381    pmemobj_close(objtab);\n382    objtab = NULL;\n383\n384    DBUG_RETURN(0);\n385  }\n\u0007INSERT Operation\nThe INSERT operation is implemented in the write_row() method, shown in Listing 13-­7. \nDuring an INSERT, the row objects are maintained in a singly linked list. If the table \nis indexed, the index table container in volatile memory is updated with the new \nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1334.29 sec"
            },
            {
              "page_number": 293,
              "text": "269\nrow objects after the persistent operation completes successfully. write_row() is an \nimportant method because, in addition to the allocation of persistent pool storage to \nthe rows, it is used to populate the indexing containers. pmemobj_tx_alloc() is used for \ninserts. write_row() transactionally allocates a new object of a given size and type_num.\nListing 13-7.  ha_pmdk.cc – Closing a database table\n417  int ha_pmdk::write_row(uchar *buf)\n418  {\n...\n421    int err = 0;\n422\n423    if (isPrimaryKey() == true)\n424      DBUG_RETURN(HA_ERR_FOUND_DUPP_KEY);\n425\n426    persistent_ptr<row> row;\n427    TX_BEGIN(objtab) {\n428      row = pmemobj_tx_alloc(sizeof (row) + table->s->reclength, 0);\n429      memcpy(row->buf, buf, table->s->reclength);\n430      row->next = proot->rows;\n431      proot->rows = row;\n432    } TX_ONABORT {\n433      DBUG_PRINT(\"info\", (\"write_row_abort errno :%d \",errno));\n434      err = errno;\n435    } TX_END\n436    stats.records++;\n437\n438    for (Field **field = table->field; *field; field++) {\n439      if ((*field)->key_start.to_ulonglong() >= 1) {\n440        std::string convertedKey = IdentifyTypeAndConvertToString((*fie\nld)->ptr, (*field)->type(),(*field)->key_length(),1);\n441        insertRowIntoIndexTable(*field, convertedKey, row);\n442      }\n443    }\n444    DBUG_RETURN(err);\n445  }\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1340.12 sec"
            },
            {
              "page_number": 294,
              "text": "270\nIn every INSERT operation, the field values are checked for a preexisting duplicate. \nThe primary key field in the table is checked using the isPrimaryKey()function (line \n423). If the key is a duplicate, the error HA_ERR_FOUND_DUPP_KEY is returned. The \nisPrimaryKey() is implemented in Listing 13-8.\nListing 13-8.  ha_pmdk.cc – Checking for duplicate primary keys\n462  bool ha_pmdk::isPrimaryKey(void)\n463  {\n464    bool ret = false;\n465    database *db = database::getInstance();\n466    table_ *tab;\n467    key *k;\n468    for (unsigned int i= 0; i < table->s->keys; i++) {\n469      KEY* key_info = &table->key_info[i];\n470      if (memcmp(\"PRIMARY\",key_info->name.str,sizeof(\"PRIMARY\"))==0) {\n471        Field *field = key_info->key_part->field;\n472        \u0007std::string convertedKey = IdentifyTypeAndConvertToString \n(field->ptr, field->type(),field->key_length(),1);\n473        if (db->getTable(table->s->table_name.str, &tab)) {\n474          if (tab->getKeys(field->field_name.str, &k)) {\n475            if (k->verifyKey(convertedKey)) {\n476              ret = true;\n477              break;\n478            }\n479          }\n480        }\n481      }\n482    }\n483    return ret;\n484  }\n\u0007UPDATE Operation\nThe server executes UPDATE statements by performing a rnd_init() or index_init() \ntable scan until it locates a row matching the key value in the WHERE clause of the UPDATE \nstatement before calling the update_row() method. If the table is an indexed table, the \nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1343.91 sec"
            },
            {
              "page_number": 295,
              "text": "271\nindex container is also updated after this operation is successful. In the update_row() \nmethod defined in Listing 13-9, the old_data field will have the previous row record in it, \nwhile new_data will have the new data.\nListing 13-9.  ha_pmdk.cc – Updating existing row data\n506  int ha_pmdk::update_row(const uchar *old_data, const uchar *new_data)\n507  {\n...\n540              if (k->verifyKey(key_str))\n541                k->updateRow(key_str, field_str);\n...\n551    if (current)\n552      memcpy(current->buf, new_data, table->s->reclength);\n...\nThe index table is also updated using the updateRow() method shown in Listing 13-­10.\nListing 13-10.  ha_pmdk.cc – Updating existing row data\n1363  bool key::updateRow(const std::string oldStr, const std::string newStr)\n1364  {\n...\n1366     persistent_ptr<row> row_;\n1367     bool ret = false;\n1368     rowItr matchingEleIt = getCurrent();\n1369\n1370     if (matchingEleIt->first == oldStr) {\n1371       row_ = matchingEleIt->second;\n1372       std::pair<const std::string, persistent_ptr<row> > r(newStr, row_);\n1373       rows.erase(matchingEleIt);\n1374       rows.insert(r);\n1375       ret = true;\n1376     }\n1377     DBUG_RETURN(ret);\n1378  }\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1348.62 sec"
            },
            {
              "page_number": 296,
              "text": "272\n\u0007DELETE Operation\nThe DELETE operation is implemented using the delete_row() method. Three different \nscenarios should be considered:\n•\t\nDeleting an indexed value from the indexed table\n•\t\nDeleting a non-indexed value from the indexed table\n•\t\nDeleting a field from the non-indexed table\nFor each scenario, different functions are called. When the operation is successful, \nthe entry is removed from both the index (if the table is an indexed table) and persistent \nmemory. Listing 13-11 shows the logic to implement the three scenarios.\nListing 13-11.  ha_pmdk.cc – Updating existing row data\n594  int ha_pmdk::delete_row(const uchar *buf)\n595  {\n...\n602    // Delete the field from non indexed table\n603    if (active_index == 64 && table->s->keys ==0 ) {\n604      if (current)\n605        deleteNodeFromSLL();\n606    \u0007} else if (active_index == 64 && table->s->keys !=0 ) { // Delete \nnon indexed column field from indexed table\n607      if (current) {\n608        deleteRowFromAllIndexedColumns(current);\n609        deleteNodeFromSLL();\n610      }\n611    } else { // Delete indexed column field from indexed table\n612    database *db = database::getInstance();\n613    table_ *tab;\n614    key *k;\n615    KEY_PART_INFO *key_part = table->key_info[active_index].key_part;\n616    if (db->getTable(table->s->table_name.str, &tab)) {\n617        if (tab->getKeys(key_part->field->field_name.str, &k)) {\n618          rowItr currNode = k->getCurrent();\n619          rowItr prevNode = std::prev(currNode);\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1353.23 sec"
            },
            {
              "page_number": 297,
              "text": "273\n620          if (searchNode(prevNode->second)) {\n621            if (prevNode->second) {\n622              deleteRowFromAllIndexedColumns(prevNode->second);\n623              deleteNodeFromSLL();\n624            }\n625          }\n626        }\n627      }\n628    }\n629    stats.records--;\n630\n631    DBUG_RETURN(0);\n632  }\nListing 13-12 shows how the deleteRowFromAllIndexedColumns() function deletes \nthe value from the index containers using the deleteRow() method.\nListing 13-12.  ha_pmdk.cc – Deletes an entry from the index containers\n634  \u0007void ha_pmdk::deleteRowFromAllIndexedColumns(const persistent_ptr<row> \n&row)\n635  {\n...\n643      if (db->getTable(table->s->table_name.str, &tab)) {\n644        if (tab->getKeys(field->field_name.str, &k)) {\n645          k->deleteRow(row);\n646        }\n...\nThe deleteNodeFromSLL() method deletes the object from the linked list residing on \npersistent memory using libpmemobj transactions, as shown in Listing 13-13.\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1356.81 sec"
            },
            {
              "page_number": 298,
              "text": "274\nListing 13-13.  ha_pmdk.cc – Deletes an entry from the linked list using \ntransactions\n651  int ha_pmdk::deleteNodeFromSLL()\n652  {\n653    if (!prev) {\n654      if (!current->next) { // When sll contains single node\n655        TX_BEGIN(objtab) {\n656          delete_persistent<row>(current);\n657          proot->rows = nullptr;\n658        } TX_END\n659      } else { // When deleting the first node of sll\n660        TX_BEGIN(objtab) {\n661          delete_persistent<row>(current);\n662          proot->rows = current->next;\n663          current = nullptr;\n664        } TX_END\n665      }\n666    } else {\n667      if (!current->next) { // When deleting the last node of sll\n668        prev->next = nullptr;\n669      } else { // When deleting other nodes of sll\n670        prev->next = current->next;\n671      }\n672      TX_BEGIN(objtab) {\n673        delete_persistent<row>(current);\n674        current = nullptr;\n675      } TX_END\n676    }\n677    return 0;\n678  }\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1361.52 sec"
            },
            {
              "page_number": 299,
              "text": "275\n\u0007SELECT Operation\nSELECT is an important operation that is required by several methods. Many methods \nthat are implemented for the SELECT operation are also called from other methods. The \nrnd_init() method is used to prepare for a table scan for non-indexed tables, resetting \ncounters and pointers to the start of the table. If the table is an indexed table, the \nMariaDB server calls the index_init() method. As shown in Listing 13-14, the pointers \nare initialized.\nListing 13-14.  ha_pmdk.cc – rnd_init() is called when the system wants the \nstorage engine to do a table scan\n869  int ha_pmdk::rnd_init(bool scan)\n870  {\n...\n874    current=prev=NULL;\n875    iter = proot->rows;\n876    DBUG_RETURN(0);\n877  }\nWhen the table is initialized, the MariaDB server calls the rnd_next(), index_first(), \nor index_read_map() method, depending on whether the table is indexed or not. These \nmethods populate the buffer with data from the current object and updates the iterator to \nthe next value. The methods are called once for every row to be scanned.\nListing 13-15 shows how the buffer passed to the function is populated with the \ncontents of the table row in the internal MariaDB format. If there are no more objects to \nread, the return value must be HA_ERR_END_OF_FILE.\nListing 13-15.  ha_pmdk.cc – rnd_init() is called when the system wants the \nstorage engine to do a table scan\n902  int ha_pmdk::rnd_next(uchar *buf)\n903  {\n...\n910    memcpy(buf, iter->buf, table->s->reclength);\n911    if (current != NULL) {\n912      prev = current;\n913    }\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1366.10 sec"
            },
            {
              "page_number": 300,
              "text": "276\n914    current = iter;\n915    iter = iter->next;\n916\n917    DBUG_RETURN(0);\n918  }\nThis concludes the basic functionality our persistent memory enabled storage \nengine set out to achieve. We encourage you to continue the development of this storage \nengine to introduce more features and functionality.\n\u0007Summary\nThis chapter provided a walk-through using libpmemobj from the PMDK to create \na persistent memory-aware storage engine for the popular open source MariaDB \ndatabase. Using persistent memory in an application can provide continuity in the \nevent of an unplanned system shutdown along with improved performance gained by \nstoring your data close to the CPU where you can access it at the speed of the memory \nbus. While database engines commonly use in-memory caches for performance, which \ntake time to warm up, persistent memory offers an immediately warm cache upon \napplication startup.\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 13  Enabling Persistence Using a Real-World Application",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1371.87 sec"
            },
            {
              "page_number": 301,
              "text": "277\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_14\nCHAPTER 14\nConcurrency and \nPersistent Memory\nThis chapter discusses what you need to know when building multithreaded \napplications for persistent memory. We assume you already have experience with \nmultithreaded programming and are familiar with basic concepts such as mutexes, \ncritical section, deadlocks, atomic operations, and so on.\nThe first section of this chapter highlights common practical solutions for building \nmultithreaded applications for persistent memory. We describe the limitation of \nthe Persistent Memory Development Kit (PMDK) transactional libraries, such as \nlibpmemobj and libpmemobj-cpp, for concurrent execution. We demonstrate simple \nexamples that are correct for volatile memory but cause data inconsistency issues on \npersistent memory in situations where the transaction aborts or the process crashes. \nWe also discuss why regular mutexes cannot be placed as is on persistent memory and \nintroduce the persistent deadlock term. Finally, we describe the challenges of building \nlock-free algorithms for persistent memory and continue our discussion of visibility vs. \npersistency from previous chapters.\nThe second section demonstrates our approach to designing concurrent data \nstructures for persistent memory. At the time of publication, we have two concurrent \nassociative C++ data structures developed for persistent memory - a concurrent \nhash map and a concurrent map. More will be added over time. We discuss both \nimplementations within this chapter. \nAll code samples are implemented in C++ using the libpmemobj-cpp library \ndescribed in Chapter 8. In this chapter, we usually refer to libpmemobj because it \nimplements the features and libpmemobj-cpp is only a C++ extension wrapper for it.  \nThe concepts are general and can apply to any programming language.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1376.47 sec"
            },
            {
              "page_number": 302,
              "text": "278\n\u0007Transactions and Multithreading\nIn computer science, ACID (atomicity, consistency, isolation, and durability) is a set of \nproperties of transactions intended to guarantee data validity and consistency in case \nof errors, power failures, and abnormal termination of a process. Chapter 7 introduced \nPMDK transactions and their ACID properties. This chapter focuses on the relevancy \nof multithreaded programs for persistent memory. Looking forward, Chapter 16 will \nprovide some insights into the internals of libpmemobj transactions.\nThe small program in Listing 14-1 shows that the counter stored within the root \nobject is incremented concurrently by multiple threads. The program opens the \npersistent memory pool and prints the value of counter. It then runs ten threads, each \nof which calls the increment() function. Once all the threads complete successfully, the \nprogram prints the final value of counter.\nListing 14-1.  Example to demonstrate that PMDK transactions do not \nautomatically support isolation\n41  using namespace std;\n42  namespace pobj = pmem::obj;\n43\n44  struct root {\n45      pobj::p<int> counter;\n46  };\n47\n48  using pop_type = pobj::pool<root>;\n49\n50  void increment(pop_type &pop) {\n51      auto proot = pop.root();\n52      pobj::transaction::run(pop, [&] {\n53          proot->counter.get_rw() += 1;\n54      });\n55  }\n56\n57  int main(int argc, char *argv[]) {\n58      pop_type pop =\n59          pop_type::open(\"/pmemfs/file\", \"COUNTER_INC\");\n60\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1380.06 sec"
            },
            {
              "page_number": 303,
              "text": "279\n61      auto proot = pop.root();\n62\n63      cout << \"Counter = \" << proot->counter << endl;\n64\n65      std::vector<std::thread> workers;\n66      workers.reserve(10);\n67      for (int i = 0; i < 10; ++i) {\n68          workers.emplace_back(increment, std::ref(pop));\n69      }\n70\n71      for (int i = 0; i < 10; ++i) {\n72          workers[i].join();\n73      }\n74\n75      cout << \"Counter = \" << proot->counter << endl;\n76\n77      pop.close();\n78      return 0;\n79  }\nYou might expect that the program in Listing 14-1 the prints a final counter value \nof 10. However, PMDK transactions do not automatically support isolation from the \nACID properties set. The result of the increment operation on line 53 is visible to \nother concurrent transactions before the current transaction has implicitly committed \nits update on line 54. That is, a simple data race is occurring in this example. A race \ncondition occurs when two or more threads can access shared data and they try to \nchange it at the same time. Because the operating system’s thread scheduling algorithm \ncan swap between threads at any time, there is no way for the application to know the \norder in which the threads will attempt to access the shared data. Therefore, the result \nof the change of the data is dependent on the thread scheduling algorithm, that is, both \nthreads are “racing” to access/change the data.\nIf we run this example multiple times, the results will vary from run to run. We can \ntry to fix the race condition by acquiring a mutex lock before the counter increment as \nshown in Listing 14-2.\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1386.00 sec"
            },
            {
              "page_number": 304,
              "text": "280\nListing 14-2.  Example of incorrect synchronization inside a PMDK transaction\n46  struct root {\n47      pobj::mutex mtx;\n48      pobj::p<int> counter;\n49  };\n50\n51  using pop_type = pobj::pool<root>;\n52\n53  void increment(pop_type &pop) {\n54      auto proot = pop.root();\n55      pobj::transaction::run(pop, [&] {\n56          std::unique_lock<pobj::mutex> lock(proot->mtx);\n57          proot->counter.get_rw() += 1;\n58      });\n59  }\n•\t\nLine 47: We added a mutex to the root data structure.\n•\t\nLine 56: We acquired the mutex lock within the transaction before \nincrementing the value of counter to avoid a race condition. Each \nthread increments the counter inside the critical section protected by \nthe mutex.\nNow if we run this example multiple times, it will always increment the value of \nthe counter stored in persistent memory by 1. But we are not done yet. Unfortunately, \nthe example in Listing 14-2 is also wrong and can cause data inconsistency issues \non persistent memory. The example works well if there are no transaction aborts. \nHowever, if the transaction aborts after the lock is released but before the transaction \nhas completed and successfully committed its update to persistent memory, other \nthreads can read a cached value of the counter that can cause data inconsistency issues. \nTo understand the problem, you need to know how libpmemobj transactions work \ninternally. For now, we discuss only the necessary details required to understand this \nissue and leave the in-depth discussion of transactions and their implementation for \nChapter 16.\nA libpmemobj transaction guarantees atomicity by tracking changes in the undo log. \nIn the case of a failure or transaction abort, the old values for uncommitted changes are \nrestored from the undo log. It is important to know that the undo log is a thread-specific \nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1390.71 sec"
            },
            {
              "page_number": 305,
              "text": "281\nentity. This means that each thread has its own undo log that is not synchronized with \nundo logs of other threads.\nFigure 14-1 illustrates the internals of what happens within the transaction when \nwe call the increment() function in Listing 14-2. For illustrative purposes, we only \ndescribe two threads. Each thread executes concurrent transactions to increment the \nvalue of counter allocated in persistent memory. We assume the initial value of counter \nis 0 and the first thread acquires the lock, while the second thread waits on the lock. \nInside the critical section, the first thread adds the initial value of counter to the undo \nlog and increments it. The mutex is released when execution flow leaves the lambda \nscope, but the transaction has not committed the update to persistent memory. The \nchanges become immediately visible to the second thread. After a user-provided lambda \nis executed, the transaction needs to flush all changes to persistent memory to mark \nthe change(s) as committed. Concurrently, the second thread adds the current value of \ncounter, which is now 1, to its undo log and performs the increment operation. At that \nmoment, there are two uncommitted transactions. The undo log of Thread 1 contains \ncounter = 0, and the undo log of Thread 2 contains counter = 1. If Thread 2 commits \nits transaction while Thread 1 aborts its transaction for some reason (crash or abort), the \nincorrect value of counter will be restored from the undo log of Thread 1.\nThe solution is to hold the mutex until the transaction is fully committed, and the data \nhas been successfully flushed to persistent memory. Otherwise, changes made by one \ntransaction become visible to concurrent transactions before it is persisted and committed. \nListing 14-3 demonstrates how to implement the increment() function correctly.\nFigure 14-1.  Illustrative execution of the Listing 14-2 example\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page305_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page305_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1395.73 sec"
            },
            {
              "page_number": 306,
              "text": "282\nListing 14-3.  Correct example for concurrent PMDK transaction\n52  void increment(pop_type &pop) {\n53      auto proot = pop.root();\n54      pobj::transaction::run(pop, [&] {\n55          proot->counter.get_rw() += 1;\n56      }, proot->mtx);\n57  }\nThe libpmemobj API allows us to specify locks that should be acquired and held for \nthe entire duration of the transaction. In the Listing 14-3 example, we pass the proot-\n>mtx mutex object to the run() method as a third parameter.\n\u0007Mutexes on Persistent Memory\nOur previous examples used pmem::obj::mutex as a type for the mtx member in our root \ndata structure instead of the regular std::mutex provided by Standard Template Library. \nThe mtx object is a member of the root object that resides in persistent memory. The \nstd::mutex type cannot be used on persistent memory because it may cause persistent \ndeadlock.\nA persistent deadlock happens if an application crash occurs while holding a mutex. \nWhen the program starts, if it does not release or reinitialize the mutex at startup, \nthreads that try to acquire it will wait forever. To avoid such situations, libpmemobj \nprovides synchronization primitives that reside in persistent memory. The main feature \nof synchronization primitives is that they are automatically reinitialized every time the \npersistent object store pool is open.\nFor C++ developers, the libpmemobj-cpp library provides C++11-like \nsynchronization primitives shown in Table 14-1.\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1400.44 sec"
            },
            {
              "page_number": 307,
              "text": "283\nFor C developers, the libpmemobj library provides pthread-like synchronization \nprimitives shown in Table 14-2. Persistent memory-aware locking implementations are \nbased on the standard POSIX Thread Library and provide semantics similar to standard \npthread locks.\nTable 14-1.  Synchronization primitives provided by libpmemob++ library\nClass\nDescription\npmem::obj::mutex\nThis class is an implementation of a persistent memory resident \nmutex which mimics in behavior the C++11 std::mutex. This class \nsatisfies all requirements of the Mutex and StandardLayoutType \nconcepts.\npmem::obj::timed_mutex\nThis class is an implementation of a persistent memory resident \ntimed_mutex which mimics in behavior the C++11 std::timed_\nmutex. This class satisfies all requirements of TimedMutex and \nStandardLayoutType concepts.\npmem::obj::shared_mutex\nThis class is an implementation of a persistent memory resident \nshared_mutex which mimics in behavior the C++17 std::shared_\nmutex. This class satisfies all requirements of SharedMutex and \nStandardLayoutType concepts.\npmem::obj:: condition_variable This class is an implementation of a persistent memory resident \ncondition variable which mimics in behavior the C++11 \nstd::condition_variable. This class satisfies all requirements of \nStandardLayoutType concept.\nTable 14-2.  Synchronization primitives provided by the libpmemobj library\nStructure\nDescription\nPMEMmutex\nThe data structure represents a persistent memory resident mutex similar \nto pthread_mutex_t.\nPMEMrwlock\nThe data structure represents a persistent memory resident read-write lock \nsimilar to pthread_rwlock_t.\nPMEMcond\nThe data structure represents a persistent memory resident condition \nvariable similar to pthread_cond_t.\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1405.25 sec"
            },
            {
              "page_number": 308,
              "text": "284\nThese convenient persistent memory-aware synchronization primitives are available \nfor C and C++ developers. But what if a developer wants to use a custom synchronization \nobject that is more appropriate for a particular use case? As we mentioned earlier, the \nmain feature of persistent memory-aware synchronization primitives is that they are \nreinitialized every time we open a persistent memory pool. The libpmemobj-cpp library \nprovides a more generic mechanism to reinitialize any user-provided type every time a \npersistent memory pool is opened.\nThe libpmemobj-cpp provides the pmem::obj::v<T> class template which \nallows creating a volatile field inside a persistent data structure. The mutex object \nis semantically a volatile entity, and the state of a mutex should not survive an \napplication restart. On application restart, a mutex object should be in the unlocked \nstate. The pmem::obj::v<T> class template is targeted for this purpose. Listing 14-4 \ndemonstrates how to use the pmem::obj::v<T> class template with std::mutex on \npersistent memory.\nListing 14-4.  Example demonstrating usage of std::mutex on persistent memory\n38  namespace pobj = pmem::obj;\n39\n40  struct root {\n41      pobj::experimental::v<std::mutex> mtx;\n42  };\n43\n44  using pop_type = pobj::pool<root>;\n45\n46  int main(int argc, char *argv[]) {\n47      pop_type pop =\n48          pop_type::open(\"/pmemfs/file\", \"MUTEX\");\n49\n50      auto proot = pop.root();\n51\n52      proot->mtx.get().lock();\n53\n54      pop.close();\n55      return 0;\n56  }\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1411.09 sec"
            },
            {
              "page_number": 309,
              "text": "285\n•\t\nLine 41: We are only storing the mtx object inside root object on \npersistent memory.\n•\t\nLines 47-48: We open the persistent memory pool with the layout \nname of “MUTEX”.\n•\t\nLine 50: We obtain a pointer to the root data structure within the \npool.\n•\t\nLine 52: We acquire the mutex.\n•\t\nLines 54-56: Close the pool and exit the program.\nAs you can see, we do not explicitly unlock the mutex within the main() function. \nIf we run this example several times, the main() function can always lock the mutex on \nline 52. This works because the pmem::obj::v<T> class template implicitly calls a default \nconstructor, which is a wrapped std::mutex object type. The constructor is called every \ntime we open the persistent memory pool so we never run into a situation where the lock \nis already acquired.\nIf we change the mtx object type on line 41 from pobj::experimental::v<std::mu\ntex> to std::mutex and try to run the program again, the example will hang during the \nsecond run on line 52 because mtx object was locked during the first run and we never \nreleased it.\n\u0007Atomic Operations and Persistent Memory\nAtomic operations cannot be used inside PMDK transactions for the reason described \nin Figure 14-1. Changes made by atomic operations inside a transaction become \nvisible to other concurrent threads before the transaction is committed. It forces data \ninconsistency issues in cases of abnormal program termination or transaction aborts. \nConsider lock-free algorithms where concurrency is achieved by atomically updating the \nstate in memory.\n\u0007Lock-Free Algorithms and Persistent Memory\nIt is intuitive to think that lock-free algorithms are naturally fit for persistent memory. In \nlock-free algorithms, thread-safety is achieved by atomic transitions between consistent \nstates, and this is exactly what we need to support data consistency in persistent \nmemory. But this assumption is not always correct.\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1415.80 sec"
            },
            {
              "page_number": 310,
              "text": "286\nTo understand the problem with lock-free algorithms, remember that a system \nwith persistent memory will usually have the virtual memory subsystem divided into \ntwo domains: volatile and persistent (described in Chapter 2). The result of an atomic \noperation may only update data in a CPU cache using a cache coherency protocol. There \nis no guarantee that the data will be flushed unless an explicit flush operation is called. \nCPU caches are only included within the persistence domain on platforms with eADR \nsupport. This is not mandatory for persistent memory. ADR is the minimal platform \nrequirement for persistent memory, and in that case, CPU caches are not flushed in a \npower failure.\nFigure 14-2 assumes a system with ADR support. The example shows concurrent \nlock-free insert operations to a singly linked list located in persistent memory. Two \nthreads are trying to insert new nodes to the tail of a linked list using a compare-and-­\nexchange (CMPXCHG instruction) operation followed by a cache flush operation (CLWB \ninstruction). Assume Thread 1 succeeds with its compare-and-exchange, so the change \nappears in a volatile domain and becomes visible to the second thread. At this moment, \nThread 1 may be preempted (changes not flushed to a persistent domain), while Thread \n2 inserts Node 5 after Node 4 and flushes it to a persistent domain. A possibility for data \ninconsistency exists because Thread 2 performed an update based on the data that is not \nyet persisted by Thread 1.\n\u0007Concurrent Data Structures for Persistent Memory\nThis section describes two concurrent data structures available in the libpmemobj-cpp \nlibrary: pmem::obj::concurrent_map and pmem::obj::concurrent_hash_map. Both are \nassociative data structures composed of a collection of key and value pairs, such that \neach possible key appears at most once in the collection. The main difference between \nthem is that the concurrent hash map is unordered, while the concurrent map is ordered \nby keys.\nFigure 14-2.  Example of a concurrent lock-free insert operation to a singly linked \nlist located in persistent memory\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page310_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page310_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1420.92 sec"
            },
            {
              "page_number": 311,
              "text": "287\nWe define concurrent in this context to be the method of organizing data structures \nfor access by multiple threads. Such data structures are intended for use in a parallel \ncomputing environment when multiple threads can concurrently call methods of a data \nstructure without additional synchronization required.\nC++ Standard Template Library (STL) data structures can be wrapped in a coarse-­\ngrained mutex to make them safe for concurrent access by letting only one thread \noperate on the container at a time. However, that approach eliminates concurrency \nand thereby restricts parallel speedup if implemented in performance-critical code. \nDesigning concurrent data structures is a challenging task. The difficulty increases \nsignificantly when we need to develop concurrent data structures for persistent memory \nand make them fault tolerant.\nThe pmem::obj::concurrent_map and pmem::obj::concurrent_hash_map structures \nwere inspired by the Intel Threading Building Blocks (Intel TBB),1 which provides \nimplementations of these concurrent data structures designed for volatile memory. You \ncan read the Pro TBB: C++ Parallel Programming with Threading Building Blocks book2 \nto get more information and learn how to use these concurrent data structures in your \napplication. The free electronic copy is available from Apress at https://www.apress.\ncom/gp/book/9781484243978.\nThere are three main methods in our concurrent associative data structures: find, \ninsert, and erase/delete. We describe each data structure with a focus on these three \nmethods.\n\u0007Concurrent Ordered Map\nThe implementation of the concurrent ordered map for persistent memory \n(pmem::obj::concurrent_map) is based on a concurrent skip list data structure. Intel \nTBB supplies tbb::concurrent_map, which is designed for volatile memory that we use \nas a baseline for a port to persistent memory. The concurrent skip list data structure \ncan be implemented as a lock-free algorithm. But Intel chose a provably correct \n1\u0007Intel Threading Building Blocks library (https://github.com/intel/tbb).\n2\u0007Michael Voss, Rafael Asenjo, James Reinders. C++ Parallel Programming with Threading Building \nBlocks; Apress, 2019; ISBN-13 (electronic): 978-1-4842-4398-5; https://www.apress.com/gp/\nbook/9781484243978.\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1427.88 sec"
            },
            {
              "page_number": 312,
              "text": "288\nscalable concurrent skip list3 implementation with fine-grain locking distinguished by \na combination of simplicity and scalability. Figure 14-3 demonstrates the basic idea of \nthe skip list data structure. It is a multilayered linked list-like data structure where the \nbottom layer is an ordered linked list. Each higher layer acts as an “express lane” for \nthe following lists and allows it to skip elements during lookup operations. An element \nin layer i appears in layer i+1 with some fixed probability p (in our implementation p \n= 1/2). That is, the frequency of nodes of a particular height decreases exponentially \nwith the height. Such properties allow it to achieve O(log n) average time complexity \nfor lookup, insert, and delete operations. O(log n) means the running time grows at \nmost proportional to “log n”. You can learn more about Big O notation on Wikipedia at \nhttps://en.wikipedia.org/wiki/Big_O_notation\nFor the implementation of pmem::obj::concurrent_map, the find and insert \noperations are thread-safe and can be called concurrently with other find and insert \noperations without requiring additional synchronizations.\n\u0007Find Operation\nBecause the find operation is non-modifying, it does not have to deal with data \nconsistency issues. The lookup operation for the target element always begins from the \ntopmost layer. The algorithm proceeds horizontally until the next element is greater \nor equal to the target. Then it drops down vertically to the next lower list if it cannot \nproceed on the current level. Figure 14-3 illustrates how the find operation works for the \nelement with key=9. The search starts from the highest level and immediately goes from \ndummy head node to the node with key=4, skipping nodes with keys 1, 2, 3. On the node \nwith key=4, the search is dropped two layers down and goes to the node with key=8. \nThen it drops one more layer down and proceeds to the desired node with key=9.\n3\u0007M. Herlihy, Y. Lev, V. Luchangco, N. Shavit. A provably correct scalable concurrent skip list. In \nOPODIS ‘06: Proceedings of the 10th International Conference On Principles Of Distributed \nSystems, 2006; https://www.cs.tau.ac.il/~shanir/nir-pubs-web/Papers/OPODIS2006-BA.\npdf.\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1432.49 sec"
            },
            {
              "page_number": 313,
              "text": "289\nThe find operation is wait-free. That is, every find operation is bound only by \nthe number of steps the algorithm takes. And a thread is guaranteed to complete \nthe operation regardless of the activity of other threads. The implementation of \npmem::obj::concurrent_map uses atomic load-with-acquire memory semantics when \nreading pointers to the next node.\n\u0007Insert Operation\nThe insert operation, shown in Figure 14-4, employs fine-grained locking schema for \nthread-safety and consists of the following basic steps to insert a new node with key=7 \ninto the list:\n\t 1.\t Allocate the new node with randomly generated height.\n\t 2.\t Find a position to insert the new node. We must find the \npredecessor and successor nodes on each level.\n\t 3.\t Acquire locks for each predecessor node and check that the \nsuccessor nodes have not been changed. If successor nodes have \nchanged, the algorithm returns to step 2.\n\t 4.\t Insert the new node to all layers starting from the bottom one. \nSince the find operation is lock-free, we must update pointers on \neach level atomically using store-­with-­release memory semantics.\nFigure 14-3.  Finding key=9 in the skip list data structure\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page313_img1.jpeg"
              ],
              "img_summary_files": [],
              "img_vision_files": [
                "output\\images\\img_vision\\Programming_Persistent_Memory_medium_457_page313_img1_vision.json"
              ],
              "summary": "Summary not available due to an error.",
              "time_taken": "1437.09 sec"
            },
            {
              "page_number": 314,
              "text": "290\nThe algorithm described earlier is thread-safe, but it is not enough to be fault \ntolerant on persistent memory. There is a possible persistent memory leak if a program \nunexpectedly terminates between the first and fourth steps of our algorithm.\nThe implementation of pmem::obj::concurrent_map does not use transactions \nto support data consistency because transactions do not support isolation and by not \nusing transactions, it can achieve better performance. For this linked list data structure, \ndata consistency is maintained because a newly allocated node is always reachable \n(to avoid persistent memory leak) and the linked list data structure is always valid. To \nsupport these two properties, persistent thread-local storage is used, which is a member \nof the concurrent skip list data structure. Persistent thread-local storage guarantees that \neach thread has its own location in persistent memory to assign the result of persistent \nmemory allocation for the new node.\nFigure 14-5 illustrates the approach of this fault-tolerant insert algorithm. When a \nthread allocates a new node, the pointer to that node is kept in persistent thread-local \nstorage, and the node is reachable through this persistent thread-local storage. Then \nthe algorithm inserts the new node to the skip list by linking it to all layers using the \nthread-safe algorithm described earlier. Finally, the pointer in the persistent thread-local \nstorage is removed because the new node is reachable now via skip list itself. In case of \nfailure, a special function traverses all nonzero pointers in persistent thread-local storage \nand completes the insert operation.\nFigure 14-4.  Inserting a new node with key=7 into the concurrent skip list\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page314_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page314_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1443.03 sec"
            },
            {
              "page_number": 315,
              "text": "291\n\u0007Erase Operation\nThe implementation of the erase operation for pmem::obj::concurrent_map is not \nthread-safe. This method cannot be called concurrently with other methods of the \nconcurrent ordered map because this is a memory reclamation problem that is hard to \nsolve in C++ without a garbage collector. There is a way to logically extract a node from \na skip list in a thread-safe manner, but it is not trivial to detect when it is safe to delete \nthe removed node because other threads may still have access to the node. There are \npossible solutions, such as hazard pointers, but these can impact the performance of the \nfind and insert operations.\n\u0007Concurrent Hash Map\nThe concurrent hash map designed for persistent memory is based on tbb::concurrent_\nhash_map that exists in the Intel TBB. The implementation is based on a concurrent hash \ntable algorithm where elements assigned to buckets based on a hash code are calculated \nfrom a key. In addition to concurrent find, insert, and erase operations, the algorithm \nemploys concurrent resizing and on-demand per-bucket rehashing.4\nFigure 14-6 illustrates the basic idea of the concurrent hash table. The hash table \nconsists of an array of buckets, and each bucket consists of a list of nodes and a read-­\nwrite lock to control concurrent access by multiple threads.\n4\u0007Anton Malakhov. Per-bucket concurrent rehashing algorithms, 2015, arXiv:1509.02235v1; \nhttps://arxiv.org/ftp/arxiv/papers/1509/1509.02235.pdf.\nFigure 14-5.  Fault-tolerant insert operation using persistent thread-local storage\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page315_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page315_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1446.00 sec"
            },
            {
              "page_number": 316,
              "text": "292\n\u0007Find Operation\nThe find operation is a read-only event that does not change the hash map state. \nTherefore, data consistency is maintained while performing a find request. The find \noperation works by first calculating the hash value for a target key and acquires read \nlock for the corresponding bucket. The read lock guarantees that there is no concurrent \nmodifications to the bucket while we are reading it. Inside the bucket, the find operation \nperforms a linear search through the list of nodes.\n\u0007Insert Operation\nThe insert method of the concurrent hash map uses the same technique to support \ndata consistency as the concurrent skip list data structure. The operation consists of the \nfollowing steps:\n\t 1.\t Allocate the new node, and assign a pointer to the new node to \npersistent thread-local storage.\n\t 2.\t Calculate the hash value of the new node, and find the \ncorresponding bucket.\nFigure 14-6.  The concurrent hash map data structure\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page316_img1.jpeg"
              ],
              "img_summary_files": [],
              "img_vision_files": [
                "output\\images\\img_vision\\Programming_Persistent_Memory_medium_457_page316_img1_vision.json"
              ],
              "summary": "Summary not available due to an error.",
              "time_taken": "1449.49 sec"
            },
            {
              "page_number": 317,
              "text": "293\n\t 3.\t Acquire the write lock to the bucket.\n\t 4.\t Insert the new node to the bucket by linking it to the list of nodes. \nBecause only one pointer has to be updated, a transaction is not \nneeded. Because only one pointer is updated, a transaction is not \nrequired.\n\u0007Erase Operation\nAlthough the erase operation is similar to an insert (the opposite action), its \nimplementation is even simpler than the insert. The erase implementation \nacquires the write lock for the required bucket and, using a transaction, removes the \ncorresponding node from the list of nodes within that bucket.\n\u0007Summary\nAlthough building an application for persistent memory is a challenging task, it is more \ndifficult when you need to create a multithreaded application for persistent memory. \nYou need to handle data consistency in a multithreaded environment when multiple \nthreads can update the same data in persistent memory.\nIf you develop concurrent applications, we encourage you to use existing libraries \nthat provide concurrent data structures designed to store data in persistent memory. \nYou should develop custom algorithms only if the generic ones do not fit your needs. \nSee the implementations of concurrent cmap and csmap engines in pmemkv, described \nin Chapter 9, which are implemented using pmem::obj::concurrent_hash_map and \npmem::obj::concurrent_map, respectively.\nIf you need to develop a custom multithreaded algorithm, be aware of the limitation \nPMDK transactions have for concurrent execution. This chapter shows that transactions \ndo not automatically provide isolation out of the box. Changes made inside one \ntransaction become visible to other concurrent transactions before they are committed. \nYou will need to implement additional synchronization if it is required by an algorithm. \nWe also explain that atomic operations cannot be used inside a transaction while \nbuilding lock-free algorithms without transactions. This is a very complicated task if your \nplatform does not support eADR.\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1455.22 sec"
            },
            {
              "page_number": 318,
              "text": "294\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 14  Concurrency and Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1465.48 sec"
            },
            {
              "page_number": 319,
              "text": "295\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_15\nCHAPTER 15\nProfiling and Performance\n\u0007Introduction\nThis chapter first discusses the general concepts for analyzing memory and storage \nperformance and how to identify opportunities for using persistent memory for both \nhigh-performance persistent storage and high-capacity volatile memory. We then \ndescribe the tools and techniques that can help you optimize your code to achieve the \nbest performance.\nPerformance analysis requires tools to collect specific data and metrics about \napplication, system, and hardware performance. In this chapter, we describe how to \ncollect this data using Intel VTune Profiler. Many other data collection options are \navailable; the techniques we describe are relevant regardless of how the data is collected.\n\u0007Performance Analysis Concepts\nMost concepts for performance analysis of persistent memory are similar to those \nalready established for performance analysis of shared memory programs or storage \nbottlenecks. This section outlines several important performance considerations you \nshould understand to profile and optimize persistent memory performance and defines \nthe terms and situations we use in this chapter.\n\u0007Compute-Bound vs. Memory-Bound\nPerformance optimization largely involves identifying the current performance bottleneck \nand improving it. The performance of compute-bound workloads is generally limited by \nthe number of instructions the CPU can process per cycle. For example, an application \ndoing a large number of calculations on very compact data without many dependencies \nis usually compute-bound. This type of workload would run faster if the CPU were faster. \nCompute-bound applications usually have high CPU utilization, close to 100%.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1465.97 sec"
            },
            {
              "page_number": 320,
              "text": "296\nIn contrast, the performance of memory-bound workloads is generally limited by \nthe memory subsystem and the long latencies of fetching data from caches and system \nmemory. An example is an application that randomly accesses data from data structures \nin DRAM. In this case, adding more compute resources would not improve such an \napplication. Adding persistent memory to improve performance is usually an option for \nmemory-bound workloads as opposed to compute-bound workloads. Memory-bound \nworkloads usually have lower CPU utilization than compute-bound workloads, exhibit \nCPU stalls due to memory transfers, and have high memory bandwidth.\n\u0007Memory Latency vs. Memory Capacity\nThis concept is essential when discussing persistent memory. For this discussion, we \nassume that DRAM access latencies are lower than persistent memory and that the \npersistent memory capacity within the system is larger than DRAM. Workloads bound by \nmemory capacity can benefit from adding persistent memory in a volatile mode, while \nworkloads that are bound by memory latency are less likely to benefit.\n\u0007Read vs. Write Performance\nWhile each persistent memory technology is unique, it is important to understand \nthat there is usually a difference in the performance of reads (loads) vs. writes (stores). \nDifferent media types exhibit varying degrees of asymmetric read-write performance \ncharacteristics, where reads are generally much faster than writes. Therefore, \nunderstanding the mix of loads and stores in an application workload is important for \nunderstanding and optimizing performance.\n\u0007Memory Access Patterns\nA memory access pattern is the pattern with which a system or application reads and \nwrites to or from the memory. Memory hardware usually relies on temporal locality \n(accessing recently used data) and spatial locality (accessing contiguous memory \naddresses) for best performance. This is often achieved through some structure of fast \ninternal caches and intelligent prefetchers. The access pattern and level of locality can \ndrastically affect cache performance and can also have implications on parallelism \nand distributions of workloads within shared memory systems. Cache coherency can \nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1467.71 sec"
            },
            {
              "page_number": 321,
              "text": "297\nalso affect multiprocessor performance, which means that certain memory access \npatterns place a ceiling on parallelism. Many well-defined memory access patterns exist, \nincluding but not limited to sequential, strided, linear, and random.\nIt is much easier to measure, control, and optimize memory accesses on systems that \nrun only one application. In the cloud and virtualized environments, applications within \nthe guests can be running any type of application and workload, including web servers, \ndatabases, or an application server. This makes it much harder to ensure memory \naccesses are fully optimized for the hardware as the access patterns are essentially \nrandom.\n\u0007I/O Storage Bound Workloads\nA program is I/O bound if it would go faster if the I/O subsystem were faster. We are \nprimarily interested in the block-based disk I/O subsystem here, but it could also include \nother subsystems such as the network. An I/O bound state is undesirable because \nit means that the CPU must stall its operation while waiting for data to be loaded or \nunloaded from main memory or storage. Depending on where the data is and the \nlatency of the storage device, this can invoke a voluntary context switching of the current \napplication thread with another. A voluntary context switch occurs when a thread blocks \nbecause it requires a resource that is not immediately available or takes a long time \nto respond. With faster computation speed being the primary goal of each successive \ncomputer generation, there is a strong imperative to avoid I/O bound states. Eliminating \nthem can often yield a more economic improvement in performance than upgrading the \nCPU or memory.\n\u0007Determining the Suitability of Workloads \nfor Persistent Memory\nPersistent memory technologies may not solve every workload performance problem. \nYou should understand the workload and platform on which it is currently running \nwhen considering persistent memory. As a simple example, consider a compute-­\nintensive workload that relies heavily on floating-point arithmetic. The performance of \nthis application is likely limited by the floating-point unit in the CPU and not any part \nof the memory subsystem. In that case, adding persistent memory to the platform will \nlikely have little impact on this application’s performance. Now consider an application \nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1473.65 sec"
            },
            {
              "page_number": 322,
              "text": "298\nthat requires extensive reading and writing from disk. It is likely that the disk accesses \nare the bottleneck for this application and adding a faster storage solution, like persistent \nmemory, could improve performance.\nThese are trivial examples, and applications will have widely different behaviors \nalong this spectrum. Understanding what behaviors to look for and how to measure \nthem is an important step to using persistent memory. This section presents the \nimportant characteristics to identify and determine if an application is a good fit for \npersistent memory. We look at applications that require in-memory persistence, \napplications that can use persistent memory in a volatile manner, and applications that \ncan use both.\n\u0007Volatile Use Cases\nChapter 10 described several libraries and use cases where applications can take \nadvantage of the performance and capacity of persistent memory to store non-volatile \ndata. For volatile use cases, persistent memory will act as an additional memory tier for \nthe platform. It may be transparent to the application, such as using Memory Mode \nsupported by Intel Optane DC persistent memory, or applications can make code \nchanges to perform volatile memory allocations using libraries such as libmemkind. \nIn both cases, memory-capacity bound workloads will benefit from adding persistent \nmemory to the platform. Application performance can dramatically improve if its \nworking dataset can fit into memory and avoid paging to disk.\n\u0007Identifying Workloads That Are Memory-Capacity Bound\nTo determine if a workload is memory-capacity bound, you must determine the \n“memory footprint” of the application. The memory footprint is the high watermark \nof memory concurrently allocated during the application’s life cycle. Since physical \nmemory is a finite resource, you should consider the fact that the operating system and \nother processes also consume memory. If the footprint of the operating system and all \nmemory consumers on the system are approaching or exceeding the available DRAM \ncapacity on the platform, you can assume that the application would benefit from \nadditional memory because it cannot fit all its data in DRAM. Many tools and techniques \ncan be used to determine memory footprint. VTune Profiler includes two different ways \nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1478.23 sec"
            },
            {
              "page_number": 323,
              "text": "299\nto find this information: Memory Consumption analysis or Platform Profiler analysis. \nVTune Profiler is a free download for Linux and Windows, available from https://\nsoftware.intel.com/en-us/vtune.\nThe Memory Consumption analysis within VTune Profiler tracks all memory \nallocations made by the application. Figure 15-1 shows a VTune Profiler bottom-­\nup report representing memory consumption of the profiled application over time. \nThe highest value on the y-axis in the Memory Consumption timeline indicates the \napplication footprint is approximately 1GiB.\nThe Memory Utilization graph in the Platform Profiler analysis shown in Figure 15-­2  \nmeasures the memory footprint using operating system statistics and produces a \ntimeline graph as a percentage of the total available memory.\nFigure 15-1.  The VTune Profiler bottom-up analysis showing memory \nconsumption with time and the associated allocating call stacks\nFigure 15-2.  The VTune Platform Profiler Memory Utilization graph as a \npercentage of total system memory\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page323_img1.jpeg",
                "output\\images\\Programming_Persistent_Memory_medium_457_page323_img2.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page323_img1_summary.json"
              ],
              "img_vision_files": [
                "output\\images\\img_vision\\Programming_Persistent_Memory_medium_457_page323_img2_vision.json"
              ],
              "summary": "Summary not available due to an error.",
              "time_taken": "1480.61 sec"
            },
            {
              "page_number": 324,
              "text": "300\nThe results in Figure 15-2 were taken from a different application than Figure 15-1. \nThis graph shows very high memory consumption, which implies this workload would \nbe a good candidate for adding more memory to the system. If your persistent memory \nhardware has variable modes, like the Memory and App Direct modes on Intel Optane \nDC persistent memory, you will need some more information to determine which mode \nto use first. The next important information is the hot working set size.\n\u0007Identifying the Hot Working Set Size of a Workload\nPersistent memory usually has different characteristics than DRAM; therefore, you \nshould make intelligent decisions about where data will reside. We will assume that \naccessing data from persistent memory has higher latency than DRAM. Given the \nchoice between accessing data in DRAM and persistent memory, we would always \nchoose DRAM for performance. However, the premise of adding persistent memory in a \nvolatile configuration assumes there is not enough DRAM to fit all the data. You need to \nunderstand how your workload accesses data to make choices about persistent memory \nconfiguration.\nThe working set size (WSS) is how much memory an application needs to keep \nworking. For example, if an application has 50GiB of main memory allocated and page \nmapped, but it is only accessing 20MiB each second to perform its job, we can say that \nthe working set size is 50GiB and the “hot” data is 20MiB. It is useful to know this for \ncapacity planning and scalability analysis. The “hot working set” is the set of objects \naccessed frequently by an application, and the “hot working set size” is the total size of \nthose objects allocated at any given time.\nDetermining the size of the working set and hot working set is not as straightforward \nas determining memory footprint. Most applications will have a wide range of objects \nwith varying degrees of “hotness,” and there will not be a clear line delineating which \nobjects are hot and which are not. You must interpret this information and determine \nthe hot working set size.\nVTune Profiler has a Memory Access analysis feature that can help determine the hot \nand working set sizes of an application (select the “Analyze dynamic memory objects” \noption before data collection begins). Once enough data has been collected, VTune \nProfiler will process the data and produce a report. In the bottom-up view within the \nGUI, a grid lists each memory object that was allocated by the application.\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1487.37 sec"
            },
            {
              "page_number": 325,
              "text": "301\nFigure 15-3 shows the results of a Memory Access analysis of an application. It shows \nthe memory size in parenthesis and the number of loads and stores that accessed it. The \nreport does not include an indication of what was concurrently allocated.\nThe report identifies the objects with the most accesses (loads and stores). The sum \nof the sizes of these objects is the working set size – the values are in parentheses. You \ndecide where to draw the line for what is and is not part of the hot working set.\nDepending on the workload, there may not be an easy way to determine the hot working \nset size, other than developer knowledge of the application. Having a rough estimate is \nimportant for deciding whether to start with Memory Mode or App Direct mode.\n\u0007Use Cases Requiring Persistence\nUse cases that take advantage of persistent memory for persistence, as opposed to the \nvolatile use cases previously described, are generally replacing slower storage devices \nwith persistent memory. Determining the suitability of a workload for this use case is \nstraightforward. If application performance is limited by storage accesses (disks, SSDs, \netc.), then using a faster storage solution like persistent memory could help. There are \nseveral ways to identify storage bottlenecks in an application. Open source tools like \ndstat or iostat give a high-level overview of disk activity, and tools such as VTune \nProfiler provide a more detailed analysis.\nFigure 15-3.  Objects accessed by the application during a Memory Access analysis \ndata collection\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page325_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page325_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1491.37 sec"
            },
            {
              "page_number": 326,
              "text": "302\nFigure 15-4 shows throughput and IOPS numbers of an NVMe drive collected \nusing Platform Profiler. This example uses a non-volatile disk for extensive storage, as \nindicated by the throughput and IOPS graphs. Applications like this may benefit from \nfaster storage like persistent memory. Another important metric to identify storage \nbottlenecks is I/O Wait time. The Platform Profiler analysis can also provide this metric \nand display how it is affecting CPU Utilization over time, as seen in Figure 15-5.\n\u0007Performance Analysis of Workloads Using \nPersistent Memory\nOptimizing a workload on a system with persistent memory follows the principles \nsimilar to those of optimizing a workload performance on a DRAM-only system.  \nThe additional factors to keep in mind are: \nFigure 15-4.  Disk throughput and IOPS graphs from VTune Profiler’s Platform \nProfiler\nFigure 15-5.  I/O Wait time from VTune Profiler’s Platform Profiler\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page326_img1.jpeg",
                "output\\images\\Programming_Persistent_Memory_medium_457_page326_img2.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page326_img1_summary.json",
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page326_img2_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1494.64 sec"
            },
            {
              "page_number": 327,
              "text": "303\n•\t\nThe writes to persistent memory may impact performance more than \nthe reads.\n•\t\nApplications can allocate objects on DRAM or persistent memory.  \nIf done indiscriminately, this can negatively impact performance.\n•\t\nIn Memory Mode (specific to Intel Optane DC persistent memory), \nusers have the option of varying the near-memory cache size (DRAM \nsize) to improve workload performance.\nKeeping these additional factors in mind, the approach to workload performance \noptimization will follow the same process of characterizing the workload, choosing the \ncorrect memory configuration, and optimizing the code for maximum performance.\n\u0007Characterizing the Workload\nThe performance of a workload on a persistent memory system depends on a \ncombination of the workload characteristics and the underlying hardware. The key \nmetrics to understand the workload characteristics are: \n•\t\nPersistent memory bandwidth\n•\t\nPersistent memory read/write ratio\n•\t\nPaging to and from traditional storage\n•\t\nWorking set size and footprint of the workload\n•\t\nNonuniform Memory Architecture (NUMA) characteristics\n•\t\nNear-memory cache behavior in Memory Mode (specific to Intel \nOptane DC persistent memory)\n\u0007Memory Bandwidth and Latency\nPersistent memory, like DRAM, has limited bandwidth. When it becomes saturated, it \ncan quickly bottleneck application performance. Bandwidth limits will vary depending \non the platform. You can calculate the peak bandwidth of your platform using hardware \nspecifications or a memory benchmarking application.\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1499.25 sec"
            },
            {
              "page_number": 328,
              "text": "304\nThe Intel Memory Latency Checker (Intel MLC) is a free tool for Linux and Windows \navailable from https://software.intel.com/en-us/articles/intelr-memory-­\nlatency-checker. Intel MLC can be used to measure bandwidth and latency of DRAM \nand persistent memory using a variety of tests:\n•\t\nMeasure idle memory latencies between each CPU socket\n•\t\nMeasure peak memory bandwidth requests with varying ratios of \nreads and writes\n•\t\nMeasure latencies at different bandwidth points\n•\t\nMeasure latencies for requests addressed to a specific memory \ncontroller from a specific core\n•\t\nMeasure cache latencies\n•\t\nMeasure b/w from a subset of the cores/sockets\n•\t\nMeasure b/w for different read/write ratios\n•\t\nMeasure latencies for random and sequential address patterns\n•\t\nMeasure latencies for different stride sizes\n•\t\nMeasure cache-to-cache data transfer latencies\nVTune Profiler has a built-in kernel to measure peak bandwidth on a system. Once \nyou know the peak bandwidth of the platform, you can then measure the persistent \nmemory bandwidth of your workload. This will reveal whether persistent memory \nbandwidth is a bottleneck. Figure 15-6 shows an example of persistent memory read and \nwrite bandwidth of an application.\nFigure 15-6.  Results from VTune Profiler persistent memory bandwidth \nmeasurements\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page328_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page328_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1502.12 sec"
            },
            {
              "page_number": 329,
              "text": "305\n\u0007Persistent Memory Read-Write Ratio\nAs described in “Performance Analysis Concepts,” the ratio of read and write traffic \nto the persistent memory plays a major role in the overall performance of a workload. \nIf the ratio of persistent memory write bandwidth to read bandwidth is high, there is \na good chance the persistent memory write latency is impacting performance. Using \nthe Platform Profiler feature in VTune Profiler is one way to collect this information. \nFigure 15-7 shows the ratio of read traffic vs. all traffic to persistent memory. This \nnumber should be close to 1.0 for best performance.\n\u0007Working Set Size and Memory Footprint\nAs described in “Determining the Suitability of Workloads for Persistent Memory,” the \nworking set size and memory footprint of the application are important characteristics \nto understand once a workload is running on a system with persistent memory. Metrics \ncan be collected using the tools and processes previously described.\n\u0007Non-Uniform Memory Architecture (NUMA) Behavior\nMulti-socket platforms typically have persistent memory attached to each socket. \nAccesses to persistent memory from a thread on one socket to another will incur longer \nlatencies. These “remote” accesses are some of the NUMA behaviors that can impact \nperformance. Multiple metrics can be collected to determine how much NUMA activity \nis occurring in a workload. On Intel platforms, data moves between sockets through the \nsocket interconnect called the QuickPath Interconnect (QPI) or Ultra Path Interconnect \n(UPI). High interconnect bandwidth may indicate NUMA-related performance issues. In \naddition to interconnect bandwidth, some hardware provides counters to track local and \nremote accesses to persistent memory.\nFigure 15-7.  Read traffic ratio from VTune Profiler’s Platform Profiler analysis\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page329_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page329_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1508.06 sec"
            },
            {
              "page_number": 330,
              "text": "306\nUnderstanding the NUMA behavior of your workload is another important step in \nunderstanding performance optimization. Figure 15-8 shows UPI bandwidth collected \nwith VTune Profiler.\nThe Platform Profiler feature in VTune Profiler can collect metrics specific to \npersistent memory.\n\u0007Tuning the Hardware\nThe memory configuration of a system is a significant factor in determining the system’s \nperformance. The workload performance depends on a combination of workload \ncharacteristics and the memory configuration. There is no single configuration that \nprovides the best value for all workloads. These factors make it important to tune the \nhardware with respect to workload characteristics and get the maximum value out of the \nsystem.\n\u0007Addressable Memory Capacity\nThe combined capacity of DRAM and persistent memory determines the total \naddressable memory available on the system. You should tune the size of persistent \nmemory to accommodate the workload’s footprint.\nThe capacity of DRAM available on the system should be large enough to \naccommodate the workload’s hot working set size. A large amount of volatile traffic going \nto persistent memory while DRAM is fully utilized is a good indicator that the workload \ncan benefit from additional DRAM size.\nFigure 15-8.  UPI traffic ratio from VTune Profiler\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page330_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page330_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1512.56 sec"
            },
            {
              "page_number": 331,
              "text": "307\n\u0007Bandwidth Requirements\nThe maximum available persistent memory bandwidth depends on the number of \nchannels populated with a persistent memory module. A fully populated system works \nwell for a workload with a high bandwidth requirement. Partially populated systems \ncan be used for workloads that are not as memory latency sensitive. Refer to the server \ndocumentation for population guidelines.\n\u0007BIOS Options\nWith the introduction of persistent memory into server platforms, many features and \noptions have been added to the BIOS that provide additional tuning capabilities. The \noptions and features available within the BIOS vary for each server vendor and persistent \nmemory product. Refer to the server BIOS documentation for all the options available; \nmost share common options, including: \n•\t\nAbility to change power levels to balance power consumption and \nperformance. More power delivered to persistent memory can \nincrease performance \n•\t\nEnable or disable persistent memory–specific features \n•\t\nTune latency or bandwidth characteristics of persistent memory \n\u0007Optimizing the Software for Persistent Memory\nThere are many ways to optimize applications to use persistent memory efficiently and \neffectively. Each application will benefit in different ways and will need to have code \nmodified accordingly. This section describes some of the optimization methods.\n\u0007Guided Data Placement\nGuided data placement is the most common avenue for optimizing volatile workloads \non a persistent memory system. Application developers can choose to allocate a data \nstructure or object in DRAM or persistent memory. It is important to choose accurately \nbecause allocating incorrectly could impact application performance. This allocation \nis usually handled via specific APIs, for example, the allocation APIs available in the \nPersistent Memory Development Kit (PMDK) and memkind library.\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1516.25 sec"
            },
            {
              "page_number": 332,
              "text": "308\nDepending on your familiarity with the code and how it works with production \nworkloads, knowing which data structures and objects to store in the different memory/\nstorage tiers may be simple. Should those data structures and objects be volatile or \npersisted? To help with searching for potential candidates, tools such as VTune Profiler \ncan identify objects with the most last-level cache (LLC) misses. The intent is to identify \nwhat data structures and objects the application uses most frequently and ensure they \nare placed in the fastest media appropriate to their access patterns. For example, an \nobject that is written once but read many times is best placed in DRAM. An object that \nis updated frequently that needs to be persisted should probably be moved to persistent \nmemory rather than traditional storage devices.\nYou must also be mindful of memory-capacity constraints. Tools such as VTune \nProfiler can help determine approximately how many hot objects will fit into the \navailable DRAM. For the remaining objects that have fewer LLC misses or that are too \nlarge to allocate from DRAM, you can put them in persistent memory. These steps will \nensure that your most accessed objects have the fastest path to the CPU (allocated in \nDRAM), while the infrequently accessed objects will take advantage of the additional \npersistent memory (as opposed to sitting out on a much slower storage devices).\nAnother consideration for optimizations is the load/store ratio for object accesses. If \nyour persistent memory hardware characteristics are such that load/read operations are \nmuch faster than stores/writes, this should be taken into account. Objects with a high \nload/store ratio should benefit from living in persistent memory.\nThere is no hard rule for what constitutes a frequent vs. infrequently accessed object. \nAlthough behaviors are application dependent, these guidelines give a starting point for \nchoosing how to allocate objects in persistent memory. After completing this process, \nstart profiling and tuning the application to further improve the performance with \npersistent memory.\n\u0007Memory Access Optimization\nThe common techniques for optimizing cache performance on DRAM-only platforms \nalso apply to persistent memory platforms. Concepts like cache-miss penalties and \nspatial/temporal data locality are important for performance. Many tools can collect \nperformance data for caches and memory. VTune Profiler has predefined metrics for \neach level of the memory hierarchy, including Intel Optane DC persistent memory \nshown in Figure 15-9.\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1523.32 sec"
            },
            {
              "page_number": 333,
              "text": "309\nThese performance metrics help to determine if memory is the bottleneck in your \napplication, and if so, which level of the memory hierarchy is the most impactful. Many \ntools can pinpoint source code locations and memory objects responsible for the \nbottleneck. If persistent memory is the bottleneck, review the “Guided Data Placement” \nsection to ensure that persistent memory is being used efficiently. Performance \noptimizations like cache blocking, software prefetching, and improved memory access \npatterns may also help relieve bottlenecks in the memory hierarchy. You must determine \nhow to refactor the software to more efficiently use memory, and metrics like these can \npoint you in the right direction.\n\u0007NUMA Optimizations\nNUMA-related performance issues were described in the “Characterizing the Workload” \nsection; we discuss NUMA in more detail in Chapter 19. If you identify performance \nissues related to NUMA memory accesses, two things should be considered: data \nallocation vs. first access, and thread migration.\nData Allocation vs. First Access\nData allocation is the process of allocating or reserving some amount of virtual address \nspace for an object. The virtual address space for a process is the set of virtual memory \naddresses that it can use. The address space for each process is private and cannot be \naccessed by other processes unless it is shared. A virtual address does not represent \nthe actual physical location of an object in memory. Instead, the system maintains a \nmultilayered page table, which is an internal data structure used to translate virtual \naddresses into their corresponding physical addresses. Each time an application thread \nFigure 15-9.  VTune Profiler memory analysis of a workload showing a \nbreakdown of CPU cache, DRAM, and persistent memory accesses\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page333_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page333_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1526.29 sec"
            },
            {
              "page_number": 334,
              "text": "310\nreferences an address, the system translates the virtual address to a physical address.  \nThe physical address points to memory physically connected to a CPU. Chapter 19 \ndescribes exactly how this operation works and shows why high-capacity memory \nsystems can benefit from using large or huge pages provided by the operating system.\nA common practice in software is to have most of the data allocations done when the \napplication starts. Operating systems try to allocate memory associated with the CPU on \nwhich the thread executes. The operating system scheduler then tries to always schedule \nthe thread on a CPU that it last ran in the hopes that the data still remains in one of the \nCPU caches. On a multi-socket system, this may result in all the objects being allocated \nin the memory of a single socket, which can create NUMA performance issues. Accessing \ndata on a remote CPU incurs a latency performance penalty.\nSome applications delay reserving memory until the data is accessed for the first \ntime. This can alleviate some NUMA issues. It is important to understand how your \nworkload allocates data to understand the NUMA performance.\nThread Migration\nThread migration, which is the movement of software threads across sockets by the \noperating system scheduler, is the most common cause of NUMA issues. Once objects \nare allocated in memory, accessing them from another physical CPU from which they \nwere originally allocated incurs a latency penalty. Even though you may allocate your \ndata on a socket where the accessing thread is currently running, unless you have \nspecific affinity bindings or other safeguards, the thread may move to any other core or \nsocket in the future. You can track thread migration by identifying which cores threads \nare running on and which sockets those cores belong to. Figure 15-10 shows an example \nof this analysis from VTune Profiler.\nFigure 15-10.  VTune Profiler identifying thread migration across cores and \nsockets (packages)\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page334_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page334_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1531.81 sec"
            },
            {
              "page_number": 335,
              "text": "311\nUse this information to determine whether thread migration is correlated with \nNUMA accesses to remote DRAM or persistent memory.\n\u0007Large and Huge Pages\nThe default memory page size in most operating systems is 4 kilobytes (KiB). Operating \nsystems provide many different page sizes for different application workloads and \nrequirements. In Linux, a Large Page is 2 megabytes (MiB), and a Huge Page is 1 gigabyte \n(GiB). The larger page sizes can be beneficial to workload performance on persistent \nmemory in certain scenarios.\nFor applications with a large addressable memory requirement, the size of the page \ntable being maintained by the operating system for virtual to physical address translation \ngrows significantly larger in size. The translation lookaside buffer (TLB) is a small cache \nto make virtual-to-physical address translations faster. The efficiency of TLB goes down \nwhen the number of page entries increases in the page table. Chapter 19 describes this in \nmore detail.\nPersistent memory systems that are meant for applications with a large memory \nrequirement will likely encounter the problem of large page tables and inefficient TLB \nusage. Using large page sizes in this scenario helps reduce the number of entries in \nthe page table. The main trade-offs when using large page sizes is a higher overhead \nfor each allocation and memory fragmentation. You must be aware of the application \nbehavior before using large pages on persistent memory. An application doing frequent \nallocation/deallocation may not be a good fit for large page optimization. The memory \nfragmentation issue is somewhat abated by the large address space available on the \npersistent memory systems.\n\u0007Summary\nProfiling and performance optimization techniques for persistent memory systems \nare similar to those techniques used on systems without persistent memory. This \nchapter outlined some important concepts for understanding performance. It also \nprovides guidance for characterizing an existing application without persistent memory \nand understanding whether it is suitable for persistent memory. Finally, it presents \nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1537.45 sec"
            },
            {
              "page_number": 336,
              "text": "312\nimportant metrics for performance analysis and tuning of applications running on \npersistent memory platforms, including some examples of how to collect the data using \nthe VTune Profiler tool.\nPerformance profiling and optimization are an iterative process that only ends \nwhen you determine that the investment required for the next improvement is too high \nfor the benefit that will be returned. Use the concepts introduced in this chapter to \nunderstand how your workloads can benefit from persistent memory, and use some of \nthe optimization techniques we discussed to tune for this type of platform.\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 15  Profiling and Performance",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1540.01 sec"
            },
            {
              "page_number": 337,
              "text": "313\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_16\nCHAPTER 16\nPMDK Internals: \nImportant Algorithms \nand Data Structures\nChapters 5 through 10 describe most of the libraries contained within the Persistent \nMemory Development Kit (PMDK) and how to use them.\nThis chapter introduces the fundamental algorithms and data structures on which \nlibpmemobj is built. After we first describe the overall architecture of the library, we \ndiscuss the individual components and the interaction between them that makes \nlibpmemobj a cohesive system.\n\u0007A Pool of Persistent Memory: High-Level \nArchitecture Overview\nFigure 16-1 shows that libpmemobj comprises many isolated components that build on \ntop of each other to provide a transactional object store.\nFigure 16-1.  The modules of the libpmemobj architecture",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page337_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page337_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1542.87 sec"
            },
            {
              "page_number": 338,
              "text": "314\nEverything is built on top of libpmem and its persistence primitives that the library \nuses to transfer data to persistent memory and persist it. Those primitives are also \nexposed through libpmemobj-specific APIs to applications that wish to perform low-­level \noperations on persistent memory, such as manual cache flushing. These APIs are exposed \nso the high-level library can instrument, intercept, and augment all stores to persistent \nmemory. This is useful for the instrumentation of runtime analysis tools such as Valgrind \npmemcheck, described in Chapter 12. More importantly, these functions are interception \npoints for data replication, both local and remote.\nReplication is implemented in a way that ensures all data written prior to calling \ndrain will be safely stored in the replica as configured. A drain operation is a barrier that \nwaits for hardware buffers to complete their flush operation to ensure all writes have \nreached the media. This works by initiating a write to the replica when a memory copy \nor a flush is performed and then waits for those writes to finish in the drain call. This \nmechanism guarantees the same behavior and ordering semantics for replicated and \nnon-replicated pools.\nOn top of persistence primitives provided by libpmem is an abstraction for fail-safe \nmodification of transactional data called unified logging. The unified log is a single \ndata structure and API for two different logging types used throughout libpmemobj to \nensure fail-safety: transactions and atomic operations. This is one of the most crucial, \nperformance-sensitive modules in the library because it is the hot code path of almost \nevery API. The unified log is a hybrid DRAM and persistent memory data structure \naccessed through a runtime context that organizes all memory operations that need \nto be performed within a single fail-safe atomic transaction and allows for critical \nperformance optimizations.\nThe persistent memory allocator operates in the unified log context of either a \ntransaction or a single atomic operation. This is the largest and most complex module in \nlibpmemobj and is used to manage the potentially large amounts of persistent memory \nassociated with the memory pool.\nEach object stored in a persistent memory pool is represented by an object handle \nof type PMEMoid (persistent memory object identifier). In practice, such a handle is \na unique object identifier (OID) of global scope, which means that two objects from \ndifferent pools will never have the same OID. An OID cannot be used as a direct pointer \nto an object. Each time the program attempts to read or write object data, it must obtain \nthe current memory address of the object by converting its OID into a pointer. In contrast \nto the memory address, the OID value for a given object does not change during the \nlife of an object, except for a realloc(), and remains valid after closing and reopening \nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1550.86 sec"
            },
            {
              "page_number": 339,
              "text": "315\nthe pool. For this reason, if an object contains a reference to another persistent object, \nfor example, to build a linked data structure, the reference must be an OID and not a \nmemory address.\nThe atomic and transactional APIs are built using a combination of the persistent \nmemory allocator and unified logs. The simplest public interface is the atomic API which \nruns a single allocator operation in a unified log context. That log context is not exposed \nexternally and is created, initialized, and destroyed within a single function call.\nThe most general-purpose interface is the transactional API, which is based on a \ncombination of undo logging for snapshots and redo logging for memory allocation and \ndeallocation. This API has ACID (atomicity, consistency, isolation, durability) properties, \nand it is a relatively thin layer that combines the utility of unified logs and the persistent \nmemory allocator.\nFor specific transactional use cases that need low-level access to the persistent \nmemory allocator, there is an “action” API. The action API is essentially a pass-through \nto the raw memory allocator interface, alongside helpers for usability. This API can \nbe leveraged to create low-overhead algorithms that issue fewer memory fences, as \ncompared to general-purpose transactions, at the cost of ease of use.\nAll public interfaces produce and operate on PMEMoids as a replacement for \npointers. This comes with space overhead because PMEMoids are 16 bytes. There is \nalso a performance overhead for the translation to a normal pointer. The upside is that \nobjects can be safely referenced between different instances of the application and even \ndifferent persistent memory pools.\nThe pool management API opens, maps, and manages persistent memory resident \nfiles or devices. This is where the replication is configured, metadata and the heap are \ninitialized, and all the runtime data is created. This is also where the crucial recovery of \ninterrupted transactions happens. Once recovery is complete, all prior transactions are \neither committed or aborted, the persistent state is consistent, and the logs are clean and \nready to be used again.\n\u0007The Uncertainty of Memory Mapping: Persistent \nMemory Object Identifier\nA key concept that is important for any persistent memory application is how to \nrepresent the relative position of an object within a pool of memory, and even beyond \nit. That is, how do you implement pointers? You could rely on normal pointers, which \nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1559.56 sec"
            },
            {
              "page_number": 340,
              "text": "316\nare relative to the beginning of the application’s virtual address space, but that comes \nwith many caveats. Using such pointers would be predicated on the pool of persistent \nmemory always being located at the same place in the virtual address space of an \napplication that maps it. This is difficult, if not impossible, to accomplish in a portable \nway on modern operating systems due to address space layout randomization (ASLR). \nTherefore, a general-purpose library for persistent memory programming must provide \na specialized persistent pointer. Figure 16-2 shows a pointer from Object A to Object B. If \nthe base address changes, the pointer no longer points to Object B.\nAn implementation of a general-purpose relative persistent pointer should satisfy \nthese two basic requirements:\n\t 1.\t The pointer must remain valid across application restarts.\n\t 2.\t The pointer should unambiguously identify a memory location in \nthe presence of many persistent memory pools, even if not located \nin a pool from which it was originally derived.\nIn addition to the previous requirements, you should also consider some potential \nperformance problems:\n•\t\nAdditional space overhead over a traditional pointer. This is important \nbecause large fat pointers would take up more space in memory and \nbecause fewer of these fat pointers would fit in a single CPU cache line. \nThis potentially increases the cost of operations in pointer-chasing \nheavy data structures, such as those found in B-tree algorithms.\n•\t\nThe cost of translating persistent pointers to real pointers. Because \ndereferencing is an extremely common operation, this calculation \nmust be as lightweight as possible and should involve as few \ninstructions as possible. This is to ensure that persistent pointer \nusage is efficient and it doesn’t generate too much code bloat during \ncompilation.\nFigure 16-2.  Example of using a normal pointer in a persistent memory pool\nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page340_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page340_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1561.51 sec"
            },
            {
              "page_number": 341,
              "text": "317\n•\t\nPreventing compiler optimizations through the dereferencing \nmethod. A complicated pointer translation might negatively impact \nthe compiler’s optimization passes. The translation method should \nideally avoid operations that depend on an external state because \nthat will prevent, among other things, auto-vectorization.\nSatisfying the preceding requirements while maintaining low-overhead and C99 \nstandard compliance is surprisingly difficult. We explored several options:\n•\t\nThe 8-byte offset pointer, relative to the beginning of the pool, was \nquickly ruled out because it did not satisfy the second requirement \nand needed a pool base pointer to be provided to the translation \nmethod.\n•\t\n8-byte self-relative pointers, where the value of the pointer is the \noffset between the object’s location and the pointer’s location. This \nis potentially the fastest implementation because the translation \nmethod can be implemented as `ptr + (*ptr)`. However, this does \nnot satisfy the second basic requirement. Additionally, it would \nrequire a special assignment method because the value of the pointer \nto the same object would differ depending on the pointer’s location.\n•\t\n8-byte offset pointers with embedded memory pool identifier, \nwhich allows the library to satisfy the second requirement. This is \nan augmentation of the first method that additionally stores the \nidentifier in the unused part of the pointer value by taking advantage \nof the fact that the usable size of the virtual address space is smaller \nthan the size of the pointer on most modern CPUs. The problem with \nthis method, however, is that the number of bits for the pool identifier \nis relatively small (16 bits on modern CPUs) and might shrink with \nfuture hardware.\n•\t\n16-byte fat offset pointer with pool identifier. This is the most obvious \nsolution, which is similar to the one earlier but has 8-byte offset \npointers and 8-byte pool identifiers. Fat pointers provide the best \nutility, at the cost of space overhead and some runtime performance.\nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1567.14 sec"
            },
            {
              "page_number": 342,
              "text": "318\nlibpmemobj uses the most generic approach of the 16-byte offset pointer. This allows \nyou to make your own choice since all other pointer types can be directly derived from \nit. libpmemobj bindings for more expressive languages than C99, such as C++, can also \nprovide different types of pointers with different trade-offs.\nFigure 16-3 shows the translation method used to convert a libpmemobj persistent \npointer, PMEMoid, into a valid C pointer. In principle, this approach is very simple. \nWe look up the base address of the pool through the pool identifier and then add the \nobject offset to it. The method itself is static inline and defined in the public header file \nfor libpmemobj to avoid a function call on every deference. The problem is the lookup \nmethod, which, for an application linked with a dynamic library, means a call to a \ndifferent compilation unit, and that might be costly for a very commonly performed \noperation. To resolve this problem, the translation method has a per-thread cache \nof the last base address, which removes the necessity of calling the lookup with each \ndereferencing for the common case where persistent pointers from the same pool are \naccessed close together.\nThe pool lookup method itself is implemented using a radix tree that stores \nidentifier-address pairs. This tree has a lock-free read operation, which is necessary \nbecause each non-cached pointer translation would otherwise have to acquire a lock to \nbe thread-safe, and that would have a severe negative performance impact and could \npotentially serialize access to persistent memory.\n\u0007Persistent Thread Local Storage: Using Lanes\nVery early in the development of PMDK, we found that persistent memory \nprogramming closely resembles multithreaded programming because it requires \nrestricting visibility of memory changes – either through locking or transactions – to \nother threads or instances of the program. But that is not the only similarity. The \nother similarity, which we discuss in this section, is how sometimes low-level code \nFigure 16-3.  Example of using a PMEMoid in a persistent memory pool\nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page342_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page342_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1572.26 sec"
            },
            {
              "page_number": 343,
              "text": "319\nneeds to store data that is unique to one thread of execution. In the persistent case, \nwe often need to associate data with a transaction rather than a thread.\nIn libpmemobj, we need a way to create an association between an in-flight \ntransaction and its persistent logs. It also requires a way to reconnect to those logs after \nan unplanned interruption. The solution is to use a data structure called a “lane,” which \nis simply a persistent byte buffer that is also transaction local.\nLanes are limited in quantity, have a fixed size, and are located at the beginning \nof the pool. Each time a transaction starts, it chooses one of the lanes to operate from. \nBecause there is a limited number of lanes, there is also a limited number of transactions \nthat can run in parallel. For this reason, the size of the lane is relatively small, but the \nnumber of lanes is big enough as to be larger than a number of application threads \nthat could feasibly run in parallel on current platforms and platforms coming in the \nforeseeable future.\nThe challenge of the lane mechanism is the selection algorithm, that is, which lane \nto choose for a specific transaction. It is a scheduler that assigns resources (lanes) to \nperform work (transactions).\nThe naive algorithm, which was implemented in the earliest versions of libpmemobj, \nsimply picked the first available lane from the pool. This approach has a few problems. \nFirst, the implementation of what effectively amounts to a single LIFO (last in, first \nout) data structure of lanes requires a lot of synchronization on the front of the stack, \nregardless of whether it is implemented as a linked list or an array, and thus reducing \nperformance. The second problem is false sharing of lane data. False sharing occurs \nwhen two or more threads operate on data that is being modified, causing CPU cache \nthrashing. And that is exactly what happens if multiple threads are continually fighting \nover the same number of lanes to start new transactions. The third problem is spreading \nthe traffic across interleaved DIMMs. Interleaving is a technique that allows sequential \ntraffic to take advantage of throughput of all of the DIMMs in the interleave set by \nspreading the physical memory across all available DIMMs. This is similar to striping \n(RAID0) across multiple disk drives. Depending on the size of the interleaved block, and \nthe platform configuration, using naive lane allocation might continuously use the same \nphysical DIMMs, lowering the overall performance.\nTo alleviate these problems, the lane scheduling algorithm in libpmemobj is more \ncomplex. Instead of using a LIFO data structure, it uses an array of 8-byte spinlocks, one \nfor each lane. Each thread is initially assigned a primary lane number, which is assigned \nin such a way as to minimize false sharing of both lane data and the spinlock array.  \nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1579.33 sec"
            },
            {
              "page_number": 344,
              "text": "320\nThe algorithm also tries to spread the lanes evenly across interleaved DIMMs. As long as \nthere are fewer active threads than lanes, no thread will ever share a lane. When a thread \nattempts to start a transaction, it will try to acquire its primary lane spinlock, and if it is \nunsuccessful, it will try to acquire the next lane in the array.\nThe final lane scheduling algorithm decision took a considerable amount of research \ninto various lane scheduling approaches. Compared to the naive implementation, \nthe current implementation has vastly improved performance, especially in heavily \nmultithreaded workloads.\n\u0007Ensuring Power-Fail Atomicity: Redo and Undo \nLogging\nThe two fundamental concepts libpmemobj uses to ensure power-fail safety are redo \nand undo logging. Redo logs are used to ensure atomicity of memory allocations, while \nundo logs are used to implement transactional snapshots. Before we discuss the many \ndifferent possible implementation approaches, this section describes the basic ideas.\n\u0007Transaction Redo Logging\nRedo logging is a method by which a group of memory modifications that need to be \ndone atomically are stored in a log and deferred until all modifications in the group are \npersistently stored. Once completed, the log is marked as complete, and the memory \nmodifications are processed (applied); the log can then be discarded. If the processing is \ninterrupted before it finishes, the logging is repeated until successful. Figure 16-4 shows \nthe four phases of transaction redo logging.\nFigure 16-4.  The phases of a transaction redo log\nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page344_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page344_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1583.42 sec"
            },
            {
              "page_number": 345,
              "text": "321\nThe benefit of this logging approach, in the context of persistent memory, is that all the \nlog entries can be written and flushed to storage at once. An optimal implementation of \nredo logging uses only two synchronization barriers: once to mark the log as complete and \nonce to discard it. The downside to this approach is that the memory modifications are \nnot immediately visible, which makes for a more complicated programming model. Redo \nlogging can sometimes be used alongside load/store instrumentation techniques which \ncan redirect a memory operation to the logged location. However, this approach can be \ndifficult to implement efficiently and is not well suited for a general-purpose library.\n\u0007Transaction Undo Logging\nUndo logging is a method by which each memory region of a group (undo transaction) \nthat needs to be modified atomically is snapshotted into a log prior to the modification. \nOnce all memory modifications are complete, the log is discarded. If the transaction \nis interrupted, the modifications in the log are rolled back to their original state. \nFigure 16-5 shows the three phases of the transaction undo logging.\nThis type of log can have lower performance characteristics compared with the redo \nlog approach because it requires a barrier for every snapshot that needs to be made, \nand the snapshotting itself must be fail-safe atomic, which presents its own challenges. \nAn undo log benefit is that the changes are visible immediately, allowing for a natural \nprogramming model.\nThe important observation here is that redo and undo logging are complimentary. \nUse redo logging for performance-critical code and where deferred modifications are \nnot a problem; use undo logging where ease of use is important. This observation led \nto the current design of libpmemobj where a single transaction takes advantage of both \nalgorithms.\nFigure 16-5.  Phases of a transaction undo log\nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page345_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page345_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1588.34 sec"
            },
            {
              "page_number": 346,
              "text": "322\n\u0007libpmemobj Unified Logging\nBoth redo and undo logging in libpmemobj share the same internal interface and data \nstructure, which is called a unified log (or ulog for short). This is because redo and undo \nlogging only differ in the execution order of the log phases, or more precisely, when \nthe log is applied on commit or recovery. In practice, however, there are performance \nconsiderations that require specialization in certain parts of the algorithm.\nThe ulog data structure contains one cache line header with metadata and a variable \nlength array of data bytes. The header consists of: \n•\t\nA checksum for both the header and data, used only for redo logs\n•\t\nA monotonically increasing generation number of a transaction in \nthe log, used only for undo logs\n•\t\nThe total length in bytes of the data array\n•\t\nAn offset of the next log in the group\nThe last field is used to create a singly linked list of all logs that participate in a single \ntransaction. This is because it is impossible to predict the total required size of the log \nat the beginning of the transaction, so the library cannot allocate a log structure that is \nthe exact required length ahead of time. Instead, the logs are allocated on demand and \natomically linked into a list.\nThe unified log supports two ways of fail-safe inserting of entries:\n\t 1.\t Bulk insert takes an array of log entries, prepares the header of \nthe log, and creates a checksum of both the header and data. Once \ndone, a non-temporal copy, followed by a fence, is performed \nto store this structure into persistent memory. This is the way in \nwhich a group of deferred memory modifications forms a redo \nlog with only one additional barrier at the end of the transaction. \nIn this case, the checksum in the header is used to verify the \nconsistency of the entire log. If that checksum doesn’t match, the \nlog is skipped during recovery.\n\t 2.\t Buffer insert takes only a single entry, checksums it together \nwith the current generation number, and stores it in persistent \nmemory through non-temporal stores followed by a fence. This \nmethod is used to create undo logs when snapshotting. Undo logs \nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1594.38 sec"
            },
            {
              "page_number": 347,
              "text": "323\nin a transaction are different than redo logs because during the \ncommit’s fast path, they need to be invalidated instead of applied. \nInstead of laboriously writing zeros into the log buffer, the log is \ninvalidated by incrementing the generation number. This works \nbecause the number is part of the data with its checksum, so \nchanging the generation number will cause a checksum failure. \nThis algorithm allows libpmemobj to have only one additional \nfence for the transaction (on top of the fences needed for \nsnapshots) to ensure fail-safety of a log, resulting in very low-\noverhead transactions.\n\u0007Persistent Allocations: The Interface \nof a Transactional Persistent Allocator\nThe internal allocator interface in libpmemobj is far more complex than a typical volatile \ndynamic memory allocator. First, it must ensure fail-safety of all its operations and \ncannot allow for any memory to become unreachable due to interruptions. Second, it \nmust be transactional so that multiple operations on the heap can be done atomically \nalongside other modifications. And lastly, it must operate on the pool state, allocating \nmemory from specific files instead of relying on the anonymous virtual memory \nprovided by the operating system. All these factors contribute to an internal API that \nhardly resembles the standard malloc() and free(), shown in Listing 16-1.\nListing 16-1.  The core persistent memory allocator interface that splits heap \noperations into two distinct steps\nint palloc_reserve(struct palloc_heap *heap, size_t size,...,\n        struct pobj_action *act);\nvoid palloc_publish(struct palloc_heap *heap,\n        struct pobj_action *actv, size_t actvcnt,\n        struct operation_context *ctx);\nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1598.99 sec"
            },
            {
              "page_number": 348,
              "text": "324\nAll memory operations, called “actions” in the API, are broken up into two individual \nsteps.\nThe first step reserves the state that is needed to perform the operation. For \nallocations, this means retrieving a free memory block, marking it as reserved, and \ninitializing the object’s content. This reservation is stored in a user-provided runtime \nvariable. The library guarantees that if an application crashes while holding reservations, \nthe persistent state is not affected. That is why these action variables must not be \npersistent.\nThe second step is the act of exercising the reservations, which is called \n“publication.” Reservations can be published individually, but the true power of this API \nlies in its ability to group and publish many different actions together.\nThe internal allocator API also has a function to create an action that will set a \nmemory location to a given value when published. This is used to modify the destination \npointer value and is instrumental in making the atomic API of libpmemobj fail-safe.\nAll internal allocator APIs that need to perform fail-safe atomic actions take \noperation context as an argument, which is the runtime instance of a single log. It \ncontains various state information, such as the total capacity of the log and the current \nnumber of entries. It exposes the functions to create either bulk or singular log entries. \nThe allocator’s functions will log and then process all metadata modifications inside of \nthe persistent log that belongs to the provided instance of the operating context.\n\u0007Persistent Memory Heap Management: Allocator \nDesign for Persistent Memory\nThe previous section described the interface for the memory allocation used internally \nin libpmemobj, but that was only the tip of the allocator iceberg. Before diving deeper \ninto this topic, we briefly describe the principles behind normal volatile allocators so you \ncan understand how persistent memory impacts the status quo.\nTraditional allocators for volatile memory are responsible for efficient – in both time \nand space – management of operating system–provided memory pages. Precisely how \nthis should be done for the generic case is an active research area of computer science; \nmany different techniques can be used. All of them try to exploit the regularities in \nallocation and deallocation patterns to minimize heap fragmentation.\nMost commonly used general-purpose memory allocators settled on an algorithm \nthat we refer to as “segregated fit with page reuse and thread caching.”\nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1605.76 sec"
            },
            {
              "page_number": 349,
              "text": "325\nThis works by using a free list for many different sizes, shown in Figure 16-6, \nuntil some predefined threshold, after which it is sensible to allocate directly from \nthe operating system. Those free lists are typically called bins or buckets and can be \nimplemented in various ways, such as a simple linked list or contiguous buffer with \nboundary tags. Each incoming memory allocation request is rounded up to match \none of the free lists, so there must be enough of them to minimize the amount of \noverprovisioned space for each allocation. This algorithm approximates a best-fit \nallocation policy that selects the memory block with the least amount of excess space for \nthe request from the ones available.\nUsing this technique allows memory allocators to have average-case O(1) complexity \nwhile retaining the memory efficiency of best fit. Another benefit is that rounding up of \nmemory blocks and subsequent segregation forces some regularity to allocation patterns \nthat otherwise might not exhibit any.\nSome allocators also sort the available memory blocks by address and, if possible, \nallocate the one that is spatially collocated with previously selected blocks. This \nimproves space efficiency by increasing the likelihood of reusing the same physical \nmemory page. It also preserves temporal locality of allocated memory objects, which can \nminimize cache and translation lookaside buffer (TLB) misses.\nOne important advancement in memory allocators is scalability in multithreaded \napplications. Most modern memory allocators implement some form of thread caching, \nwhere the vast majority of allocation requests are satisfied directly from memory that \nis exclusively assigned to a given thread. Only when memory assigned to a thread is \nentirely exhausted, or if the request is very large, the allocation will contend with other \nthreads for operating system resources.\nThis allows for allocator implementations that have no locks of any kind, not even \natomics, on the fast path. This can have a potentially significant impact on performance, \neven in the single-threaded case. This technique also prevents allocator-induced false \nsharing between threads, since a thread will always allocate from its own region of \nFigure 16-6.  Example of free lists in a memory allocator\nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page349_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page349_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1611.79 sec"
            },
            {
              "page_number": 350,
              "text": "326\nmemory. Additionally, the deallocation path often returns the memory block to the \nthread cache from which it originated, again preserving locality.\nWe mentioned earlier that volatile allocators manage operating system–provided \npages but did not explain how they acquire those pages. This will become very important \nlater as we discuss how things change for persistent memory. Memory is usually \nrequested on demand from the operating system either through sbrk(), which moves \nthe break segment of the application, or anonymous mmap(), which creates new virtual \nmemory mapping backed by the page cache. The actual physical memory is usually not \nassigned until the page is written to for the first time. When the allocator decides that it \nno longer needs a page, it can either completely remove the mapping using unmap() or \nit can tell the operating system to release the backing physical pages but keep the virtual \nmapping. This enables the allocator to reuse the same addresses later without having to \nmemory map them again.\nHow does all of this translate into persistent memory allocators and libpmemobj \nspecifically?\nThe persistent heap must be resumable after application restart. This means that \nall state information must be either located on persistent memory or reconstructed on \nstartup. If there are any active bookkeeping processes, those need to be restarted from \nthe point at which they were interrupted. There cannot be any volatile state held in \npersistent memory, such as thread cache pointers. In fact, the allocator must not operate \non any pointers at all because the virtual address of the heap can change between \nrestarts.\nIn libpmemobj, the heap is rebuilt lazily and in stages. The entire available memory \nis divided into equally sized zones (except for the last one, which can be smaller than \nthe others) with metadata at the beginning of each one. Each zone is subsequentially \ndivided into variably sized memory blocks called chunks. Whenever there is an \nallocation request, and the runtime state indicates that there is no memory to satisfy it, \nthe zone’s metadata is processed, and the corresponding runtime state is initialized.  \nThis minimizes the startup time of the pool and amortizes the cost of rebuilding the \nheap state across many individual allocation requests.\nThere are three main reasons for having any runtime state at all. First, access \nlatency of persistent memory can be higher than that of DRAM, potentially impacting \nperformance of data structures placed on it. Second, separating the runtime state from \nthe persistent state enables a workflow where the memory is first reserved in runtime \nstate and initialized, and only then the allocation is reflected on the persistent state.  \nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1623.36 sec"
            },
            {
              "page_number": 351,
              "text": "327\nThis mechanism was described in the previous section. Finally, maintaining fail-safety of \ncomplex persistent data structures is expensive, and keeping them in DRAM allows the \nallocator to sidestep that cost.\nThe runtime allocation scheme employed by libpmemobj is segregated fit with chunk \nreuse and thread caching as described earlier. Free lists in libpmemobj, called buckets, \nare placed in DRAM and are implemented as vectors of pointers to persistent memory \nblocks. The persistent representation of this data structure is a bitmap, located at the \nbeginning of a larger buffer from which the smaller blocks are carved out. These buffers \nin libpmemobj, called runs, are variably sized and are allocated from the previously \nmentioned chunks. Very large allocations are directly allocated as chunks. Figure 16-7 \nshows the libpmemobj implementation.\nPersistent allocators must also ensure consistency in the presence of failures, \notherwise, memory might become unreachable after an ungraceful shutdown of the \napplication. One part of the solution is the API we outlined in the previous section. The \nother part is the careful design of the algorithms inside the allocator that ensures no \nmatter when the application is aborted, the state is consistent. This is also aided by redo \nlogs, which are used to ensure atomicity of groups of noncontiguous persistent metadata \nchanges.\nFigure 16-7.  On-media layout of libpmemobj’s heap\nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page351_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page351_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1624.08 sec"
            },
            {
              "page_number": 352,
              "text": "328\nOne of the most impactful aspects of persistent memory allocation is how the \nmemory is provisioned from the operating system. We previously explained that for \nnormal volatile allocators, the memory is usually acquired through anonymous memory \nmappings that are backed by the page cache. In contrast, persistent heaps must use file-­\nbased memory mappings, backed directly by persistent memory. The difference might \nbe subtle, but it has a significant impact on the way the allocator must be designed. The \nallocator must manage the entire virtual address space, retain information about any \npotential noncontiguous regions of the heap, and avoid excessive overprovisioning of \nvirtual address space. Volatile allocators can rely on the operating system to coalesce \nnoncontiguous physical pages into contiguous virtual ones, whereas persistent allocators \ncannot do the same without explicit and complicated techniques. Additionally, for some \nfile system implementations, the allocator cannot assume that the physical memory is \nallocated at the time of the first page fault, so it must be conservative with internal block \nallocations.\nAnother problem for allocation from file-based mappings is that of perception. \nNormal allocators, due to memory overcommitment, seemingly never run out of memory \nbecause they are allocating the virtual address space, which is effectively infinite. There \nare negative performance consequences of address space bloat, and memory allocators \nactively try to avoid it, but they are not easily measurable in a typical application. In \ncontrast, memory heaps allocate from a finite resource, the persistent memory device, or \na file. This exacerbates the common phenomenon that is heap fragmentation by making \nit trivially measurable, creating the perception that persistent memory allocators are \nless efficient than volatile ones. They can be, but the operating system does a lot of work \nbehind the scene to hide fragmentation of traditional memory allocators.\n\u0007ACID Transactions: Efficient Low-Level Persistent \nTransactions\nThe four components we just described – lanes, redo logs, undo logs, and the \ntransactional memory allocator – form the basis of libpmemobj's implementation of \nACID transactions that we defined in Chapter 4.\nA transaction’s persistent state consists of three logs. First is an undo log, which \ncontains snapshots of user data. Second is an external redo log, which contains \nallocations and deallocations performed by the user. Third is an internal redo log, which \nis used to perform atomic metadata allocations and deallocations. This is technically not \nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1629.40 sec"
            },
            {
              "page_number": 353,
              "text": "329\npart of the transaction but is required to allocate the log extensions if they are needed. \nWithout the internal redo log, it would be impossible to reserve and then publish a new \nlog object in a transaction that already had user-made allocator actions in the external \nredo log.\nAll three logs have individual operation-context instances that are stored in runtime \nstate of the lanes. This state is initialized when the pool is opened, and that is also when \nall the logs of the prior instance of the application are either processed or discarded. \nThere is no special persistent variable that indicates whether past transactions in the log \nwere successful or not. That information is directly derived from checksums stored in \nthe log.\nWhen a transaction begins, and it is not a nested transaction, it acquires a lane, \nwhich must not contain any valid uncomitted logs. The runtime state of the transaction \nis stored in a thread-local variable, and that is where the lane variable is stored once \nacquired.\nTransactional allocator operations use the external redo log and its associated \noperation context to call the appropriate reservation method which in turn creates an \nallocator action to be published at the time of transaction commit. The allocator actions \nare stored in a volatile array. If the transaction is aborted, all the actions are canceled, \nand the associated state is discarded. The complete redo log for memory allocations is \ncreated only at the time of transaction commit. If the library is interrupted while creating \nthe redo log, the next time the pool is opened, the checksum will not match, and the \ntransaction will be aborted by rolling back using the undo log.\nTransactional snapshots use the undo log and its context. The first time a snapshot \nis created, a new memory modification action is created in the external redo log. When \npublished, that action increments the generation number of the associated undo log, \ninvalidating its contents. This guarantees that if the external log is fully written and \nprocessed, it automatically discards the undo log, committing the entire transaction. If \nthe external log is discarded, the undo log is processed, and the transaction is aborted.\nTo ensure that there are never two snapshots of the same memory location (this \nwould be an inefficient use of space), there is a runtime range tree that is queried every \ntime the application wants to create an undo log entry. If the new range overlaps with an \nexisting snapshot, adjustments to the input arguments are made to avoid duplication. \nThe same mechanism is also used to prevent snapshots of newly allocated data. \nWhenever new memory in a transaction is allocated, the reserved memory range is \ninserted into the ranges tree. Snapshotting new objects is redundant because they will be \ndiscarded automatically in the case of an abort.\nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1635.24 sec"
            },
            {
              "page_number": 354,
              "text": "330\nTo ensure that all memory modifications performed inside the transaction are \ndurable on persistent memory once committed, the ranges tree is also used to iterate \nover all snapshots and call the appropriate flushing function on the modified memory \nlocations.\n\u0007Lazy Reinitialization of Variables: Storing \nthe Volatile State on Persistent Memory\nWhile developing software for persistent memory, it is often useful to store the runtime \n(volatile) state inside of persistent memory locations. Keeping that state consistent, \nhowever, is extremely difficult, especially in multithreaded applications.\nThe problem is the initialization of the runtime state. One solution is to simply iterate \nover all objects at the start of the application and initialize the volatile variables then, but \nthat might significantly contribute to startup time of applications with large persistent \npools. The other solution is to lazily reinitialize the variables on access, which is what \nlibpmemobj does for its built-in locks. The library also exposes this mechanism through \nan API for use with custom algorithms.\nLazy reinitialization of the volatile state is implemented using a lock-free algorithm that \nrelies on a generation number stored alongside each volatile variable on persistent memory \nand inside the pool header. The pool header resident copy is increased by two every time \na pool is opened. This means that a valid generation number is always even. When a \nvolatile variable is accessed, its generation number is checked against the one stored in \nthe pool header. If they match, it means that the object can be used and is simply returned \nto the application; otherwise, the object needs to be initialized before returning to ensure \nthe initialization is thread-safe and is performed exactly once in a single instance of the \napplication.\nThe naive implementation could use a double-checked locking, where a thread \nwould try to acquire a lock prior to initialization and verify again if the generation \nnumbers match. If they still do not match, initialize the object, and increase the number. \nTo avoid the overhead that comes with using locks, the actual implementation first \nuses a compare-and-swap to set the generation number to a value that is equal to the \ngeneration number of the pool minus one, which is an odd number that indicates \nan initialization operation is in progress. If this compare-and-swap were to fail, the \nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1640.87 sec"
            },
            {
              "page_number": 355,
              "text": "331\nalgorithm would loop back to check if the generation number matches. If it is successful, \nthe running thread initializes the variable and once again increments the generation \nnumber – this time to an even number that should match the number stored in the pool \nheader.\n\u0007Summary\nThis chapter described the architecture and inner workings of libpmemobj. We \nalso discuss the reasons for the choices that were made during the design and \nimplementation of libpmemobj. With this knowledge, you can accurately reason about \nthe semantics and performance characteristics of code written using this library.\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 16  PMDK Internals: Important Algorithms and Data Structures",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1644.56 sec"
            },
            {
              "page_number": 356,
              "text": "333\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_17\nCHAPTER 17\nReliability, Availability, \nand Serviceability (RAS)\nThis chapter describes the high-level architecture of reliability, availability, and \nserviceability (RAS) features designed for persistent memory. Persistent memory RAS \nfeatures were designed to support the unique error-handling strategy required for an \napplication when persistent memory is used. Error handling is an important part of the \nprogram’s overall reliability, which directly affects the availability of applications. The \nerror-handling strategy for applications impacts what percentage of the expected time \nthe application is available to do its job.\nPersistent memory vendors and platform vendors will both decide which RAS \nfeatures and how they will be implemented at the lowest hardware levels. Some \ncommon RAS features were designed and documented in the ACPI specification, which \nis maintained and owned by the UEFI Forum (https://uefi.org/). In this chapter, \nwe try to attain a general perspective of these ACPI-defined RAS features and call out \nvendor-specific details if warranted.\n\u0007Dealing with Uncorrectable Errors\nThe main memory of a server is protected using error correcting codes (ECC). This is \na common hardware feature that can automatically correct many memory errors that \nhappen due to transient hardware issues, such as power spikes, soft media errors, and so \non. If an error is severe enough, it will corrupt enough bits that ECC cannot correct; the \nresult is called an uncorrectable error (UE).\nUncorrectable errors in persistent memory require special RAS handling that differs \nfrom how a platform may traditionally handle volatile memory uncorrectable errors.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1649.17 sec"
            },
            {
              "page_number": 357,
              "text": "334\nPersistent memory uncorrectable errors are persistent. Unlike volatile memory, if \npower is lost or an application crashes and restarts, the uncorrectable error will remain \non the hardware. This can lead to an application getting stuck in an infinite loop such as\n\t 1.\t Application starts\n\t 2.\t Reads a memory address\n\t 3.\t Encounters uncorrectable error\n\t 4.\t Crashes (or system crashes and reboots)\n\t 5.\t Starts and resumes operation from where it left off\n\t 6.\t Performs a read on the same memory address that triggered the \nprevious restart\n\t 7.\t Crashes (or system crashes and reboots)\n\t 8.\t …\n\t 9.\t Repeats infinitely until manual intervention\nThe operating system and applications may need to address uncorrectable errors in \nthree main ways:\n•\t\nWhen consuming previously undetected uncorrectable errors during \nruntime\n•\t\nWhen unconsumed uncorrectable errors are detected at runtime\n•\t\nWhen mitigating uncorrectable memory locations detected at boot\n\u0007Consumed Uncorrectable Error Handling\nWhen an uncorrectable error is detected on a requested memory address, data \npoisoning is used to inform the CPU that the data requested has an uncorrectable error. \nWhen the hardware detects an uncorrectable memory error, it routes a poison bit along \nwith the data to the CPU. For the Intel architecture, when the CPU detects this poison \nbit, it sends a processor interrupt signal to the operating system to notify it of this error. \nThis signal is called a machine check exception (MCE). The operating system can then \nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1652.85 sec"
            },
            {
              "page_number": 358,
              "text": "335\nexamine the uncorrectable memory error, determine if the software can recover, and \nperform recovery actions via an MCE handler. Typically, uncorrectable errors fall into \nthree categories:\n•\t\nUncorrectable errors that may have corrupted the state of the CPU \nand require a system reset.\n•\t\nUncorrectable errors that can be recovered by software can be \nhandled during runtime.\n•\t\nUncorrectable errors that require no action.\nOperating system vendors handle this uncorrectable error notification in different \nways, but some common elements exist for all of them.\nUsing Linux as an example, when the operating system receives a processor \ninterrupt for an uncorrectable error, it proceeds to offline the page of memory where \nthe uncorrectable error occurred and add the error to a list of areas containing known \nuncorrectable errors. This list of known uncorrectable errors is called the bad block list. \nLinux will also mark the page that contains the uncorrectable error to be cleared when \nthe page is recycled for use by another application.\nThe PMDK libraries automatically check the list of pages with uncorrectable errors in \nthe operating system and prevent an application from opening a persistent memory pool \nif it contains errors. If a page of memory is in use by an application, Linux attempts to kill \nit using the SIGBUS mechanism.\nAt this point, the application developer can decide what to do with this error \nnotification. The simplest way for you to handle uncorrectable errors is to let the \napplication die when it gets a SIGBUS so you do not need to write the complicated logic \nof handling a SIGBUS at runtime. Instead, on restart, the application can use PMDK \nto detect that the persistent memory pool contains errors and repair the data during \napplication initialization. For many applications, this repair can be as simple as reverting \nto a backup error-free copy of the data.\nFigure 17-1 shows a simplified sequence of how Linux can handle an uncorrectable \n(but not fatal) error that was consumed by an application.\nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1659.81 sec"
            },
            {
              "page_number": 359,
              "text": "336\n\u0007Unconsumed Uncorrectable Error Handling\nRAS features are defined to inform software of uncorrectable errors that have been \ndiscovered on the persistent memory media but have not yet been consumed by \nsoftware. The goal of this feature is to allow the operating system to opportunistically \noffline or clear pages with known uncorrectable errors before they can be used by an \napplication. If the address of the uncorrectable error is already in use by an application, \nthe operating system may also choose to notify it of the unconsumed uncorrectable error \nor wait until the application consumes the error. The operating system may choose to \nwait on the chance that the application never tries to access the affected page and later \nreturn the page to the operating system for recycling. At this time, the operating system \nwould clear or offline the uncorrectable error.\nFigure 17-1.  Linux consumed uncorrectable error-handling sequence\nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page359_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page359_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1662.68 sec"
            },
            {
              "page_number": 360,
              "text": "337\nUnconsumed uncorrectable error handling may be implemented differently on \ndifferent vendor platforms, but at the core, there will always be a mechanism to discover \nthe unconsumed uncorrectable error, a mechanism to signal the operating system of an \nunconsumed uncorrectable error, and a mechanism for the operating system to query \ninformation about the unconsumed uncorrectable error. As shown in Figure 17-2, these \nthree mechanisms work together to proactively keep the operating system informed of \nall discovered uncorrectable errors during runtime.\n\u0007Patrol Scrub\nPatrol scrub (also known as memory scrubbing) is a long-standing RAS feature for \nvolatile memory that can also be extended to persistent memory. It is an excellent \nexample of how a platform can discover uncorrectable errors in the background during \nnormal operation.\nPatrol scrubbing is done using a hardware engine, on either the platform or on the \nmemory device, which generates requests to memory addresses on the memory device. \nThe engine generates memory requests at a predefined frequency. Given enough time, \nit will eventually access every memory address. The frequency in which patrol scrub \ngenerates requests produces no noticeable impact on the memory device’s quality of \nservice.\nFigure 17-2.  Unconsumed uncorrectable error handling\nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page360_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page360_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1665.75 sec"
            },
            {
              "page_number": 361,
              "text": "338\nBy generating read requests to memory addresses, the patrol scrubber allows the \nhardware an opportunity to run ECC on a memory address and correct any correctable \nerrors before they can become uncorrectable errors. Optionally, if an uncorrectable \nerror is discovered, the patrol scrubber can trigger a hardware interrupt and notify the \nsoftware layer of its memory address.\n\u0007Unconsumed Uncorrectable Memory-Error Persistent Memory \nRoot-Device Notification\nThe ACPI specification describes a method for hardware to notify software of \nunconsumed uncorrectable errors called the Unconsumed Uncorrectable Memory-­\nError Persistent Memory Root-Device Notification. Using the ACPI-defined framework, \nthe operating system can subscribe to be notified by the platform whenever an \nuncorrectable memory error is detected. It is the platform’s responsibility to receive \nnotification from persistent memory devices that an uncorrectable error has been \ndetected and take appropriate action to generate a persistent memory root-device \nnotification. Upon receipt of root-device notification, the operating system can then use \nexisting ACPI methods, such as Address Range Scrub (ARS), to discover the address of \nthe newly created uncorrectable memory error and take appropriate actions.\n\u0007Address Range Scrub\nARS is a device-specific method (_DSM) defined in the ACPI specification. Privileged \nsoftware can call an ACPI _DSM such as ARS at runtime to retrieve or scan for the \nlocations of uncorrectable memory errors for all persistent memory in the platform. \nBecause ARS is implemented by the platform, each vendor may implement some of the \nfunctionality differently.\nAn ARS accepts a given system address range from the caller and, like patrol scrub, \ninspects each memory address in that range for memory errors. When ARS completes, \nthe caller is given a list of memory addresses in the given range that contains memory \nerrors. Inspection of each memory address may be handled by persistent memory \nhardware or by the platform itself. Unlike a patrol scrub, ARS inspects each memory \naddress at a very high frequency. This increased frequency of the scrub may impact \nthe quality of service for the persistent memory hardware. Thus, ARS can optionally be \ninvoked by the caller to return the results of the previous ARS, sometimes referred to as a \nshort ARS.\nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1673.54 sec"
            },
            {
              "page_number": 362,
              "text": "339\nTraditionally, the operating system executes ARS in one of two ways to obtain the \naddresses of uncorrectable errors after a boot. Either a full scan is executed on all the \navailable persistent memory during system boot or after an unconsumed uncorrectable \nmemory error root-device notification is received. In both instances, the intent is to \ndiscover these addresses before they are consumed by applications.\nOperating systems will compare the list of uncorrectable errors returned by ARS to \ntheir persistent list of uncorrectable errors. If new errors are detected, the list is updated. \nThis list is intended to be consumed by higher-level software, such as the PMDK libraries.\n\u0007Clearing Uncorrectable Errors\nUncorrectable errors for persistent memory will survive power loss and may require special \nhandling to clear corrupted data from the memory address. When an uncorrectable error \nis cleared, the data at the requested memory address is modified, and the error is cleared. \nBecause hardware cannot silently modify application data, clearing uncorrectable errors is \nthe software’s responsibility. Clearing uncorrectable errors is optional, and some operating \nsystems may choose to only offline memory pages that contain memory errors instead of \nrecycling memory pages that contain uncorrectable errors. In some operating systems, \nprivileged applications may have access to clear uncorrectable errors. Nevertheless, an \noperating system is not required to provide this access.\nThe ACPI specification defines a Clear Uncorrectable Error DSM for operating \nsystems to instruct the platform to clear the uncorrectable errors. While persistent \nmemory programming is byte addressable, clearing uncorrectable errors is not. Different \nvendor implementations of persistent memory may specify the alignment and size of the \nmemory unit that is to be cleared by a Clear Uncorrectable Error. Any internal platform \nor operating system list of memory errors should also be updated upon successful \nexecuting of the Clear Uncorrectable Error DSM command.\n\u0007Device Health\nSystem administrators may wish to act and mitigate any device health issues before they \nbegin to affect the availability of applications using persistent memory. To that end, operating \nsystems or management applications will want to discover an accurate picture of persistent \nmemory device health to correctly determine the reliability of the persistent memory.  \nThe ACPI specification defines a few vendor-agnostic health discovery methods, but many \nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1679.17 sec"
            },
            {
              "page_number": 363,
              "text": "340\nvendors choose to implement additional persistent memory device methods for attributes \nthat are not covered by the vendor-agnostic methods. Many of these vendor-specific \nhealth discovery methods are implemented as an ACPI device-­specific method (_DSM). \nApplications should be aware of degradation to the quality of service if they call ACPI \nmethods directly, since some platform implementations may impact memory traffic when \nACPI methods are invoked. Avoid excessive polling of device health methods when possible.\nOn Linux, the ndctl utility can be used to query the device health of persistent \nmemory modules. Listing 17-1 shows an example output of an Intel Optane DC \npersistent memory module.\nListing 17-1.  Using ndctl to query the health of persistent memory modules\n$ sudo ndctl list -DH -d nmem1\n[\n  {\n    \"dev\":\"nmem1\",\n    \"id\":\"8089-a2-1837-00000bb3\",\n    \"handle\":17,\n    \"phys_id\":44,\n    \"security\":\"disabled\",\n    \"health\":{\n      \"health_state\":\"ok\",\n      \"temperature_celsius\":30.0,\n      \"controller_temperature_celsius\":30.0,\n      \"spares_percentage\":100,\n      \"alarm_temperature\":false,\n      \"alarm_controller_temperature\":false,\n      \"alarm_spares\":false,\n      \"alarm_enabled_media_temperature\":false,\n      \"alarm_enabled_ctrl_temperature\":false,\n      \"alarm_enabled_spares\":false,\n      \"shutdown_state\":\"clean\",\n      \"shutdown_count\":1\n    }\n  }\n]\nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1682.85 sec"
            },
            {
              "page_number": 364,
              "text": "341\nConveniently, ndctl also provides a monitoring command and daemon to \ncontinually monitor the health of the systems’ persistent memory modules. For a list of \nall the available options, refer to the ndctl-monitor(1) man page. Examples for using \nthis monitor method include\nExample 1: Run a monitor as a daemon to monitor DIMMs on bus “nfit_test.1,”\n$ sudo ndctl monitor --bus=nfit_test.1 --daemon\nExample 2: Run a monitor as a one-shot command, and output the notifications to  \n/var/log/ndctl.log.\n$ sudo ndctl monitor --log=/var/log/ndctl.log\nExample 3: Run a monitor daemon as a system service.\n$ sudo systemctl start ndctl-monitor.service\nYou can obtain similar information using the persistent memory device-specific \nutility. For example, you can use the ipmctl utility on Linux and Windows∗ to obtain \nhardware-level data similar to that shown by ndctl. Listing 17-2 shows health \ninformation for DIMMID 0x0001 (nmem1 equivalent in ndctl terms).\nListing 17-2.  Health information for DIMMID 0x0001\n$ sudo ipmctl show -sensor -dimm 0x0001\n DimmID | Type                        | CurrentValue\n=====================================================\n 0x0001 | Health                      | Healthy\n 0x0001 | MediaTemperature            | 30C\n 0x0001 | ControllerTemperature       | 31C\n 0x0001 | PercentageRemaining         | 100%\n 0x0001 | LatchedDirtyShutdownCount   | 1\n 0x0001 | PowerOnTime                 | 27311231s\n 0x0001 | UpTime                      | 6231933s\n 0x0001 | PowerCycles                 | 170\n 0x0001 | FwErrorCount                | 8\n 0x0001 | UnlatchedDirtyShutdownCount | 107\nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1687.57 sec"
            },
            {
              "page_number": 365,
              "text": "342\n\u0007ACPI-Defined Health Functions (_NCH, _NBS)\nThe ACPI specification includes two vendor-agnostic methods for operating systems and \nmanagement software to call for determining the health of a persistent memory device.\nGet NVDIMM Current Health Information (_NCH) can be called by the operating \nsystems at boot time to get the current health of the persistent memory device and \ntake appropriate action. The values reported by _NCH can change during runtime and \nshould be monitored for changes. _NCH contains health information that shows if\n•\t\nThe persistent memory requires maintenance\n•\t\nThe persistent memory device performance is degraded\n•\t\nThe operating system can assume write persistency loss on \nsubsequent power events\n•\t\nThe operating system can assume all data will be lost on subsequent \npower events\nGet NVDIMM Boot Status (_NBS) allows operating systems a vendor-agnostic method \nto discover persistent memory health status that does not change during runtime. The \nmost significant attribute reported by _NBS is Data Loss Count (DLC). Data Loss Count is \nexpected to be used by applications and operating systems to help identify the rare case \nwhere a persistent memory dirty shutdown has occurred. See “Unsafe/Dirty Shutdown” \nlater in this chapter for more information on how to properly use this attribute.\n\u0007Vendor-Specific Device Health (_DSMs)\nMany vendors may want to add further health attributes beyond what exists in _NBS \nand _NCH. Vendors are free to design their own ACPI persistent memory device-specific \nmethods (_DSM) to be called by the operating system and privileged applications. \nAlthough vendors implement persistent memory health discovery differently, a few \ncommon health attributes are likely to exist to determine if a persistent memory device \nrequires service. These health attributes may include information such as an overall health \nsummary of the persistent memory, current persistent memory temperature, persistent \nmedia error counts, and total device lifetime utilization. Many operating systems, such as \nLinux, include support to retrieve and report the vendor-unique health statistics through \ntools such as ndctl. The Intel persistent memory _DSM interface document can be found \nunder the “Related Specification” section of https://docs.pmem.io/.\nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1696.27 sec"
            },
            {
              "page_number": 366,
              "text": "343\n\u0007ACPI NFIT Health Event Notification\nDue to the potential loss of quality of service, operating systems and privileged \napplications may not want to actively poll persistent memory devices to retrieve device \nhealth. Thus, the ACPI specification has defined a passive notification method to allow \nthe persistent memory device to notify when a significant change in device health \nhas occurred. Persistent memory device vendors and platform BIOS vendors decide \nwhich device health changes are significant enough to trigger an NVDIMM Firmware \nInterface Table (NFIT) health event notification. Upon receipt of an NFIT health event, \na notification to the operating system is expected to call an _NCH or a _DSM attached to \nthe persistent memory device and take appropriate action based on the data returned.\n\u0007Unsafe/Dirty Shutdown\nAn unsafe or dirty shutdown on persistent memory means that the persistent memory \ndevice power-down sequence or platform power-down sequence may have failed \nto write all in-flight data from the system’s persistence domain to persistent media. \n(Chapter 2 describes persistence domains.) A dirty shutdown is expected to be a very \nrare event, but they can happen due to a variety of reasons such as physical hardware \nissues, power spikes, thermal events, and so on.\nA persistent memory device does not know if any application data was lost as a result \nof the incomplete power-down sequence. It can only detect if a series of events occurred \nin which data may have been lost. In the best-case scenario, there might not have been \nany applications that were in the process of writing data when the dirty shutdown \noccurred.\nThe RAS mechanism described here requires the platform BIOS and persistent \nmemory vendor to maintain a persistent rolling counter that is incremented anytime a \ndirty shutdown is detected. The ACPI specification refers to such a mechanism as the Data \nLoss Count (DLC) that can be returned as part of the Get NVDIMM Boot Status(_NBS) \npersistent memory device method.\nReferring to the output from ndctl in Listing 17-1, the \"shutdown_count\" is reported \nin the health information. Similarly, the output from ipmctl in Listing 17-2 reports \n\"LatchedDirtyShutdownCount\" as the dirty shutdown counter. For both outputs, a value \nof 1 means no issues were detected.\nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1700.88 sec"
            },
            {
              "page_number": 367,
              "text": "344\n\u0007Application Utilization of Data Loss Count (DLC)\nApplications may want to use the DLC counter provided by _NBS to detect if possible \ndata loss occurred while saving data from the system’s persistence domain to the \npersistent media. If such a loss can be detected, applications can perform data recovery \nor rollback using application-specific features.\nThe application’s responsibilities and possible implementation suggestions for \napplications are outlined as follows:\n\t 1.\t Application first creates its initial metadata and stores it in a \npersistent memory file:\n\t\na.\t Application retrieves current DLC via operating system–specific \nmeans for the physical persistent memory that make up the \nlogical volume the applications metadata resides on.\n\t\nb.\t Application calculates the current Logical Data Loss Count \n(LDLC) as the sum of the DLC for all physical persistent memory \nthat make up the logical volume the applications metadata  \nresides on.\n\t\nc.\t Application stores the current LDLC in its metadata file and \nensures that the update of the LDLC has been flushed to the \nsystem’s persistence domain. This is done by using a flush that \nforces the write data all the way to the persistent memory power-\nfail safe domain. (Chapter 2 contains more information about \nflushing data to the persistence domain.)\n\t\nd.\t Application determines GUID or UUID for the logical volume \nthe applications metadata resides on, stores this in its metadata \nfile, and ensures the update of the GUID/UUID to the persistence \ndomain. This is used by the application to later identify if the \nmetadata file has been moved to another logical volume, where \nthe current DLC is no longer valid.\n\t\ne.\t Application creates and sets a “clean” flag in its metadata file and \nensures the update of the clean flag to the persistence domain. \nThis is used by the application to determine if the application was \nactively writing data to persistence during dirty shutdown.\nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1705.59 sec"
            },
            {
              "page_number": 368,
              "text": "345\n\t 2.\t Every time the application runs and retrieves its metadata from \npersistent memory:\n\t\na.\t Application checks the GUID/UUID saved in its metadata with the \ncurrent UUID for the logical volume the applications metadata \nresides on. If they match, then the LDLC is describing the same \nlogical volume the app was using. If they do not match, then the \nDLC is for some other logical volume and no longer applies. The \napplication decides how to handle this.\n\t\nb.\t Application calculates the current LDLC as the sum of the DLC  \nfor all physical persistent memory the application’s metadata \nresides on.\n\t\nc.\t Application compares the current LDLC calculated with the saved \nLDLC retrieved from its metadata.\n\t\nd.\t If the current LDLC does not match the saved LDLC, then one \nor more persistent memory have detected a dirty shutdown and \npossible data loss. If they do match, no further action is required \nby the application.\n\t\ne.\t Application checks the status of the saved “clean” flag in its \nmetadata; if the clean flag is NOT set, this application was writing \nat the time of the shutdown failure.\n\t\nf.\t If the clean flag is NOT set, perform software data recovery or \nrollback using application-specific functionality.\n\t\ng.\t Application stores the new current LDLC in its metadata file \nand ensures that the update of the count has been flushed to the \nsystem’s persistence domain. This may require unsetting the clean \nflag if it was previously set.\n\t\nh.\t Application sets the clean flag in its metadata file and ensures that \nthe update of the clean flag has been flushed to the persistence \ndomain.\nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1710.30 sec"
            },
            {
              "page_number": 369,
              "text": "346\n\t 3.\t Every time the application will write to the file:\n\t\na.\t Before the application writes data, it clears the “clean” flag in its \nmetadata file and ensures that the flag has been flushed to the \npersistence domain.\n\t\nb.\t Application writes data to its persistent memory space.\n\t\nc.\t After the application completes writing data, it sets the “clean” flag \nin its metadata file and ensures the flag has been flushed to the \npersistence domain.\nPMDK libraries make these steps significantly easier and account for interleaving set \nconfigurations.\n\u0007Summary\nThis chapter describes some of the RAS features that are available to persistent memory \ndevices and that are relevant to persistent memory applications. It should have given you \na deeper understanding of uncorrectable errors and how applications can respond to \nthem, how operating systems can detect health status changes to improve the availability \nof applications, and how applications can best detect dirty shutdowns and use the data \nloss counter.\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 17  Reliability, Availability, and Serviceability (RAS)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1715.11 sec"
            },
            {
              "page_number": 370,
              "text": "347\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_18\nCHAPTER 18\nRemote Persistent \nMemory\nThis chapter provides an overview of how persistent memory – and the programming \nconcepts that were introduced in this book – can be used to access persistent memory \nlocated in remote servers connected via a network. A combination of TCP/IP or RDMA \nnetwork hardware and software running on the servers containing persistent memory \nprovide direct remote access to persistent memory.\nHaving remote direct memory access via a high-performance network connection is \na critical use case for most cloud deployments of persistent memory. Typically, in high-­\navailability or highly redundant use cases, data written locally to persistent memory is \nnot considered reliable until it has been replicated to two or more remote persistent \nmemory devices on separate remote servers. We describe this push model design later in \nthis chapter.\nWhile it is certainly possible to use existing TCP/IP networking infrastructures to \nremotely access the persistent memory, this chapter focuses on the use of remote direct \nmemory access (RDMA). Direct memory access (DMA) allows data movement on a \nplatform to be off-loaded to a hardware DMA engine that moves that data on behalf of \nthe CPU, freeing it to do other important tasks during the data move. RDMA applies the \nsame concept and enables data movement between remote servers to occur without the \nCPU on either server having to be directly involved.\nThis chapter’s content and the PMDK librpmem remote persistent memory library \nthat is discussed assume the use of RDMA, but the concepts discussed here can apply to \nother networking interconnects and protocols.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1719.92 sec"
            },
            {
              "page_number": 371,
              "text": "348\nFigure 18-1 outlines a simple remote persistent memory configuration with one \ninitiator system that is replicating writes to persistent memory on a single remote target \nsystem. While this shows the use of persistent memory on both the initiator and target, \nit is possible to read data from initiator DRAM and write to persistent memory on the \nremote target system, or read from the initiator’s persistent memory and write to the \nremote target’s DRAM.\n\u0007RDMA Networking Protocols\nExamples of popular RDMA networking protocols used throughout the cloud and \nenterprise data centers include: \n•\t\nInfiniBand is an I/O architecture and high-performance specification \nfor data transmission between high-speed, low-latency, and highly \nscalable CPUs, processors, and storage.\n•\t\nRoCE (RDMA over Converged Ethernet) is a network protocol that \nallows RDMA over an Ethernet network.\n•\t\niWARP (Internet Wide Area RDMA Protocol) is a networking protocol \nthat implements RDMA for efficient data transfer over Internet \nProtocol networks.\nAll three protocols support high-performance data movement to and from persistent \nmemory using RDMA.\nFigure 18-1.  Initiator and target system using RDMA\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page371_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page371_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1723.10 sec"
            },
            {
              "page_number": 372,
              "text": "349\nThe RDMA protocols are governed by the RDMA Wire Protocol Standards, which are \ndriven by the IBTA (InfiniBand Trade Association) and the IEFT (Internet Engineering \nTask Force) specifications. The IBTA (https://www.infinibandta.org/) governs the \nInfiniBand and RoCE protocols, while the IEFT (https://www.ietf.org/) governs \niWARP.\nLow-latency RDMA networking protocols allow the network interface controller \n(NIC) to control the movement of data between an initiator node source buffer and the \nsink buffer on the target node without needing either node’s CPU to be involved in the \ndata movement. In fact, RDMA Read and RDMA Write operations are often referred \nto as one-sided operations because all of the information required to move the data is \nsupplied by the initiator and the CPU on the target node is not typically interrupted or \neven aware of the data transfer.\nTo perform remote data transfers, information from the target node’s buffers must \nbe passed to the initiator before the remote operation( s) will begin. This requires \nconfiguring the local initiator’s RDMA resources and buffers. Similarly, the remote target \nnode’s RDMA resources that will require CPU resources will need to be initialized and \nreported to the initiator. However, once the resources for the RDMA transfers are set up \nand applications initiate the RDMA request using the CPU, the NIC does the actual data \nmovement on behalf of the RDMA-aware application.\nRDMA-aware applications are responsible for: \n•\t\nInterrogating each NIC on every initiator and target system to \ndetermine supported features\n•\t\nSelecting a NIC for each end of the RDMA point-to-point connection\n•\t\nCreating the connection with the selected NICs, described as an \nRDMA protection domain\n•\t\nAllocating queues for the incoming and outgoing message on each \nNIC and assigning those hardware resources to the protection domain\n•\t\nAllocating DRAM or persistent memory buffers for use with RDMA, \nregistering those buffers with the NIC, and assigning those buffers to \nthe protection domain\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1728.73 sec"
            },
            {
              "page_number": 373,
              "text": "350\nThree basic RDMA commands are used by most RDMA-capable applications and \nlibraries:\nRDMA Write: A one-sided operation where only the initiator supplies all of the \ninformation required for the transfer to occur. This transfer is used to write data to the \nremote target node. The write request contains all source and sink buffer information. \nThe remote target system is not typically interrupted and thus completely unaware of \nthe write operations occurring through the NIC. When the initiator’s NIC sends a write \nto the target, it will generate a “software write completion interrupt.” A software write \ncompletion interrupt means that the write message has been sent to the target NIC \nand is not an indicator of the write completion. Optionally, RDMA Writes can use an \nimmediate option that will interrupt the target node CPU and allow software running \nthere to be immediately notified of the write completion.\nRDMA Read: A one-sided operation where only the initiator supplies all of the \ninformation required for the transfer to occur. This transfer is used to read data from \nthe remote target node. The read request contains all source buffer and target sink \nbuffer information, and the remote target system is not typically interrupted and thus \ncompletely unaware of the read operations occurring through the NIC. The initiator \nsoftware read completion interrupt is an acknowledgment that the read has traversed \nall the way through the initiator’s NIC, over the network, into the target system’s NIC, \nthrough the target internal hardware mesh and memory controllers, to the DRAM or \npersistent memory to retrieve the data. Then it returns all the way back to the initiator \nsoftware that registered for the completion notification.\nRDMA Send (and Receive): The two-sided RDMA Send means that both the \ninitiator and target must supply information for the transfer to complete. This is because \nthe target NIC will be interrupted when the RDMA Send is received by the target NIC \nand requires a hardware receive queue to be set up and pre-populated with completion \nentries before the NIC will receive an RDMA Send transfer operation. Data from the \ninitiator application is bundled in a small, limited sized buffer and sent to the target \nNIC. The target CPU will be interrupted to handle the send operation and any data it \ncontains. If the initiator needs to be notified of receipt of the RDMA Send, or to handle a \nmessage back to the initiator, another RDMA Send operation must be sent in the reverse \ndirection after the initiator has set up its own receive queue and queued completion \nentries to it. The use of the RDMA Send command and the contents of the payload \nare application-specific implementation details. An RDMA Send is typically used for \nbookkeeping and updates of read and write activity between the initiator and the target, \nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1736.72 sec"
            },
            {
              "page_number": 374,
              "text": "351\nsince the target application has no other context of what data movement has taken place. \nFor example, because there is no good way to know when writes have completed on \nthe target, an RDMA Send is often used to notify the target node what is happening. For \nsmall amounts of data, the RDMA Send is very efficient, but it always requires target-­\nside interaction to complete. An RDMA Write with immediate data operation will also \nallow the target node to be interrupted when the write has completed as a different \nmechanism for bookkeeping.\n\u0007Goals of the Initial Remote Persistent Memory \nArchitecture\nThe goal of the first remote persistent memory implementation was based on minimal \nchanges – or ideally, no changes – to the current RDMA hardware and software stacks \nused with volatile memory. From a network hardware, middleware, and software \narchitecture standpoint, writing to remote volatile memory is identical to writing to \nremote persistent memory. The knowledge that a specific memory-mapped file is \nbacked by persistent memory vs. volatile memory is entirely the responsibility of the \napplication to maintain. None of the lower layers in the networking stack are aware of the \nfact that the write is to a persistent memory region or volatile memory. The responsibility \nof knowing which write persistence method to use for a given target connection, and \nmaking those remote writes persistent, falls to the application.\n\u0007Guaranteeing Remote Persistence\nUntil this chapter, much of the book focuses on the use and programming of persistent \nmemory on the local machine. You are now aware of some of the challenges of using \npersistent memory, the persistence domain, and the need to understand and use a \nflushing mechanism to ensure the data is persistent. These same programming concepts \nand challenges apply to remote persistent memory with the additional constraints of \nmaking it work within the existing network protocol and network latency.\nThe SNIA NVM programming model (described in Chapter 3) requires applications \nto flush data that has been written to persistent memory to guarantee that the written \ndata made it into the persistence domain. This same requirement applies to writing to \nremote persistent memory. After the RDMA Write or Send operation has moved the data \nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1741.33 sec"
            },
            {
              "page_number": 375,
              "text": "352\nfrom the initiator node to the persistent memory on the target node, that write or send \ndata needs to be flushed to the persistence domain on the remote system. Alternatively, \nthe remote write or send data needs to bypass CPU caches on the remote node to avoid \nhaving to be flushed.\nDifferent vendor-specific platform features add an extra challenge to RDMA and to \nremote persistent memory. Intel platforms typically use a feature called allocating writes \nor Direct Data IO (DDIO) which allows incoming writes to be placed directly into the \nCPU’s L3 cache. The data is immediately visible to any application wanting to read the \ndata. However, having allocating writes enabled means that RDMA Writes to persistent \nmemory now have to be flushed to the persistence domain on the target node.\nOn Intel platforms, allocating writes can be disabled by turning on non-allocating \nwrite I/O flows which forces the write data to bypass cache and be placed directly into \nthe persistent memory, governed by the location of the RDMA Write sink buffer. This \nwould slow down applications that will immediately touch the newly written data \nbecause they incur the penalty to pull the data into CPU cache. However, this simplifies \nmaking remote writes to persistent memory simpler and faster because cache flushing \non the remote target node can be avoided. An additional complication to using non-­\nallocating write mode on an Intel platform is that an entire PCI root complex must be \nenabled for this write mode. This means that any inbound writes that come through \nthat PCI root complex, for any device connected downstream of it, will have write-data \nbypass CPU caches, causing possible additional performance latency as a side effect.\nIntel specifies two methods for forcing writes to remote persistent memory into the \npersistence domain:\n\t 1.\t A general-purpose remote replication method that does not rely \non Intel non-­allocating write mode and assumes some or all of the \nremote write data will end up in CPU cache on the target system\n\t 2.\t A high-performance appliance remote replication method that \nuses the Intel platform-specific non-allocating write mode and \nis probably more suited to an appliance product where there is \ncomplete control over the hardware configuration to control what \nis connected to which PCI root complex\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1747.26 sec"
            },
            {
              "page_number": 376,
              "text": "353\n\u0007General-Purpose Remote Replication Method\nThe general-purpose remote replication method (GPRRM), also referred to as the \ngeneral-purpose server persistency method (GPSPM), relies on the initiator RDMA \napplication to maintain a list of virtual addresses on the remote target system that have \nbeen written to with previous RDMA Write requests. When all remote writes to persistent \nmemory are issued, the application issues an RDMA Send request from the initiator NIC \nto the target NIC. The RDMA Send request contains a list of virtual starting addresses \nand lengths that the target system will consume when the application software running \non the target node interrupts the system to process the send request. The application \nwalks the list of regions, flushing each cache line in the requested region to the persistent \nmemory using an optimized flush machine instruction (CLWB, CLFLUSHOPT, etc.). When \ncomplete, an SFENCE machine instruction is required to fence those previous writes and \nforce them to complete before handling additional writes. The application on the target \nsystem then issues an RDMA Send request back to interrupt the initiator software of \nthe completed flush operations. This is an indicator to the application that the previous \nwrites were made persistent.\nFigure 18-2 outlines the general-purpose remote replication method sequence of \noperation.\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1750.85 sec"
            },
            {
              "page_number": 377,
              "text": "354\n\u0007How Does the General-Purpose Remote Replication Method \nMake Data Persistent?\nAfter the RDMA Write or any number of writes have been sent, the write data will either \nbe in the L3 CPU cache (due to the default allocating writes) or persistent memory \n(assuming it does not all fit in L3) with potentially some write data still pending in NIC \ninternal buffers. An RDMA Send request, by definition, will force previous writes to \nbe pushed out of the NIC to the target L3 CPU cache and interrupt the target CPU. At \nthis point, all previously issued RDMA Writes to persistent memory are now in L3 or \npersistent memory. The RDMA Send request contains a list of cache lines that the \ninitiator is requesting the target system to flush to its persistence domain. The target \nFigure 18-2.  The general-purpose remote replication method\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page377_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page377_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1753.00 sec"
            },
            {
              "page_number": 378,
              "text": "355\nsystem issues optimized flush instructions to flush each cache line in the list to the \npersistence domain. This is followed by an SFENCE to guarantee these writes complete \nbefore new writes are handled. At this point, the previous writes that were flushed in the \nRDMA Send list are now persistent.\n\u0007Performance Implications of the General-Purpose Remote \nReplication Method\nThe general-purpose remote replication method requires that RDMA of the initiator \nsoftware follows a number of RDMA Write(s) with an RDMA Send. After the target NIC \nfinishes flushing the requested regions, an RDMA Send from the target goes back to \nthe initiator to affirm that the initiator application can consider those writes persistent. \nThis additional send/receive/send/receive messaging has an effect on latency and \nthroughput to make the writes persistent and has 50% higher latency than the appliance \nremote replication method. The extra messaging has an effect on overall bandwidth and \nscalability of all the RDMA connections running on those NICs.\nAlso, if the size of the RDMA Write that needs to be made persistent is small, the \nefficiency of the connection drops dramatically as the extra messaging overhead \nbecomes a significant component of the overall latency. Additionally, the target \nnode CPU and caches are consumed for that operation. The same data is essentially \ntransmitted twice: once from NIC (via PCIe) to the CPU L3 cache and then from the CPU \nL3 cache to the memory controller (iMC).\n\u0007Appliance Remote Replication Method\nUsers of persistent memory on an Intel platform can use non-allocating write flows by \nenabling the feature on the specific PCI root complex where incoming writes from the \nNIC will enter into the CPU’s internal fabric and out to the persistent memory. Using the \nnon-allocating write flow, the incoming RDMA Writes will bypass CPU caches and go \ndirectly to the persistence domain. This means that writes do not need to be flushed to \nthe persistence domain by the target system CPU.\nThe I/O pipeline still needs to be flushed to the persistence domain. This is more \nefficiently accomplished by issuing a small RDMA Read to any memory address on the \nsame RDMA connection as the RDMA Writes; the memory address does not need to \nbe one that was written or is persistent. The RDMA specification clearly states that an \nRDMA Read will force the previous RDMA Writes to complete first. This ordering rule is \nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1759.86 sec"
            },
            {
              "page_number": 379,
              "text": "356\nalso true of the PCIe interconnect to which the target NIC is connected. PCIe Reads will \nperform a pipeline flush and force previous PCIe writes to complete first.\nFigure 18-3 outlines the basic appliance remote replication method, often referred to \nas the appliance persistency method, described earlier.\nFigure 18-3.  The appliance remote replication method\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page379_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page379_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1760.99 sec"
            },
            {
              "page_number": 380,
              "text": "357\n\u0007How Does the Appliance Remote Replication Method Make Data \nPersistent?\nThe combination of bypassing CPU caches on the target system for the inbound RDMA \nWrites to persistent memory with the ordering semantics of the RDMA and PCIe \nprotocols results in an efficient mechanism to make data persistent. Since the RDMA \nRead to persistent memory will force previous writes first to persistent memory and the \npersistence domain, the RDMA Read completion that comes back after those writes are \ncomplete is the initiator application’s acknowledgment that those writes are now durable.\nChapter 2 defines the persistence domain in depth, including how the platform \nensures that all writes get to the media from the persistence domain in the event of a \npower loss.\n\u0007Performance Implications of the Appliance Remote Replication \nMethod\nThis single extra round trip using an RDMA Read is roughly 50% lower latency than the \ngeneral-purpose server persistency method, which requires two round-trip messages \nbefore the writes can be declared durable. As with the first method, as the size of the \nwrites to be made durable gets smaller, the RDMA Read round-trip overhead becomes a \nsignificant component of the overall latency.\n\u0007General Software Architecture\nThe software stack for the use of remote persistent memory typically uses the same \nmemory-mapped files discussed in Chapter 3. Persistent memory is presented to the \nRDMA application as a memory-mapped file. The application registers the persistent \nmemory with the local NIC on both ends of the connection, and the resulting registry \nkey is shared with the initiator application for use in the RDMA Read and Write requests. \nThis is the identical process required for using traditional volatile DRAM with RDMA.\nA layering of kernel and application-level software components is typically used to \nallow an application to make use of both persistent memory and an RDMA connection. \nThe IBTA defines verbs interfaces that are typically implemented by the kernel drivers \nfor the NIC and the middleware software application library. Additional libraries may be \nlayered above the verbs layer to provide generic RDMA services via a common API- and \nNIC-specific provider that implements the library.\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1766.62 sec"
            },
            {
              "page_number": 381,
              "text": "358\nOn Linux, the Open Fabric Alliance (OFA) libibverbs library provides ring-3 interfaces \nto configure and use the RDMA connection for NICs that support IB, RoCE, and iWARP \nRDMA network protocols. The OFA libfabric ring-3 application library can be layered \non top of libibverbs to provide a generic high-level common API that can be used with \ntypical RDMA NICs. This common API requires a provider plug-in to implement the \ncommon API for the specific network protocol. The OFA web site contains many example \napplications and performance tests that can be used on Linux with a variety of RDMA-\ncapable NICs. Those examples provide the backbone of the PMDK librpmem library.\nWindows implements remotely mounted NTFS volumes via the ring-3 SMB Direct \nApplication library, which provides a number of storage protocols including block \nstorage over RDMA.\nFigure 18-4 provides the basic high-level architecture for a typical RDMA application \non Linux, using all of the publically available libraries and interfaces. Notice that a separate \nside-band connection is typically needed to set up the RDMA connections themselves.\n\u0007librpmem Architecture and Its Use in Replication\nPMDK implements both the general-purpose remote replication method and the \nappliance remote replication method in the librpmem library. As of PMDK v1.7, the \nlibrpmem library implements the synchronous and asynchronous replication of local \nwrites to persistent memory on remote systems. librpmem is a low-level library, like \nlibpmem, which allows other libraries to use its replication features.\nFigure 18-4.  General RDMA software architecture\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page381_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page381_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1770.51 sec"
            },
            {
              "page_number": 382,
              "text": "359\nlibpmemobj uses a synchronous write model, meaning that the local initiator write \nand all of the remotely replicated writes must complete before the local write will be \ncompleted back to the application. The libpmemobj library also implements a simple \nactive-passive replication architecture, where all persistent memory transactions are \ndriven through the active initiator node and the remote targets passively standby, \nreplicating the write data. While the passive target systems have the latest write data \nreplicated, the implementation makes no attempt to fail over, fail back, or load balance \nusing the remote systems. The following sections describe the significant performance \ndrawbacks to this implementation.\nlibpmemobj uses the local memory pool configuration information provided in a \nconfiguration file to describe the remote network–connected memory-mapped files. \nA remote rpmemd program installed on each remote target system is started and \nconnected to the librpmem library on the initiator using a secure encrypted socket \nconnection. Through this connection, librpmem, on behalf of libpmemobj, will set up the \nRDMA point-to-point connection with each target system, determine the persistence \nmethod the target supports (general purpose or appliance method), allocate remote \nmemory-mapped persistent memory files, register the persistent memory on the remote \nNIC, and retrieve the resulting memory keys for the registered memory.\nOnce all the RDMA connections to all the targets are established, all required \nqueues are instantiated, and memory buffers have all been allocated and registered, \nthe libpmemobj library is ready to begin remotely replicating all application write \ndata to its local memory-mapped file. When the application calls pmemobj_persist() \nin libpmemobj, the library will generate a corresponding rpmem_persist() call \ninto librpmem which, in turn, calls the libfabric fi_write() to do the RDMA Write. \nlibrpmem then initiates the RDMA Read or Send persistence method (as governed \nby an understanding of the currently enabled target node’s current configuration) by \ncalling libfabric fi_read() or fi_send(). RDMA Read is used in the appliance remote \nreplication method, and RDMA Send is used in the general-purpose remote replication \nmethod.\nFigure 18-5 outlines the high-level components and interfaces described earlier and \nused by both the initiator and remote target system using librpmem and libpmemobj.\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1777.58 sec"
            },
            {
              "page_number": 383,
              "text": "360\nThe major components (shown in Figure 18-5) are described in the following to \nhelp you understand the high-level architecture that is used by the PMDK’s remote \nreplication feature:\nlibrpmem – PMDK Remote RDMA Access Library: The \ncontainer for the initiator node for all the initiator PMDK \nfunctionality that is related to remote replication using RDMA.\nrpmemd – PMDK Remote RDMA Configuration Daemon: The \ncontainer for the target node for all the target PMDK functionality \nthat is related to remote replication using RDMA. It will block any \nlocal access to the pmempool set that has been configured for \nremote usage and executes the remote target interrupt handlers \nrequired for the general-purpose remote replication method.\nInitiator and Target SSH: This component is used by both \nlibrpmem and rpmemd libraries to set up a simple socket \nconnection, close a previously opened socket connection, and \nsend communication packets back and forth.\nLibfabric: The OFA defined high-level ring-3 application API for \nsetting up and using a fabric connection in a fabric and vendor-­\nagnostic way. This high-level interface supports RoCE, InfiniBand, \nand iWARP, as well Intel Omni-Path Architecture products \nand other network protocols using libfabric-specific transport \nproviders.\nFigure 18-5.  RDMA architecture using libpmemobj and librpmem\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page383_img1.jpeg"
              ],
              "img_summary_files": [],
              "img_vision_files": [
                "output\\images\\img_vision\\Programming_Persistent_Memory_medium_457_page383_img1_vision.json"
              ],
              "summary": "Summary not available due to an error.",
              "time_taken": "1781.16 sec"
            },
            {
              "page_number": 384,
              "text": "361\nLibibverbs: The OFA defined high-level RDMA fabric-based \ninterface. This high-level interface supports RoCE, InfiniBand, \nand iWARP and is commonly used in most Linux distributions.\nTarget Node Platform Configuration File: Simple text file \ngenerated by the IT admin or user to describe the platform \ncapabilities of the remote target node. This file describes specific \ncapabilities that affect what durability method can be used, that \nis, ADR-enabled platform, non-allocating write flows enabled by \nthe NIC, and platform type. It also specifies the default socket-­\nconnection port that rpmemd will listen on.\nInitiator Node PMDK pmempool Set Configuration File: An \nexisting persistent memory poolset configuration file is generated \nby the system or application administrator that describes local \nsets of files that will be treated as a pool of persistent memory on \nthe local platform. It also describes local files for local replication \nand remote target hostnames for remote replication.\nTarget Node PMDK pmempool Set Configuration File: An \nexisting persistent memory poolset configuration file is generated \nby the system or application administrator that describes local \nsets of files that will be treated as a pool of persistent memory on \nthe local platform. On the target node, this set is the collection of \nfiles that the initiator node is replicating data into.\nInitiator and Target Node Operating System syslog: The \nstandard Linux syslog on each node used by librpmem and \nrpmemd for outputting useful data for both debug and non-debug \ninformation. Since there is little information from rpmemd that \nis visible on the initiator system, extensive information will be \noutput to the target system syslog when rpmemd is started with the \n\"-d\" (debug) runtime option. Even without the debug enabled, \nrpmemd will output socket events like open, close, create, lost \nconnection, and similar RDMA events.\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1786.89 sec"
            },
            {
              "page_number": 385,
              "text": "362\n\u0007Configuring Remote Replication Using Poolsets\nYou are probably already familiar with using poolsets (introduced in Chapter 7) libpmemobj \nto initialize remote replication, which requires two such poolset files. The file used on the \ninitiator side by the libpmemobj-enabled application must describe the local memory pool \nand point to poolset configuration file on the target node, whereas the poolset file on the \ntarget node must describe the memory pool shared by the target system.\nListing 18-1 shows a poolset file that will allow replicating local writes to the \n“remotepool.set” on a remote host.\nListing 18-1.  poolwithremotereplica.set – An example of replicating local data to \na remote host\nPMEMPOOLSET\n256G /mnt/pmem0/pool1\nREPLICA user@example.com remotepool.set\nListing 18-2 shows a poolset file that describes the memory-mapped files shared for \nthe remote access. In many ways, a remote poolset file is the same as the regular poolset \nfile, but it must fulfill additional requirements:\n•\t\nExist in a poolset directory specified in the rpmemd configuration file\n•\t\nShould be uniquely identified by its name, which an rpmem-enabled \napplication has to use to replicate to the specified memory pool\n•\t\nCannot define any additional replicas, local or remote\nListing 18-2.  remotereplica.set – An example of how to describe the memory \npool on the remote host\nPMEMPOOLSET\n256G /mnt/pmem1/pool2\n\u0007Performance Considerations\nOnce persistent memory is accessible via a remote network connection, significantly \nlower latency can be achieved compared with writing to a remote SSD or legacy block \nstorage device. This is because the RDMA hardware is writing the remote write data \nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1790.48 sec"
            },
            {
              "page_number": 386,
              "text": "363\ndirectly into the final persistent memory location, whereas remote replication to an SSD \nrequires an RDMA Write into the DRAM on the remote server, followed by a second local \nDMA operation to move the remote write data from volatile DRAM into the final storage \nlocation on the SSD or other legacy block storage device.\nThe performance challenge with replicating data to remote persistent memory is that \nwhile large block sizes of 512KiB or larger can achieve good performance, as the size of \nthe writes being replicated gets smaller, the network overhead becomes a larger portion \nof the total latency, and performance can suffer.\nIf the persistent memory is being used as an SSD replacement, the typical native \nblock storage size is 4K, avoiding some of the inefficiencies seen with small transfers. \nIf the persistent memory replaces a traditional SSD and data is written remotely to the \nSSD, the latency improvements with persistent memory can be 10x or more.\nThe synchronous replication model implemented in librpmem means that small \ndata structures and pointer updates in local persistent memory result in small, very \ninefficient, RDMA Writes followed by a small RDMA Read or Send to make that small \namount of write data persistent. This results in significant performance degradation \ncompared to writing only to local persistent memory. It makes the replication \nperformance very dependent on the local persistent memory write sequences, which \nis heavily dependent on the application workload. In general, the larger the average \nrequest size and the lower the number of rpmem_persist() calls that are required for a \ngiven workload will improve the overall latency required for guaranteeing that data is \npersistent.\nIt is possible to follow multiple RDMA Writes with single RDMA Read or Send \nto make all of the preceding writes persistent. This reduces the impact of the size of \nRDMA Writes on the overall performance of the proposed solution. But using this \nmitigation, remember you are not guaranteed that any of the RDMA Writes is persistent \nuntil RDMA Read completion returns or you receive RDMA Send with a confirmation. \nThe implementation that allows this approach is implemented in rpmem_flush() and \nrpmem_drain() API call pair, where rpmem_flush() performs RDMA Write and returns \nimmediately and rpmem_drain() posts RDMA Read and waits for its completion (at the \ntime of publication it is not implemented in the write/send model).\nThere are many performance considerations, including the high-level networking \nmodel being used. Traditional best-in-class networking architecture typically relies \non a pull model between the initiator and target. In a pull model, the initiator requests \nresources from the target, but the target server only pulls the data across via RDMA \nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1798.44 sec"
            },
            {
              "page_number": 387,
              "text": "364\nRead when it has the resources and connection bandwidth. This server-centric view \nallows the target node to handle hundreds or thousands of connections since it is in \ncomplete control of all resources for all of the connections and initiates the networking \ntransactions when it chooses. With the speed and low latency of persistent memory, a \npush model can be used where the initiator and target have pre-allocated and registered \nmemory resources and directly RDMA Write the data without waiting for server-side \nresource coordination. Microsoft’s SNIA DevCon RDMA presentation describes the \npush/pull model in more detail: (https://www.snia.org/sites/default/files/\nSDC/2018/presentations/PM/Talpey_Tom_Remote_Persistent_Memory.pdf).\n\u0007Remote Replication Error Handling\nlibrpmem replication failures will occur for either a lost socket connection or a lost \nRDMA connection. Any error status returned from rpmem_persist(), rpmem_flush(), \nand rpmem_drain() is typically treated as an unrecoverable failure. The libpmemobj \nuser of librpmem API should treat this as a lost socket or RDMA condition and should \nwait for all remaining librpmem API calls to complete, call rpmem_close() to close the \nconnection and clean up the stack, and then force the application to exit. When the \napplication restarts, the files will be reopened on both ends, and libpmemobj will check \nonly the file metadata. We recommend you do not proceed before synchronizing local \nand remote memory pools with the pmempool-sync(1) command.\n\u0007Say Hello to the Replicated World\nThe beauty of the libpmemobj remote replication is that it does not require any changes \nto the existing libpmemobj application. If you take any libpmemobj application and \nprovide it with the poolset file configured to use the remote replica, it will simply start \nreplicating. No coding required.\nTo illustrate how to replicate persistent memory, we look at a Hello World type \nprogram demonstrating the replication process directly using the librpmem library. \nListing 18-3 shows a part of the C program that writes the “Hello world” message to \nremote memory. If it discovers that the message in English is already there, it translates \nit to Spanish and writes it back to remote memory. We walk through the lines of the \nprogram at the end of the listing.\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1804.10 sec"
            },
            {
              "page_number": 388,
              "text": "365\nListing 18-3.  The main routine of the Hello World program with replication\n    37    #include <assert.h>\n    38    #include <errno.h>\n    39    #include <unistd.h>\n    40    #include <stdio.h>\n    41    #include <stdlib.h>\n    42    #include <string.h>\n    43\n    44    #include <librpmem.h>\n    45\n    46    /*\n    47     * English and Spanish translation of the message\n    48     */\n    49    enum lang_t {en, es};\n    50    static const char *hello_str[] = {\n    51        [en] = \"Hello world!\",\n    52        [es] = \"¡Hola Mundo!\"\n    53    };\n    54\n    55    /*\n    56     * structure to store the current message\n    57     */\n    58    #define STR_SIZE    100\n    59    struct hello_t {\n    60        enum lang_t lang;\n    61        char str[STR_SIZE];\n    62    };\n    63\n    64    /*\n    65     * write_hello_str -- write a message to the local memory\n    66     */\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1807.68 sec"
            },
            {
              "page_number": 389,
              "text": "366\n    67    static inline void\n    68    write_hello_str(struct hello_t *hello, enum lang_t lang)\n    69    {\n    70        hello->lang = lang;\n    71        strncpy(hello->str, hello_str[hello->lang], STR_SIZE);\n    72    }\n   104    int\n   105    main(int argc, char *argv[])\n   106    {\n   107        /* for this example, assume 32MiB pool */\n   108        size_t pool_size = 32 * 1024 * 1024;\n   109        void *pool = NULL;\n   110        int created;\n   111\n   112        /* allocate a page size aligned local memory pool */\n   113        long pagesize = sysconf(_SC_PAGESIZE);\n   114        assert(pagesize >= 0);\n   115        int ret = posix_memalign(&pool, pagesize, pool_size);\n   116        assert(ret == 0 && pool != NULL);\n   117\n   118        /* skip to the beginning of the message */\n   119        size_t hello_off = 4096; /* rpmem header size */\n   120        struct hello_t *hello = (struct hello_t *)(pool + hello_off);\n   121\n   122        \u0007RPMEMpool *rpp = remote_open(\"target\", \"pool.set\", pool, \npool_size,\n   123                &created);\n   124        if (created) {\n   125            /* reset local memory pool */\n   126            memset(pool, 0, pool_size);\n   127            write_hello_str(hello, en);\n   128        } else {\n   129            /* read message from the remote pool */\n   130            ret = rpmem_read(rpp, hello, hello_off, sizeof(*hello), 0);\n   131            assert(ret == 0);\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1813.31 sec"
            },
            {
              "page_number": 390,
              "text": "367\n   132\n   133            /* translate the message */\n   134            \u0007const int lang_num = (sizeof(hello_str) / sizeof(hello_\nstr[0]));\n   135            \u0007enum lang_t lang = (enum lang_t)((hello->lang + 1) % \nlang_num);\n   136            write_hello_str(hello, lang);\n   137        }\n   138\n   139        /* write message to the remote pool */\n   140        ret = rpmem_persist(rpp, hello_off, sizeof(*hello), 0, 0);\n   141        printf(\"%s\\n\", hello->str);\n   142        assert(ret == 0);\n   143\n   144        /* close the remote pool */\n   145        ret = rpmem_close(rpp);\n   146        assert(ret == 0);\n   147\n   148        /* release local memory pool */\n   149        free(pool);\n   150        return 0;\n   151    }\n•\t\nLine 68: Simple helper routine for writing message to the local memory.\n•\t\nLine 115: Allocate a big enough block of memory, which is aligned \nto the page size. The required block size is hard-coded, whereas \nthe alignment is required if you want to make this memory block \navailable for RDMA transfers.\n•\t\nLine 122: The remote_open() routine creates or opens the remote \nmemory pool.\n•\t\nLines 126-127: The local memory pool is initialized here. It is \nperformed only once when the remote memory pool was just \ncreated, so it does not contain any message.\n•\t\nLine 130: A message from the remote memory pool is read to the \nlocal memory here.\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1817.92 sec"
            },
            {
              "page_number": 391,
              "text": "368\n•\t\nLines 134-136: If a message from the remote memory pool was read \ncorrectly, it is translated locally.\n•\t\nLine 140: The newly initialized or translated message is written to the \nremote memory pool.\n•\t\nLine 145: Close the remote memory pool.\n•\t\nLine 149: Release remote memory pool.\nThe last missing piece of the whole process is how the remote replication is set up. It \nis all done in the remote_open() routine presented in Listing 18-4.\nListing 18-4.  A remote_open routine from the Hello World program with \nreplication\n    74    /*\n    75     * remote_open -- setup the librpmem replication\n    76     */\n    77    static inline RPMEMpool*\n    78    remote_open(const char *target, const char *poolset, void *pool,\n    79            size_t pool_size, int *created)\n    80    {\n    81        /* fill pool_attributes */\n    82        struct rpmem_pool_attr pool_attr;\n    83        memset(&pool_attr, 0, sizeof(pool_attr));\n    84        strncpy(pool_attr.signature, \"HELLO\", RPMEM_POOL_HDR_SIG_LEN);\n    85\n    86        /* create a remote pool */\n    87        unsigned nlanes = 1;\n    88        \u0007RPMEMpool *rpp = rpmem_create(target, poolset, pool, pool_\nsize, &nlanes,\n    89                &pool_attr);\n    90        if (rpp) {\n    91            *created = 1;\n    92            return rpp;\n    93        }\n    94\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1822.73 sec"
            },
            {
              "page_number": 392,
              "text": "369\n    95        /* create failed so open a remote pool */\n    96        assert(errno == EEXIST);\n    97        \u0007rpp = rpmem_open(target, poolset, pool, pool_size, &nlanes, \n&pool_attr);\n    98        assert(rpp != NULL);\n    99        *created = 0;\n   100\n   101        return rpp;\n   102    }\n•\t\nLine 88: A remote memory pool can be either created or opened. \nWhen it is used for the first time, it must be created so that it is \navailable for the opening afterward. We first try to create it here.\n•\t\nLine 97: Here we attempt to open the remote memory pool. We \nassume it exists because of the error code received during the create \ntry (EEXIST).\n\u0007Execution Example\nThe Hello World application produces the output shown in Listing 18-5.\nListing 18-5.  An output from the Hello World application for librpmem\n[user@initiator]$ ./hello\nHello world!\n[user@initiator]$ ./hello\n¡Hola Mundo!\nListing 18-6 shows the contents of the target persistent memory pool where we see \nthe “Hola Mundo” string.\nListing 18-6.  The ¡Hola Mundo! snooped on the replication target\n[user@target]$ hexdump –s 4096 –C /mnt/pmem1/pool2\n00001000  01 00 00 00 c2 a1 48 6f  6c 61 20 4d 75 6e 64 6f  |......Hola \nMundo|\n00001010  21 00 00 00 00 00 00 00  00 00 00 00 00 00 00 \n00  |!...............|\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1827.65 sec"
            },
            {
              "page_number": 393,
              "text": "370\n00001020  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 \n00  |................|\n*\n00002000\n\u0007Summary\nIt is important to know that neither the general-purpose remote replication method \nnor the appliance remote replication method is ideal because vendor-specific platform \nfeatures are required to use non-allocating writes, adding the complication of effecting \nperformance on an entire PCI root complex. Conversely, flushing remote writes using \nallocating writes requires a painful interrupt of the target system to intercept an RDMA \nSend request and flush the list of regions contained within the send buffer. Waking the \nremote node is extremely painful in a cloud environment because there are hundreds \nor thousands of inbound RDMA requests from many different connections; avoid this if \npossible.\nThere are cloud service providers using these two methods today and getting \nphenomenal performance results. If the persistent memory is used as a replacement for \na remotely accessed SSD, huge reductions in latency can be achieved.\nAs the first iteration of remote persistence support, we focused on application/\nlibrary changes to implement these high-level persistence methods, without hardware, \nfirmware, driver, or protocol changes. At the time of publication, IBTA and IETF drafts \nfor a new wire protocol extension for persistent memory is nearing completion. This will \nprovide native hardware support for RDMA to persistent memory and allow hardware \nentities to route each I/ O to its destination memory device without the need to change \nallocating write mode and without the potential to adversely affect performance on \ncollateral devices connected to the same root port. See Appendix E for more details on \nthe new extensions to RDMA, specifically for remote persistence.\nRDMA protocol extensions are only one step into further remote persistent memory \ndevelopment. Several other areas of improvement are already identified and shall be \naddressed to the remote persistent memory users community, including atomicity of \nremote operations, advanced error handling (including RAS), dynamic configuration of \nremote persistent memory and custom setup, and real 0% CPU utilization on remote/\ntarget replication side.\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1834.71 sec"
            },
            {
              "page_number": 394,
              "text": "371\nAs this book has demonstrated, unlocking the true potential of persistent memory \nmay require new approaches to existing software and application architecture. \nHopefully, this chapter gave you an overview of this complex topic, the challenges of \nworking with remote persistent memory, and the many aspects of software architecture \nto consider when unlocking the true performance potential.\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 18  Remote Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1837.28 sec"
            },
            {
              "page_number": 395,
              "text": "373\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1_19\nCHAPTER 19\nAdvanced Topics\nThis chapter covers several topics that we briefly described earlier in the book but did \nnot expand upon as it would have distracted from the focus points. The in-depth details \non these topics are here for your reference.\n\u0007Nonuniform Memory Access (NUMA)\nNUMA is a computer memory design used in multiprocessing where the memory \naccess time depends on the memory location relative to the processor. NUMA is used \nin a symmetric multiprocessing (SMP) system. An SMP system is a “tightly coupled and \nshare everything” system in which multiple processors working under a single operating \nsystem can access each other’s memory over a common bus or “interconnect” path. \nWith NUMA, a processor can access its own local memory faster than nonlocal memory \n(memory that is local to another processor or memory shared between processors). The \nbenefits of NUMA are limited to particular workloads, notably on servers where the data \nis often associated strongly with certain tasks or users.\nCPU memory access is always fastest when the CPU can access its local memory. \nTypically, the CPU socket and the closest memory banks define a NUMA node. Whenever \na CPU needs to access the memory of another NUMA node, it cannot access it directly \nbut is required to access it through the CPU owning the memory. Figure ­19-­1 shows a \ntwo-socket system with DRAM and persistent memory represented as “memory.”",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1840.96 sec"
            },
            {
              "page_number": 396,
              "text": "374\nOn a NUMA system, the greater the distance between a processor and a memory \nbank, the slower the processor’s access to that memory bank. Performance-sensitive \napplications should therefore be configured so they allocate memory from the closest \npossible memory bank.\nPerformance-sensitive applications should also be configured to execute on a set \nnumber of cores, particularly in the case of multithreaded applications. Because first-­\nlevel caches are usually small, if multiple threads execute on one core, each thread \nwill potentially evict cached data accessed by a previous thread. When the operating \nsystem attempts to multitask between these threads, and the threads continue to evict \neach other’s cached data, a large percentage of their execution time is spent on cache \nline replacement. This issue is referred to as cache thrashing. We therefore recommend \nthat you bind a multithreaded application to a NUMA node rather than a single core, \nsince this allows the threads to share cache lines on multiple levels (first-, second-, and \nlast-level cache) and minimizes the need for cache fill operations. However, binding \nan application to a single core may be performant if all threads are accessing the \nsame cached data. numactl allows you to bind an application to a particular core or \nNUMA node and to allocate the memory associated with a core or set of cores to that \napplication.\n\u0007NUMACTL Linux Utility\nOn Linux we can use the numactl utility to display the NUMA hardware configuration \nand control which cores and threads application processes can run. The libnuma library \nincluded in the numactl package offers a simple programming interface to the NUMA \npolicy supported by the kernel. It is useful for more fine-grained tuning than the numactl \nutility. Further information is available in the numa(7) man page.\nFigure 19-1.  A two-socket CPU NUMA architecture showing local and remote \nmemory access\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page396_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page396_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1846.90 sec"
            },
            {
              "page_number": 397,
              "text": "375\nThe numactl --hardware command displays an inventory of the available NUMA \nnodes within the system. The output shows only volatile memory, not persistent \nmemory. We will show how to use the ndctl command to show NUMA locality of \npersistent memory in the next section. The number of NUMA nodes does not always \nequal the number of sockets. For example, an AMD Threadripper 1950X has 1 socket \nand 2 NUMA nodes. The following output from numactl was collected from a two-socket \nIntel Xeon Platinum 8260L processor server with a total of 385GiB DDR4, 196GiB per \nsocket.\n# numactl --hardware\navailable: 2 nodes (0-1)\nnode 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\nnode 0 size: 192129 MB\nnode 0 free: 187094 MB\nnode 1 cpus: 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 \n45 46 47 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 \n94 95\nnode 1 size: 192013 MB\nnode 1 free: 191478 MB\nnode distances:\nnode   0   1\n  0:  10  21\n  1:  21  10\nThe node distance is a relative distance and not an actual time-based latency in \nnanoseconds or milliseconds.\nnumactl lets you bind an application to a particular core or NUMA node and allocate \nthe memory associated with a core or set of cores to that application. Some useful \noptions provided by numactl are described in Table 19-1.\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1850.59 sec"
            },
            {
              "page_number": 398,
              "text": "376\n\u0007NDCTL Linux Utility\nThe ndctl utility is used to create persistent memory capacity for the operating system, \ncalled namespaces, as well as enumerating, enabling, and disabling the dimms, \nregions, and namespaces. Using the –v (verbose) option shows what NUMA node \n(numa_node) persistent memory DIMMS (-D), regions (-R), and namespaces (-N) \nbelong to. Listing 19-1 shows the region and namespaces for a two-socket system. We \ncan correlate the numa_node with the corresponding NUMA node shown by the numactl \ncommand.\nListing 19-1.  Region and namespaces for a two-socket system\n# ndctl list -Rv\n{\n  \"regions\":[\n    {\n      \"dev\":\"region1\",\n      \"size\":1623497637888,\n      \"available_size\":0,\n      \"max_available_extent\":0,\n      \"type\":\"pmem\",\n      \"numa_node\":1,\nTable 19-1.  numactl command options for binding processes to NUMA nodes or \nCPUs\nOption\nDescription\n--membind, -m\nOnly allocate memory from specific NUMA nodes. The allocation will fail \nif there is not enough memory available on these nodes.\n--cpunodebind, -N\nOnly execute the process on CPUs from the specified NUMA nodes.\n--physcpubind, -C\nOnly execute process on the given CPUs.\n--localalloc, -l\nAlways allocate on the current NUMA node.\n--preferred\nPreferably allocate memory on the specified NUMA node. If memory \ncannot be allocated, fall back to other nodes.\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1856.32 sec"
            },
            {
              "page_number": 399,
              "text": "377\n      \"iset_id\":-2506113243053544244,\n      \"persistence_domain\":\"memory_controller\",\n      \"namespaces\":[\n        {\n          \"dev\":\"namespace1.0\",\n          \"mode\":\"fsdax\",\n          \"map\":\"dev\",\n          \"size\":1598128390144,\n          \"uuid\":\"b3e203a0-2b3f-4e27-9837-a88803f71860\",\n          \"raw_uuid\":\"bd8abb69-dd9b-44b7-959f-79e8cf964941\",\n          \"sector_size\":512,\n          \"align\":2097152,\n          \"blockdev\":\"pmem1\",\n          \"numa_node\":1\n        }\n      ]\n    },\n    {\n      \"dev\":\"region0\",\n      \"size\":1623497637888,\n      \"available_size\":0,\n      \"max_available_extent\":0,\n      \"type\":\"pmem\",\n      \"numa_node\":0,\n      \"iset_id\":3259620181632232652,\n      \"persistence_domain\":\"memory_controller\",\n      \"namespaces\":[\n        {\n          \"dev\":\"namespace0.0\",\n          \"mode\":\"fsdax\",\n          \"map\":\"dev\",\n          \"size\":1598128390144,\n          \"uuid\":\"06b8536d-4713-487d-891d-795956d94cc9\",\n          \"raw_uuid\":\"39f4abba-5ca7-445b-ad99-fd777f7923c1\",\n          \"sector_size\":512,\n          \"align\":2097152,\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1861.13 sec"
            },
            {
              "page_number": 400,
              "text": "378\n          \"blockdev\":\"pmem0\",\n          \"numa_node\":0\n        }\n      ]\n    }\n  ]\n}\n\u0007Intel Memory Latency Checker Utility\nTo get absolute latency numbers between NUMA nodes on Intel systems, you can use \nthe Intel Memory Latency Checker (Intel MLC), available from https://software.\nintel.com/en-us/articles/intel-memory-latency-checker.\nIntel MLC provides several modes specified through command-line arguments:\n•\t\n--latency_matrix prints a matrix of local and cross-socket memory \nlatencies.\n•\t\n--bandwidth_matrix prints a matrix of local and cross-socket \nmemory bandwidths.\n•\t\n--peak_injection_bandwidth prints peak memory bandwidths of \nthe platform for various read-write ratios.\n•\t\n--idle_latency prints the idle memory latency of the platform.\n•\t\n--loaded_latency prints the loaded memory latency of the platform.\n•\t\n--c2c_latency prints the cache-to-cache data transfer latency of the \nplatform.\nExecuting mlc or mlc_avx512 with no arguments runs all the modes in sequence \nusing the default parameters and values for each test and writes the results to the \nterminal. The following example shows running just the latency matrix on a two-socket \nIntel system.\n# ./mlc_avx512 --latency_matrix -e -r\nIntel(R) Memory Latency Checker - v3.6\nCommand line parameters: --latency_matrix -e -r\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1865.74 sec"
            },
            {
              "page_number": 401,
              "text": "379\nUsing buffer size of 2000.000MiB\nMeasuring idle latencies (in ns)...\n                Numa node\nNuma node            0       1\n       0          84.2   141.4\n       1         141.5    82.4\n•\t\n--latency_matrix prints a matrix of local and cross-socket memory \nlatencies.\n•\t\n-e means that the hardware prefetcher states do not get modified.\n•\t\n-r is random access reads for latency thread.\nMLC can be used to test persistent memory latency and bandwidth in either DAX or \nFSDAX modes. Commonly used arguments include\n•\t\n-L requests that large pages (2MB) be used (assuming they have been \nenabled).\n•\t\n-h requests huge pages (1GB) for DAX file mapping.\n•\t\n-J specifies a directory in which files for mmap will be created (by \ndefault no files are created). This option is mutually exclusive with –j.\n•\t\n-P CLFLUSH is used to evict stores to persistent memory.\nExamples:\nSequential read latency:\n# mlc_avx512 --idle_latency –J/mnt/pmemfs\nRandom read latency:\n# mlc_avx512 --idle_latency -l256 –J/mnt/pmemfs\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1869.33 sec"
            },
            {
              "page_number": 402,
              "text": "380\n\u0007NUMASTAT Utility\nThe numastat utility on Linux shows per NUMA node memory statistics for processors \nand the operating system. With no command options or arguments, it displays NUMA \nhit and miss system statistics from the kernel memory allocator. The default numastat \nstatistics shows per-node numbers, in units of pages of memory, for example:\n$ sudo numastat\n                           node0           node1\nnuma_hit                 8718076         7881244\nnuma_miss                      0               0\nnuma_foreign                   0               0\ninterleave_hit             40135           40160\nlocal_node               8642532         2806430\nother_node                 75544         5074814\n•\t\nnuma_hit is memory successfully allocated on this node as intended.\n•\t\nnuma_miss is memory allocated on this node despite the process \npreferring some different node. Each numa_miss has a numa_foreign \non another node.\n•\t\nnuma_foreign is memory intended for this node but is actually \nallocated on a different node. Each numa_foreign has a numa_miss on \nanother node.\n•\t\ninterleave_hit is interleaved memory successfully allocated on this \nnode as intended.\n•\t\nlocal_node is memory allocated on this node while a process was \nrunning on it.\n•\t\nother_node is memory allocated on this node while a process was \nrunning on another node.\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1874.24 sec"
            },
            {
              "page_number": 403,
              "text": "381\n\u0007Intel VTune Profiler – Platform Profiler\nOn Intel systems, you can use the Intel VTune Profiler - Platform Profiler, previously \ncalled VTune Amplifier, (discussed in Chapter 15) to show CPU and memory statistics, \nincluding hit and miss rates of CPU caches and data accesses to DDR and persistent \nmemory. It can also depict the system’s configuration to show what memory devices are \nphysically located on which CPU.\n\u0007IPMCTL Utility\nPersistent memory vendor- and server-specific utilities can also be used to show DDR \nand persistent memory device topology to help identify what devices are associated \nwith which CPU sockets. For example, the ipmctl show –topology command displays \nthe DDR and persistent memory (non-volatile) devices with their physical memory slot \nlocation (see Figure 19-2), if that data is available.\n'LPP,'\u0003_\u00030HPRU\\7\\SH\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003&DSDFLW\\\u0003\u0003_\u00033K\\VLFDO,'_\u0003'HYLFH/RFDW\n                                                                            \n\u0013[\u0013\u0013\u0013\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0015\u001b\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B$\u0015\n\u0013[\u0013\u0013\u0014\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0015F\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B%\u0015\n\u0013[\u0013\u0013\u0015\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0016\u0013\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B&\u0015\n\u0013[\u0013\u0014\u0013\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0016\u0019\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B'\u0015\n\u0013[\u0013\u0014\u0014\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0016D\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B(\u0015\n\u0013[\u0013\u0014\u0015\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0016H\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B)\u0015\n\u0013[\u0014\u0013\u0013\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0017\u0017\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B$\u0015\n\u0013[\u0014\u0013\u0014\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0017\u001b\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B%\u0015\n\u0013[\u0014\u0013\u0015\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0017F\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B&\u0015\n\u0013[\u0014\u0014\u0013\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0018\u0015\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B'\u0015\n\u0013[\u0014\u0014\u0014\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0018\u0019\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B(\u0015\n\u0013[\u0014\u0014\u0015\u0014\u0003_\u0003/RJLFDO\u00031RQ\u00109RODWLOH\u0003'HYLFH\u0003_\u0003\u0015\u0018\u0015\u0011\u0017\u0003*L%\u0003_\u0003\u0013[\u0013\u0013\u0018D\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B)\u0015\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003\u0016\u0015\u0011\u0013\u0003*L%\u0003\u0003_\u0003\u0013[\u0013\u0013\u0015\u0019\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B$\u0014\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003\u0016\u0015\u0011\u0013\u0003*L%\u0003\u0003_\u0003\u0013[\u0013\u0013\u0015D\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B%\u0014\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\n_\u0003\u0016\u0015\u0011\u0013\u0003*L%\u0003\u0003_\u0003\u0013[\u0013\u0013\u0015H\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B&\u0014\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003\u0016\u0015\u0011\u0013\u0003*L%\u0003\u0003_\u0003\u0013[\u0013\u0013\u0016\u0017\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B'\u0014\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003\u0016\u0015\u0011\u0013\u0003*L%\u0003\u0003_\u0003\u0013[\u0013\u0013\u0016\u001b\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B(\u0014\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003\u0016\u0015\u0011\u0013\u0003*L%\n_\u0003\u0013[\u0013\u0013\u0016F\u0003\u0003\u0003\u0003_\u0003&38\u0014B',00B)\u0014\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003\u0016\u0015\u0011\u0013\u0003*L%\u0003\u0003_\u0003\u0013[\u0013\u0013\u0017\u0015\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B$\u0014\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003\u0016\u0015\u0011\u0013\u0003*L%\u0003\u0003_\u0003\u0013[\u0013\u0013\u0017\u0019\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B%\u0014\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003\u0016\u0015\u0011\u0013\u0003*L%\u0003\u0003_\u0003\u0013[\u0013\u0013\u0017D\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B&\u0014\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003\u0016\u0015\u0011\u0013\u0003*L%\u0003\u0003_\u0003\u0013[\u0013\u0013\u0018\u0013\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B'\u0014\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003\u0016\u0015\u0011\u0013\u0003*L%\u0003\u0003_\u0003\u0013[\u0013\u0013\u0018\u0017\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B(\u0014\n1\u0012$\u0003\u0003\u0003\u0003_\u0003''5\u0017\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003_\u0003\u0016\u0015\u0011\u0013\u0003*L%\u0003\u0003_\u0003\u0013[\u0013\u0013\u0018\u001b\u0003\u0003\u0003\u0003_\u0003&38\u0015B',00B)\u0014\n\u0007\u0003VXGR\u0003LSPFWO\u0003VKRZ\u0003\u0010WRSRORJ\\\nFigure 19-2.  Topology report from the ipmctl show –topology command\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1882.84 sec"
            },
            {
              "page_number": 404,
              "text": "382\n\u0007BIOS Tuning Options\nThe BIOS contains many tuning options that change the behavior of CPU, memory, \npersistent memory, and NUMA. The location and name may vary between server \nplatform types, server vendors, persistent memory vendors, or BIOS versions. However, \nmost applicable tunable options can usually be found in the Advanced menu under \nMemory Configuration and Processor Configuration. Refer to your system BIOS user \nmanual for descriptions of each available option. You may want to test several BIOS \noptions with the application(s) to understand which options bring the most value.\n\u0007Automatic NUMA Balancing\nPhysical limitations to hardware are encountered when many CPUs and a lot of memory \nare required. The important limitation is the limited communication bandwidth \nbetween the CPUs and the memory. The NUMA architecture modification addresses \nthis issue. An application generally performs best when the threads of its processes are \naccessing memory on the same NUMA node as the threads are scheduled. Automatic \nNUMA balancing moves tasks (which can be threads or processes) closer to the memory \nthey are accessing. It also moves application data to memory closer to the tasks that \nreference it. The kernel does this automatically when automatic NUMA balancing \nis active. Most operating systems implement this feature. This section discusses the \nfeature on Linux; refer to your Linux distribution documentation for specific options as \nthey may vary.\nAutomatic NUMA balancing is enabled by default in most Linux distributions and \nwill automatically activate at boot time when the operating system detects it is running \non hardware with NUMA properties. To determine if the feature is enabled, use the \nfollowing command:\n$ sudo cat /proc/sys/kernel/numa_balancing\nA value of 1 (true) indicates the feature is enabled, whereas a value of 0 (zero/false) \nmeans it is disabled.\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1900.04 sec"
            },
            {
              "page_number": 405,
              "text": "383\nAutomatic NUMA balancing uses several algorithms and data structures, which are \nonly active and allocated if automatic NUMA balancing is active on the system, using a \nfew simple steps:\n•\t\nA task scanner periodically scans the address space and marks the \nmemory to force a page fault when the data is next accessed.\n•\t\nThe next access to the data will result in a NUMA Hinting Fault. Based \non this fault, the data can be migrated to a memory node associated \nwith the thread or process accessing the memory.\n•\t\nTo keep a thread or process, the CPU it is using and the memory it is \naccessing together, the scheduler groups tasks that share data.\nManual NUMA tuning of applications using numactl will override any system-wide \nautomatic NUMA balancing settings. Automatic NUMA balancing simplifies tuning \nworkloads for high performance on NUMA machines. Where possible, we recommend \nstatically tuning the workload to partition it within each node. Certain latency-sensitive \napplications, such as databases, usually work best with manual configuration. However, \nin most other use cases, automatic NUMA balancing should help performance.\n\u0007Using Volume Managers with Persistent Memory\nWe can provision persistent memory as a block device on which a file system can be \ncreated. Applications can access persistent memory using standard file APIs or memory \nmap a file from the file system and access the persistent memory directly through load/\nstore operations. The accessibility options are described in Chapters 2 and 3.\nThe main advantages of volume managers are increased abstraction, flexibility, and \ncontrol. Logical volumes can have meaningful names like “databases” or “web.” Volumes \ncan be resized dynamically as space requirements change and migrated between \nphysical devices within the volume group on a running system.\nOn NUMA systems, there is a locality factor between the CPU and the DRR and \npersistent memory that is directly attached to it. Accessing memory on a different CPU \nacross the interconnect incurs a small latency penalty. Latency-sensitive applications, \nsuch as databases, understand this and coordinate their threads to run on the same \nsocket as the memory they are accessing.\nCompared with SSD or NVMe capacity, persistent memory is relatively small. If \nyour application requires a single file system that consumes all persistent memory on \nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1905.68 sec"
            },
            {
              "page_number": 406,
              "text": "384\nthe system rather than one file system per NUMA node, a software volume manager \ncan be used to create concatenations or stripes (RAID0) using all the system’s capacity. \nFor example, if you have 1.5TiB of persistent memory per CPU socket on a two-socket \nsystem, you could build a concatenation or stripe (RAID0) to create a 3TiB file system. If \nlocal system redundancy is more important than large file systems, mirroring (RAID1) \npersistent memory across NUMA nodes is possible. In general, replicating the data \nacross physical servers for redundancy is better. Chapter 18 discusses remote persistent \nmemory in detail, including using remote direct memory access (RDMA) for data \ntransfer and replication across systems.\nThere are too many volume manager products to provide step-by-step recipes for all of \nthem within this book. On Linux, you can use Device Mapper (dmsetup), Multiple Device \nDriver (mdadm), and Linux Volume Manager (LVM) to create volumes that use the capacity \nfrom multiple NUMA nodes. Because most modern Linux distributions default to using \nLVM for their boot disks, we assume that you have some experience using LVM. There is \nextensive information and tutorials within the Linux documentation and on the Web.\nFigure 19-3 shows two regions on which we can create either an fsdax or sector \ntype namespace that creates the corresponding /dev/pmem0 and /dev/pmem1 devices. \nUsing /dev/pmem[01], we can create an LVM physical volume which we then combine \nto create a volume group. Within the volume group, we are free to create as many logical \nvolumes of the requested size as needed. Each logical volume can support one or more \nfile systems.\nFigure 19-3.  Linux Volume Manager architecture using persistent memory regions \nand namespaces\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page406_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page406_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1910.90 sec"
            },
            {
              "page_number": 407,
              "text": "385\nWe can also create a number of possible configurations if we were to create multiple \nnamespaces per region or partition the /dev/pmem* devices using fdisk or parted, for \nexample. Doing this provides greater flexibility and isolation of the resulting logical \nvolumes. However, if a physical NVDIMM fails, the impact is significantly greater since it \nwould impact some or all of the file systems depending on the configuration.\nCreating complex RAID volume groups may protect the data but at the cost of not \nefficiently using all the persistent memory capacity for data. Additionally, complex RAID \nvolume groups do not support the DAX feature that some applications may require.\n\u0007The mmap( ) MAP_SYNC Flag\nIntroduced in the Linux kernel v4.15, the MAP_SYNC flag ensures that any needed file \nsystem metadata writes are completed before a process is allowed to modify directly \nmapped data. The MAP_SYNC flag was added to the mmap() system call to request the \nsynchronous behavior; in particular, the guarantee provided by this flag is\nWhile a block is writeably mapped into page tables of this mapping, it is \nguaranteed to be visible in the file at that offset also after a crash.\nThis means the file system will not silently relocate the block, and it will ensure that the \nfile’s metadata is in a consistent state so that the blocks in question will be present after \na crash. This is done by ensuring that any needed metadata writes were done before the \nprocess is allowed to write pages affected by that metadata.\nWhen a persistent memory region is mapped using MAP_SYNC, the memory \nmanagement code will check to see whether there are metadata writes pending for the \naffected file. However, it will not actually flush those writes out. Instead, the pages are \nmapped read only with a special flag, forcing a page fault when the process first attempts \nto perform a write to one of those pages. The fault handler will then synchronously flush \nout any dirty metadata, set the page permissions to allow the write, and return. At that \npoint, the process can write the page safely, since all the necessary metadata changes \nhave already made it to persistent storage.\nThe result is a relatively simple mechanism that will perform far better than \nthe currently available alternative of manually calling fsync() before each write to \npersistent memory. The additional IO from fsync() can potentially cause the process to \nblock in what was supposed to be a simple memory write, introducing latency that may \nbe unexpected and unwanted.\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1917.86 sec"
            },
            {
              "page_number": 408,
              "text": "386\nThe mmap(2) man page in the Linux Programmer’s manual describes the MAP_SYNC \nflag as follows:\nMAP_SYNC (since Linux 4.15)\nThis flag is available only with the MAP_SHARED_VALIDATE mapping \ntype; mappings of type MAP_SHARED will silently ignore this flag. This flag \nis supported only for files supporting DAX (direct mapping of persistent \nmemory). For other files, creating a mapping with this flag results in an \nEOPNOTSUPP error.\nShared file mappings with this flag provide the guarantee that while some \nmemory is writably mapped in the address space of the process, it will be \nvisible in the same file at the same offset even after the system crashes or is \nrebooted. In conjunction with the use of appropriate CPU instructions, this \nprovides users of such mappings with a more efficient way of making data \nmodifications persistent.\n\u0007Summary\nIn this chapter, we presented some of the more advanced topics for persistent memory \nincluding page size considerations on large memory systems, NUMA awareness and \nhow it affects application performance, how to use volume managers to create DAX file \nsystems that span multiple NUMA nodes, and the MAP_SYNC flag for mmap(). Additional \ntopics such as BIOS tuning were intentionally left out of this book as it is vendor and \nproduct specific. Performance and benchmarking of persistent memory products are left \nto external resources as there are too many tools – vdbench, sysbench, fio, etc. – and too \nmany options for each one, to cover in this book.\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1920.95 sec"
            },
            {
              "page_number": 409,
              "text": "387\nOpen Access  This chapter is licensed under the terms of the Creative \nCommons Attribution 4.0 International License (http://creativecommons.\norg/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made.\nThe images or other third party material in this chapter are included in the chapter’s \nCreative Commons license, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the chapter’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need \nto obtain permission directly from the copyright holder.\nChapter 19  Advanced Topics",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1923.60 sec"
            },
            {
              "page_number": 410,
              "text": "389\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1\n\u0007APPENDIX A\nHow to Install NDCTL \nand DAXCTL on Linux\nThe ndctl utility is used to manage the libnvdimm (non-volatile memory device) \nsubsystem in the Linux kernel and to administer namespaces. The daxctl utility provides \nenumeration and provisioning commands for any device-dax namespaces you create. \ndaxctl is only required if you work directly with device-dax namespaces. We presented \na use-case for the ‘system-ram’ dax type in Chapter 10, that can use persistent memory \ncapacity to dynamically extend the usable volatile memory capacity in Linux. Chapter 10  \nalso showed how libmemkind can use device dax namespaces for volatile memory in \naddition to using DRAM. The default, and recommended, namespace for most developers \nis filesystem-dax (fsdax). Both Linux-only utilities - ndctl and daxctl - are open source and \nare intended to be persistent memory vendor neutral. Microsoft Windows has integrated \ngraphical utilities and PowerShell Commandlets to administer persistent memory.\nlibndctl and libdaxctl are required for several Persistent Memory Development Kit \n(PMDK) features if compiling from source. If ndctl is not available, the PMDK may not \nbuild all components and features, but it will still successfully compile and install. In \nthis appendix, we describe how to install ndctl and daxctl using the Linux package \nrepository only. To compile ndctl from source code, refer to the README on the ndctl \nGitHub repository (https://github.com/pmem/ndctl) or https://docs.pmem.io.\n\u0007Prerequisites\nInstalling ndctl and daxctl using packages automatically installs any missing \ndependency packages on the system. A full list of dependencies is usually listed when \ninstalling the package. You can query the package repository to list dependencies or use \nan online package took such as https://pkgs.org to find the package for your operating",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1928.41 sec"
            },
            {
              "page_number": 411,
              "text": "390\nsystem and list the package details. For example, Figure A-1 shows the packages \nrequired for ndctl v64.1 on Fedora 30 (https://fedora.pkgs.org/30/fedora-x86_64/\nndctl-64.1-1.fc30.x86_64.rpm.html).\n\u0007Installing NDCTL and DAXCTL Using the Linux \nDistribution Package Repository\nThe ndctl and daxctl utilities are delivered as runtime binaries with the option to \ninstall development header files which can be used to integrate their features in to your \napplication or when compiling PMDK from source code. To create debug binaries, \nyou need to compile ndctl and daxctl from source code. Refer to the README on the \nproject page https://github.com/pmem/ndctl or https://docs.pmem.io for detailed \ninstructions.\nFigure A-1.  Detailed package information for ndctl v64.1 on Fedora 30\nAppendix A  How to Install NDCTL and DAXCTL on Linux",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page411_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page411_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1931.48 sec"
            },
            {
              "page_number": 412,
              "text": "391\n\u0007Searching for Packages Within a Package Repository\nThe default package manager utility for your operating system will allow you to query \nthe package repository using regular expressions to identify packages to install. Table A-1 \nshows how to search the package repository using the command-line utility for several \ndistributions. If you prefer to use a GUI, feel free to use your favorite desktop utility to \nperform the same search and install operations described here.\nAdditionally, you can use an online package search tools such as https://pkgs.org \nthat allow you to search for packages across multiple distros. Figure A-2 shows the results \nfor many distros when searching for “libpmem.”\nTable A-1.  Searching for ndctl and daxctl packages in \ndifferent Linux distributions\nOperating System\nCommand\nFedora 21 or Earlier\n$ yum search ndctl \n$ yum search daxctl\nFedora 22 or Later\n$ dnf search ndctl \n$ dnf search daxctl\nRHEL AND CENTOS\n$ yum search ndctl \n$ yum search daxctl\nSLES AND OPENSUSE\n$ zipper search ndctl \n$ zipper search daxctl\nCANONICAL/Ubuntu\n$ aptitude search ndctl\n$ apt-cache search ndctl\n$ apt search ndctl \n$ aptitude search daxctl\n$ apt-cache search daxctl\n$ apt search daxctl\nAppendix A  How to Install NDCTL and DAXCTL on Linux",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1935.17 sec"
            },
            {
              "page_number": 413,
              "text": "392\n\u0007Installing NDCTL and DAXCTL from the Package  \nRepository\nInstructions for some popular Linux distributions follow. Skip to the section for your \noperating system. If your operating system is not listed here, it may share the same \npackage family as one listed here so you can use the same instructions. Should your \noperating system not meet either criteria, see the ndctl project home page https://\ngithub.com/pmem/ndctl or https://docs.pmem.io for installation instructions.\nFigure A-2.  https://pkgs.org search results for “ndctl”\nAppendix A  How to Install NDCTL and DAXCTL on Linux",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page413_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page413_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1938.04 sec"
            },
            {
              "page_number": 414,
              "text": "393\nNote  The version of the ndctl and daxctl available with your operating system \nmay not match the most current project release. If you require a newer release \nthan your operating system delivers, consider compiling the projects from the \nsource code. We do not describe compiling and installing from the source code \nin this book. Instructions can be found on https://docs.pmem.io/getting-­\nstarted-­guide/installing-ndctl#installing-ndctl-from-source-\non-­linux and https://github.com/pmem/ndctl.\n\u0007Installing PMDK on Fedora 22 or Later\nTo install individual packages, you can execute\n$ sudo dnf install <package>\nFor example, to install just the ndctl runtime utility and library, use\n$ sudo dnf install ndctl\nTo install all packages, use\nRuntime:\n$ sudo dnf install ndctl daxctl\nDevelopment library:\n$ sudo dnf install ndctl-devel\n\u0007Installing PMDK on RHEL and CentOS 7.5 or Later\nTo install individual packages, you can execute\n$ sudo yum install <package>\nFor example, to install just the ndctl runtime utility and library, use\n$ sudo yum install ndctl\nAppendix A  How to Install NDCTL and DAXCTL on Linux",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1941.62 sec"
            },
            {
              "page_number": 415,
              "text": "394\nTo install all packages, use\nRuntime:\n$ yum install ndctl daxctl\nDevelopment:\n$ yum install ndctl-devel\n\u0007Installing PMDK on SLES 12 and OpenSUSE or Later\nTo install individual packages, you can execute\n$ sudo zypper install <package>\nFor example, to install just the ndctl runtime utility and library, use\n$ sudo zypper install ndctl\nTo install all packages, use\nAll Runtime:\n$ zypper install ndctl daxctl\nAll Development:\n$ zypper install libndctl-devel\n\u0007Installing PMDK on Ubuntu 18.04 or Later\nTo install individual packages, you can execute\n$ sudo zypper install <package>\nFor example, to install just the ndctl runtime utility and library, use\n$ sudo zypper install ndctl\nTo install all packages, use\nAll Runtime:\n$ sudo apt-get install ndctl daxctl\nAll Development:\n$ sudo apt-get install libndctl-dev\nAppendix A  How to Install NDCTL and DAXCTL on Linux",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1944.28 sec"
            },
            {
              "page_number": 416,
              "text": "395\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1\n\u0007APPENDIX B\nHow to Install the \nPersistent Memory \nDevelopment Kit (PMDK)\nThe Persistent Memory Development Kit (PMDK) is available on supported operating \nsystems in package and source code formats. Some features of the PMDK require  \nadditional packages. We describe instructions for Linux and Windows.\n\u0007PMDK Prerequisites\nIn this appendix, we describe installing the PMDK libraries using the packages available \nin your operating system package repository. To enable all PMDK features, such as \nadvanced reliability, accessibility, and serviceability (RAS), PMDK requires libndctl and \nlibdaxctl. Package dependencies automatically install these requirements. If you are \nbuilding and installing using the source code, you should install NDCTL first using the \ninstructions provided in Appendix C.\n\u0007Installing PMDK Using the Linux Distribution \nPackage Repository\nThe PMDK is a collection of different libraries; each one provides different functionality. \nThis provides greater flexibility for developers as only the required runtime or header \nfiles need to be installed without installing unnecessary libraries.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1948.99 sec"
            },
            {
              "page_number": 417,
              "text": "396\n\u0007Package Naming Convention\nLibraries are available in runtime, development header files (∗-devel), and debug \n(∗-debug) versions. Table B-1 shows the runtime (libpmem), debug (libpmem-debug), \nand development and header files (libpmem-devel) for Fedora. Package names may \ndiffer between Linux distributions. We provide instructions for some of the common \nLinux distributions later in this section.\n\u0007Searching for Packages Within a Package Repository\nTable B-2 shows the list of available libraries as of PMDK v1.6. For an up-to-date list, see \nhttps://pmem.io/pmdk.\nTable B-2.  PMDK libraries as of PMDK v1.6\nLibrary\nDescription\nLIBPMEM\nLow-level persistent memory support library\nLIBRPMEM\nRemote Access to persistent memory library\nLIBPMEMBLK\nPersistent Memory Resident Array of Blocks library\nLIBPMEMCTO\nClose-to-Open Persistence library (Deprecated in PMDK v1.5)\nLIBPMEMLOG\nPersistent Memory Resident Log File library\nLIBPMEMOBJ\nPersistent Memory Transactional Object Store library\nLIBPMEMPOOL\nPersistent Memory pool management library\nPMEMPOOL\nUtilities for Persistent Memory\nTable B-1.  Example runtime, debug, and development package naming \nconvention\nLibrary\nDescription\nLIBPMEM\nLow-level persistent memory support library\nLIBPMEM-DEBUG\nDebug variant of the libpmem low-level persistent memory library\nLIBPMEM-DEVEL\nDevelopment files for the low-level persistent memory library\nAppendix B  How to Install the Persistent Memory Development Kit (PMDK)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1952.58 sec"
            },
            {
              "page_number": 418,
              "text": "397\nThe default package manager utility for your operating system will allow you to \nquery the package repository using regular expressions to identify packages to install. \nTable B-3 shows how to search the package repository using the command-line utility \nfor several distributions. If you prefer to use a GUI, feel free to use your favorite desktop \nutility to perform the same search and install operations described here.\nAdditionally, you can use an online package search tools such as https://pkgs.org \nthat allow you to search for packages across multiple distros. Figure B-1 shows the results \nfor many distros when searching for “libpmem.”\nTable B-3.  Searching for ∗pmem∗ packages on different Linux operating systems\nOperating System\nCommand\nFedora 21 or Earlier\n$ yum search pmem\nFedora 22 or Later\n$ dnf search pmem\n$ dnf repoquery *pmem*\nRHEL AND CENTOS\n$ yum search pmem\nSLES AND OPENSUSE\n$ zipper search pmem\nCANONICAL/Ubuntu\n$ aptitude search pmem\n$ apt-cache search pmem\n$ apt search pmem\nAppendix B  How to Install the Persistent Memory Development Kit (PMDK)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1956.17 sec"
            },
            {
              "page_number": 419,
              "text": "398\n\u0007Installing PMDK Libraries from the Package Repository\nInstructions for some popular Linux distributions follow. Skip to the section for your \noperating system. If your operating system is not listed here, it may share the same \npackage family as one listed here so you can use the same instructions. Should your \noperating system not meet either criteria, see https://docs.pmem.io for installation \ninstructions and the PMDK project home page (https://github.com/pmem/pmdk) to see \nthe most recent instructions.\nFigure B-1.  Search results for “libpmem” on https://pkgs.org\nAppendix B  How to Install the Persistent Memory Development Kit (PMDK)",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page419_img1.jpeg"
              ],
              "img_summary_files": [],
              "img_vision_files": [
                "output\\images\\img_vision\\Programming_Persistent_Memory_medium_457_page419_img1_vision.json"
              ],
              "summary": "Summary not available due to an error.",
              "time_taken": "1958.31 sec"
            },
            {
              "page_number": 420,
              "text": "399\nNote  The version of the PMDK libraries available with your operating system may \nnot match the most current PMDK release. If you require a newer release than your \noperating system delivers, consider compiling PMDK from the source code. We \ndo not describe compiling and installing PMDK from the source code in this book. \nInstructions can be found on https://docs.pmem.io/getting-started-\nguide/installing-pmdk/compiling-pmdk-from-source and https://\ngithub.com/pmem/pmdk.\n\u0007Installing PMDK on Fedora 22 or Later\nTo install individual libraries, you can execute\n$ sudo dnf install <library>\nFor example, to install just the libpmem runtime library, use\n$ sudo dnf install libpmem\nTo install all packages, use\nAll Runtime:\n$ sudo dnf install libpmem librpmem libpmemblk libpmemlog/\n   libpmemobj libpmempool pmempool\nAll Development:\n$ sudo dnf install libpmem-devel librpmem-devel \\\n   libpmemblk-devel libpmemlog-devel libpmemobj-devel \\\n   libpmemobj++-devel libpmempool-devel\nAll Debug:\n$ sudo dnf install libpmem-debug librpmem-debug \\\n   libpmemblk-debug libpmemlog-debug libpmemobj-debug \\\n   libpmempool-debug\nAppendix B  How to Install the Persistent Memory Development Kit (PMDK)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1963.02 sec"
            },
            {
              "page_number": 421,
              "text": "400\n\u0007Installing PMDK on RHEL and CentOS 7.5 or Later\nTo install individual libraries, you can execute\n$ sudo yum install <library>\nFor example, to install just the libpmem runtime library, use\n$ sudo yum install libpmem\nTo install all packages, use\nAll Runtime:\n$ sudo yum install libpmem librpmem libpmemblk libpmemlog \\\n    libpmemobj libpmempool pmempool\nAll Development:\n$ sudo yum install libpmem-devel librpmem-devel \\\n    libpmemblk-devel libpmemlog-devel libpmemobj-devel \\\n    libpmemobj++-devel libpmempool-devel\nAll Debug:\n$ sudo yum install libpmem-debug librpmem-debug \\\n    libpmemblk-debug libpmemlog-debug libpmemobj-debug \\\n    libpmempool-debug\n\u0007Installing PMDK on SLES 12 and OpenSUSE or Later\nTo install individual libraries, you can execute\n$ sudo zypper install <library>\nFor example, to install just the libpmem runtime library, use\n$ sudo zypper install libpmem\nTo install all packages, use\nAll Runtime:\n$ sudo zypper install libpmem librpmem libpmemblk libpmemlog \\\n    libpmemobj libpmempool pmempool\nAppendix B  How to Install the Persistent Memory Development Kit (PMDK)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1966.71 sec"
            },
            {
              "page_number": 422,
              "text": "401\nAll Development:\n$ sudo zypper install libpmem-devel librpmem-devel \\\n    libpmemblk-devel libpmemlog-devel libpmemobj-devel \\\n    libpmemobj++-devel libpmempool-devel\nAll Debug:\n$ sudo zypper install libpmem-debug librpmem-debug \\\n    libpmemblk-debug libpmemlog-debug libpmemobj-debug \\\n    libpmempool-debug\n\u0007Installing PMDK on Ubuntu 18.04 or Later\nTo install individual libraries, you can execute\n$ sudo zypper install <library>\nFor example, to install just the libpmem runtime library, use\n$ sudo zypper install libpmem\nTo install all packages, use\nAll Runtime:\n$ sudo apt-get install libpmem1 librpmem1 libpmemblk1 \\\n    libpmemlog1 libpmemobj1 libpmempool1\nAll Development:\n$ sudo apt-get install libpmem-dev librpmem-dev \\\n    libpmemblk-dev libpmemlog-dev libpmemobj-dev \\\n    libpmempool-dev libpmempool-dev\nAll Debug:\n$ sudo apt-get install libpmem1-debug \\\n    librpmem1-debug libpmemblk1-debug \\\n    libpmemlog1-debug libpmemobj1-debug libpmempool1-debug\nAppendix B  How to Install the Persistent Memory Development Kit (PMDK)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1970.40 sec"
            },
            {
              "page_number": 423,
              "text": "402\n\u0007Installing PMDK on Microsoft Windows\nThe recommended and easiest way to install PMDK on Windows is to use Microsoft vcpkg. \nVcpkg is an open source tool and ecosystem created for library management. To build \nPMDK from source that can be used in a different packaging or development solution, see \nthe README on https://github.com/pmem/pmdk or https://docs.pmem.io.\nTo install the latest PMDK release and link it to your Visual Studio solution, you first \nneed to clone and set up vcpkg on your machine as described on the vcpkg GitHub page \n(https://github.com/Microsoft/vcpkg).\nIn brief:\n> git clone https://github.com/Microsoft/vcpkg\n> cd vcpkg\n> .\\bootstrap-vcpkg.bat\n> .\\vcpkg integrate install\n> .\\vcpkg install pmdk:x64-windows\nNote  The last command can take a while as PMDK builds and installs.\nAfter successful completion of all of the preceding steps, the libraries are ready to \nbe used in Visual Studio with no additional configuration is required. Just open Visual \nStudio with your existing project or create a new one (remember to use platform x64) \nand then include headers to project as you always do.\nAppendix B  How to Install the Persistent Memory Development Kit (PMDK)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1975.41 sec"
            },
            {
              "page_number": 424,
              "text": "403\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1\n\u0007APPENDIX C\nHow to Install IPMCTL \non Linux and Windows\nThe ipmctl utility is used to configure and manage Intel Optane DC persistent memory \nmodules (DCPMM). This is a vendor-specific utility available for Linux and Windows. It \nsupports functionality to:\n•\t\nDiscover DCPMMs on the platform\n•\t\nProvision the platform memory configuration\n•\t\nView and update the firmware on DCPMMs\n•\t\nConfigure data-at-rest security on DCPMMs\n•\t\nMonitor DCPMM health\n•\t\nTrack performance of DCPMMs\n•\t\nDebug and troubleshoot DCPMMs\nipmctl refers to the following interface components:\n•\t\nlibipmctl: An application programming interface (API) library for \nmanaging PMMs\n•\t\nipmctl: A command-line interface (CLI) application for configuring \nand managing PMMs from the command line\n•\t\nipmctl-monitor: A monitor daemon/system service for monitoring \nthe health and status of PMMs",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1981.35 sec"
            },
            {
              "page_number": 425,
              "text": "404\n\u0007IPMCTL Linux Prerequisites\nipmctl requires libsafec as a dependency.\n\u0007libsafec\nlibsafec is available as a package in the Fedora package repository. For other Linux \ndistributions, it is available as a separate downloadable package for local installation:\n•\t\nRHEL/CentOS EPEL 7 packages can be found at\nhttps://copr.fedorainfracloud.org/coprs/jhli/safeclib/.\n•\t\nOpenSUSE/SLES packages can be found at\nhttps://build.opensuse.org/package/show/home:jhli/safeclib.\n•\t\nUbuntu packages can be found at\nhttps://launchpad.net/~jhli/+archive/ubuntu/libsafec.\nAlternately, when compiling ipmctl from source code, use the -DSAFECLIB_SRC_\nDOWNLOAD_AND_STATIC_LINK=ON option to download sources and statically link to \nsafeclib.\n\u0007IPMCTL Linux Packages\nAs a vendor-specific utility, it is not included in most Linux distribution package \nrepositories other than Fedora. EPEL7 packages can be found at https://copr.\nfedorainfracloud.org/coprs/jhli/ipmctl. OpenSUSE and SLES packages can be \nfound at https://build.opensuse.org/package/show/home:jhli/ipmctl.\n\u0007IPMCTL for Microsoft Windows\nThe latest Windows EXE binary for ipmctl can be downloaded from the “Releases” \nsection of the GitHub project page (https://github.com/intel/ipmctl/releases) as \nshown in Figure C-1.\nAppendix C  How to Install IPMCTL on Linux and Windows",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1983.19 sec"
            },
            {
              "page_number": 426,
              "text": "405\nRunning the executable installs ipmctl and makes it available via the command-line \nand PowerShell interfaces.\n\u0007Using ipmctl\nThe ipmctl utility provides system administrators with the ability to configure \nIntel Optane DC persistent memory modules which can then be used by Windows \nPowerShellCmdlets or ndctl on Linux to create namespaces on which file systems can \nbe created. Applications can then create persistent memory pools and memory map \nthem to get direct access to the persistent memory. Detailed information about the \nmodules can also be extracted to help with errors or debugging.\nipmctl has a rich set of commands and options that can be displayed by running \nipmctl without any command verb, as shown in Listing C-1.\nFigure C-1.  ipmctl releases on GitHub (https://github.com/intel/ipmctl/\nreleases)\nAppendix C  How to Install IPMCTL on Linux and Windows",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page426_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page426_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1986.47 sec"
            },
            {
              "page_number": 427,
              "text": "406\nListing C-1.  Listing the command verbs and simple usage information\n# ipmctl version\nIntel(R) Optane(TM) DC Persistent Memory Command Line Interface Version \n01.00.00.3279\n# ipmctl\nIntel(R) Optane(TM) DC Persistent Memory Command Line Interface\n    Usage: ipmctl <verb>[<options>][<targets>][<properties>]\nCommands:\n    Display the CLI help.\n    help\n    Display the CLI version.\n    version\n    Update the firmware on one or more DIMMs\n    load -source (File Source) -dimm[(DimmIDs)]\n    \u0007Set properties of one/more DIMMs such as device security and modify \ndevice.\n    set -dimm[(DimmIDs)]\n    Erase persistent data on one or more DIMMs.\n    delete -dimm[(DimmIDs)]\n    Show information about one or more Regions.\n    show -region[(RegionIDs)] -socket(SocketIDs)\n    Provision capacity on one or more DIMMs into regions\n    create -dimm[(DimmIDs)] -goal -socket(SocketIDs)\n    Show region configuration goal stored on one or more DIMMs\n    show -dimm[(DimmIDs)] -goal -socket[(SocketIDs)]\n    Delete the region configuration goal from one or more DIMMs\n    delete -dimm[(DimmIDs)] -goal -socket(SocketIDs)\nAppendix C  How to Install IPMCTL on Linux and Windows",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1992.51 sec"
            },
            {
              "page_number": 428,
              "text": "407\n    Load stored configuration goal for specific DIMMs\n    load -source (File Source) -dimm[(DimmIDs)] -goal -socket(SocketIDs)\n    Store the region configuration goal from one or more DIMMs to a file\n    dump -destination (file destination) -system -config\n    Modify the alarm threshold(s) for one or more DIMMs.\n    set -sensor(List of Sensors) -dimm[(DimmIDs)]\n    Starts a playback or record session\n    start -session -mode -tag\n    Stops the active playback or recording session.\n    stop -session\n    Dump the PBR session buffer to a file\n    dump -destination (file destination) -session\n    Show basic information about session pbr file\n    show -session\n    Load Recording into memory\n    load -source (File Source) -session\n    Clear the namespace LSA partition on one or more DIMMs\n    delete -dimm[(DimmIDs)] -pcd[(Config)]\n    Show error log for given DIMM\n    show -error(Thermal|Media) -dimm[(DimmIDs)]\n    Dump firmware debug log\n    dump -destination (file destination) -debug -dimm[(DimmIDs)]\n    Show information about one or more DIMMs.\n    show -dimm[(DimmIDs)] -socket[(SocketIDs)]\n    \u0007Show basic information about the physical  processors in the host \nserver.\n    show -socket[(SocketIDs)]\n    Show health statistics\n    show -sensor[(List of Sensors)] -dimm[(DimmIDs)]\nAppendix C  How to Install IPMCTL on Linux and Windows",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1995.07 sec"
            },
            {
              "page_number": 429,
              "text": "408\n    Run a diagnostic test on one or more DIMMs\n    start -diagnostic[(Quick|Config|Security|FW)] -dimm[(DimmIDs)]\n    Show the topology of the DCPMMs installed in the host server\n    show -topology -dimm[(DimmIDs)] -socket[(SocketIDs)]\n    Show information about total DIMM resource allocation.\n    show -memoryresources\n    Show information about BIOS memory management capabilities.\n    show -system -capabilities\n    Show information about firmware on one or more DIMMs.\n    show -dimm[(DimmIDs)] -firmware\n    Show the ACPI tables related to the DIMMs in the system.\n    show -system[(NFIT|PCAT|PMTT)]\n    Show pool configuration goal stored on one or more DIMMs\n    show -dimm[(DimmIDs)] -pcd[(Config|LSA)]\n    Show user preferences and their current values\n    show -preferences\n    Set user preferences\n    set -preferences\n    Show Command Access Policy Restrictions for DIMM(s).\n    show -dimm[(DimmIDs)] -cap\n    Show basic information about the host server.\n    show -system -host\n    Show event stored on one in the system log\n    show -event -dimm[(DimmIDs)]\n    Set event's action required flag on/off\n    set -event(EventID)  ActionRequired=(0)\n    Capture a snapshot of the system state for support purposes\n    dump -destination (file destination) -support\nAppendix C  How to Install IPMCTL on Linux and Windows",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "1999.89 sec"
            },
            {
              "page_number": 430,
              "text": "409\n    Show performance statistics per DIMM\n    show -dimm[(DimmIDs)] -performance[(Performance Metrics)]\n \u0007Please see ipmctl <verb> -help <command> i.e 'ipmctl show -help -dimm' for \nmore information on specific command\nEach command has its own man page. A full list of man pages can be found from the \nIPMCTL(1) man page by running “man ipmctl”.\nAn online ipmctl User Guide can be found at https://docs.pmem.io. This guide \nprovides detailed step-by-step instructions and in-depth information about ipmctl and \nhow to use it to provision and debug issues. An ipmctl Quick Start Guide can be found \nat https://software.intel.com/en-us/articles/quick-start-guide-configure-\nintel-­optane-dc-persistent-memory-on-linux.\nFor a short video walk-through of using ipmctl and ndctl, you can watch the \n“Provision Intel Optane DC Persistent Memory in Linux” webinar recording (https://\nsoftware.intel.com/en-us/videos/provisioning-intel-optane-dc-persistent-\nmemory-modules-in-linux).\nIf you have questions relating to ipmctl, Intel Optane DC persistent memory, or a \ngeneral persistent memory question, you can ask it in the Persistent Memory Google \nForum (https://groups.google.com/forum/#!forum/pmem). Questions or issues \nspecific to ipmctl should be posted as an issue or question on the ipmctl GitHub issues \nsite (https://github.com/intel/ipmctl/issues).\nAppendix C  How to Install IPMCTL on Linux and Windows",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2008.08 sec"
            },
            {
              "page_number": 431,
              "text": "411\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1\n\u0007APPENDIX D\nJava for Persistent \nMemory\nJava is one of the most popular programming languages available because it is fast, \nsecure, and reliable. There are lots of applications and web sites implemented in Java. \nIt is cross-platform and supports multi-CPU architectures from laptops to datacenters, \ngame consoles to scientific supercomputers, cell phones to the Internet, and CD/DVD \nplayers to automotive. Java is everywhere!\nAt the time of writing this book, Java did not natively support storing data persistently \non persistent memory, and there were no Java bindings for the Persistent Memory \nDevelopment Kit (PMDK), so we decided Java was not worthy of a dedicated chapter.  \nWe didn’t want to leave Java out of this book given its popularity among developers,  \nso we decided to include information about Java in this appendix.\nIn this appendix, we describe the features that have already been integrated in to \nOracle’s Java Development Kit (JDK) [https://www.oracle.com/java/] and OpenJDK \n[https://openjdk.java.net/]. We also provide information about proposed persistent \nmemory functionality in Java as well as two external Java libraries in development.\n\u0007Volatile Use of Persistent Memory\nJava does support persistent memory for volatile use cases on systems that have \nheterogeneous memory architectures. That is a system with DRAM, persistent memory, \nand non-volatile storage such as SSD or NVMe drives.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2008.59 sec"
            },
            {
              "page_number": 432,
              "text": "412\n\u0007Heap Allocation on Alternative Memory Devices\nBoth Oracle JDK v10 and OpenJDK v10 implemented JEP 316: Heap allocation on \nalternative memory devices [http://openjdk.java.net/jeps/316]. The goal of this \nfeature is to enable the HotSpot VM to allocate the Java object heap on an alternative \nmemory device, such as persistent memory, specified by the user.\nAs described in Chapter 3, Linux and Windows can expose persistent memory \nthrough the file system. Examples are NTFS and XFS or ext4. Memory-mapped files \non these direct access (DAX) file systems bypass the page cache and provide a direct \nmapping of virtual memory to the physical memory on the device.\nTo allocate the Java heap using memory-mapped files on a DAX file system, Java \nadded a new runtime option, -XX:AllocateHeapAt=<path>. This option takes a path \nto the DAX file system and uses memory mapping to allocate the object heap on the \nmemory device. Using this option enables the HotSpot VM to allocate the Java object \nheap on an alternative memory device, such as persistent memory, specified by the user. \nThe feature does not intend to share a non-volatile region between multiple running \nJVMs or reuse the same region for further invocations of the JVM.\nFigure D-1 shows the architecture of this new heap allocation method using both \nDRAM and persistent memory backed virtual memory.\nThe Java heap is allocated only from persistent memory. The mapping to DRAM is \nshown to emphasize that non-heap components like code cache, gc bookkeeping, and \nso on, are allocated from DRAM.\nFigure D-1.  Java heap memory allocated from DRAM and persistent memory \nusing the “-XX:AllocateHeapAt=<path>” option\nAppendix D  Java for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page432_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page432_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2021.29 sec"
            },
            {
              "page_number": 433,
              "text": "413\nThe existing heap-related flags such as -Xmx, -Xms, and garbage collection–related \nflags will continue to work as before. For example:\n$ java –Xmx32g –Xms16g –XX:AllocateHeapAt=/pmemfs/jvmheap \\ \nApplicationClass\nThis allocates an initial 16GiB heap size (-Xms) with a maximum heap size up to \n32GiB (-Xmx32g). The JVM heap can use the capacity of a temporary file created within \nthe path specified by --XX:AllocateHeapAt=/pmemfs/jvmheap. JVM automatically \ncreates a temporary file of the form jvmheap.XXXXXX, where XXXXXX is a randomly \ngenerated number. The directory path should be a persistent memory backed file system \nmounted with the DAX option. See Chapter 3 for more information about mounting file \nsystems with the DAX feature.\nTo ensure application security, the implementation must ensure that file(s) created \nin the file system are:\n•\t\nProtected by correct permissions, to prevent other users from \naccessing it\n•\t\nRemoved when the application terminates, in any possible scenario\nThe temporary file is created with read-write permissions for the user running the \nJVM, and the JVM deletes the file before terminating.\nThis feature targets alternative memory devices that have the same semantics as DRAM, \nincluding the semantics of atomic operations, and can therefore be used instead of DRAM for \nthe object heap without any change to existing application code. All other memory structures \nsuch as the code heap, metaspace, thread stacks, etc., will continue to reside in DRAM.\nSome use cases of this feature include\n•\t\nIn multi-JVM deployments, some JVMs such as daemons, services, \netc., have lower priority than others. Persistent memory would \npotentially have higher access latency compared to DRAM. Low-­\npriority processes can use persistent memory for the heap, allowing \nhigh-priority processes to use more DRAM.\n•\t\nApplications such as big data and in-memory databases have an \never-increasing demand for memory. Such applications could use \npersistent memory for the heap since persistent memory modules \nwould potentially have a larger capacity compared to DRAM.\nAppendix D  Java for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2021.80 sec"
            },
            {
              "page_number": 434,
              "text": "414\nMore information about this feature can be found in these resources:\n•\t\nOracle JavaSE 10 Documentation [https://docs.oracle.com/\njavase/10/tools/java.htm#GUID-3B1CE181-CD30-4178-9602-\n230B800D4FAE__BABCBGHF]\n•\t\nOpenJDK JEP 316: Heap Allocation on Alternative Memory Devices \n[http://openjdk.java.net/jeps/316]\n\u0007Partial Heap Allocation on Alternative Memory Devices\nHotSpot JVM 12.0.1 introduced a feature to allocate old generation of Java heap on an \nalternative memory device, such as persistent memory, specified by the user.\nThe feature in G1 and parallel GC allows them to allocate part of heap memory in \npersistent memory to be used exclusively for old generation objects. The rest of the heap \nis mapped to DRAM, and young generation objects are always placed here.\nOperating systems expose persistent memory devices through the file system, so \nthe underlying media can be accessed directly, or direct access (DAX). File systems that \nsupport DAX include NTFS on Microsoft Windows and ext4 and XFS on Linux. Memory-­\nmapped files in these file systems bypass the file cache and provide a direct mapping of \nvirtual memory to the physical memory on the device. The specification of a path to a \nDAX mounted file system uses the flag -XX:AllocateOldGenAt=<path> which enables \nthis feature. There are no additional flags to enable this feature.\nWhen enabled, young generation objects are placed in DRAM only, while old \ngeneration objects are always allocated in persistent memory. At any given point, the \ngarbage collector guarantees that the total memory committed in DRAM and persistent \nmemory is always less than the size of the heap as specified by -Xmx.\nWhen enabled, the JVM also limits the maximum size of the young generation based \non available DRAM, although it is recommended that users set the maximum size of the \nyoung generation explicitly.\nFor example, if the JVM is executed with -Xmx756g on a system with 32GB DRAM and \n1024GB persistent memory, the garbage collector will limit the young generation size \nbased on the following rules:\n•\t\nNo -XX:MaxNewSize or -Xmn is specified: The maximum young \ngeneration size is set to 80% of available memory (25.6GB).\nAppendix D  Java for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2025.59 sec"
            },
            {
              "page_number": 435,
              "text": "415\n•\t\n-XX:MaxNewSize or -Xmn is specified: The maximum young \ngeneration size is capped at 80% of available memory (25.6GB) \nregardless of the amount specified.\n•\t\nUsers can use -XX:MaxRAM to let the VM know how much DRAM is \navailable for use. If specified, maximum young gen size is set to 80% \nof the value in MaxRAM.\n•\t\nUsers can specify the percentage of DRAM to use, instead of the \ndefault 80%, for young generation with\n•\t\n-XX:MaxRAMPercentage.\n•\t\nEnabling logging with the logging option gc+ergo=info will print the \nmaximum young generation size at startup.\n\u0007Non-volatile Mapped Byte Buffers\nJEP 352: Non-Volatile Mapped Byte Buffers [https://openjdk.java.net/jeps/352] \nadds a new JDK-specific file mapping mode so that the FileChannel API can be used to \ncreate MappedByteBuffer instances that refer to persistent memory. The feature should \nbe available in Java 14 when it is released, which is after the publication of this book.\nThis JEP proposes to upgrade MappedByteBuffer to support access to persistent \nmemory. The only API change required is a new enumeration employed by FileChannel \nclients to request mapping of a file located on a DAX file system rather than a \nconventional, file storage system. Recent changes to the MappedByteBufer API mean \nthat it supports all the behaviors needed to allow direct memory updates and provide \nthe durability guarantees needed for higher level, Java client libraries to implement \npersistent data types (e.g., block file systems, journaled logs, persistent objects, etc.). The \nimplementations of FileChannel and MappedByteBuffer need revising to be aware of \nthis new backing type for the mapped file.\nThe primary goal of this JEP is to ensure that clients can access and update persistent \nmemory from a Java program efficiently and coherently. A key element of this goal is to \nensure that individual writes (or small groups of contiguous writes) to a buffer region can \nbe committed with minimal overhead, that is, to ensure that any changes which might \nstill be in cache are written back to memory.\nAppendix D  Java for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2031.43 sec"
            },
            {
              "page_number": 436,
              "text": "416\nA second, subordinate goal is to implement this commit behavior using a restricted, \nJDK-internal API defined in class unsafe, allowing it to be reused by classes other than \nMappedByteBuffer that may need to commit to persistent memory.\nA final, related goal is to allow buffers mapped over persistent memory to be tracked \nby the existing monitoring and management APIs.\nIt is already possible to map a persistent memory device file to a MappedByteBuffer \nand commit writes using the current force() method, for example, using Intel’s \nlibpmem library as device driver or by calling out to libpmem as a native library. \nHowever, with the current API, both those implementations provide a “sledgehammer” \nsolution. A force cannot discriminate between clean and dirty lines and requires a \nsystem call or JNI call to implement each writeback. For both those reasons, the existing \ncapability fails to satisfy the efficiency requirement of this JEP.\nThe target OS/CPU platform combinations for this JEP are Linux/x64 and Linux/\nAArch64. This restriction is imposed for two reasons. This feature will only work on OSes \nthat support the mmap system call MAP_SYNC flag, which allows synchronous mapping \nof non-volatile memory. That is true of recent Linux releases. It will also only work on \nCPUs that support cache line writeback under user space control. x64 and AArch64 both \nprovide instructions meeting this requirement.\n\u0007Persistent Collections for Java (PCJ)\nThe Persistent Collections for Java library (PCJ) is an open source Java library being \ndeveloped by Intel for persistent memory programming. More information on PCJ, \nincluding source code and sample code, is available on GitHub at https://github.com/\npmem/pcj.\nAt the time of writing this book, the PCJ library was still defined as a “pilot” project \nand still in an experimental state. It is being made available now in the hope it is useful in \nexploring the retrofit of existing Java code to use persistent memory as well as exploring \npersistent Java programming in general.\nThe library offers a range of thread-safe persistent collection classes including arrays, \nlists, and maps. It also offers persistent support for things like strings and primitive integer \nand floating-point types. Developers can define their own persistent classes as well.\nInstances of these persistent classes behave much like regular Java objects, but \ntheir fields are stored in persistent memory. Like regular Java objects, their lifetime is \nreachability-based; they are automatically garbage collected if there are no outstanding \nAppendix D  Java for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2038.39 sec"
            },
            {
              "page_number": 437,
              "text": "417\nreferences to them. Unlike regular Java objects, their lifetime can extend beyond a single \ninstance of the Java virtual machine and beyond machine restarts.\nBecause the contents of persistent objects are retained, it’s important to maintain \ndata consistency of objects even in the face of crashes and power failures. Persistent \ncollections and other objects in the library offer persistent data consistency at the Java \nmethod level. Methods, including field setters, behave as if the method’s changes to \npersistent memory all happen or none happen. This same method-level consistency can \nbe achieved with developer-defined classes using a transaction API offer by PCJ.\nPCJ uses the libpmemobj library from the Persistent Memory Development Kit \n(PMDK) which we discussed in Chapter 7. For additional information on PMDK, please \nvisit https://pmem.io/ and https://github.com/pmem/pmdk.\n\u0007Using PCJ in Java Applications\nTo import this library into an existing Java application, include the project’s target/\nclasses directory in your Java classpath and the project’s target/cppbuild directory in \nyour java.library.path. For example:\n$ javac -cp .:<path>/pcj/target/classes <source>\n$ java -cp .:<path>/pcj/target/classes \\\n    -Djava.library.path=<path>/pcj/target/cppbuild <class>\nThere are several ways to use the PCJ library:\n\t 1.\t Use instances of built-in persistent classes in your applications.\n\t 2.\t Extend built-in persistent classes with new methods.\n\t 3.\t Declare new persistent classes or extend built-in classes with \nmethods and persistent fields.\nPCJ source code examples can be found in the resources listed in the following:\n•\t\nIntroduction to Persistent Collections for Java – https://github.\ncom/pmem/pcj/blob/master/Introduction.txt\n•\t\nCode Sample: Introduction to Java∗ API for Persistent Memory \nProgramming – https://software.intel.com/en-us/articles/\ncode-sample-introduction-to-java-api-for-persistent-\nmemory-programming\nAppendix D  Java for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2043.30 sec"
            },
            {
              "page_number": 438,
              "text": "418\n•\t\nCode Sample: Create a “Hello World” Program Using Persistent \nCollections for Java∗ (PCJ) – https://software.intel.com/en-us/\narticles/code-sample-create-a-hello-world-program-using-­\npersistent-collections-for-java-pcj\n\u0007Low-Level Persistent Library (LLPL)\nThe Low-Level Persistence Library (LLPL) is an open source Java library being \ndeveloped by Intel for persistent memory programming. By providing Java access to \npersistent memory at a memory block level, LLPL gives developers a foundation for \nbuilding custom abstractions or retrofitting existing code. More information on LLPL, \nincluding source code, sample code, and javadocs, is available on GitHub at https://\ngithub.com/pmem/llpl.\nThe library offers management of heaps of persistent memory and manual allocation \nand deallocation of blocks of persistent memory within a heap. A Java persistent memory \nblock class provides methods to read and write Java integer types within a block as well \nas copy bytes from block to block and between blocks and (volatile) Java byte arrays.\nSeveral different kinds of heaps and corresponding memory blocks are available \nto aid in implementing different data consistency schemes. Examples of such \nimplementable schemes:\n•\t\nTransactional: Data in memory is usable after a crash or power failure\n•\t\nPersistent: Data in memory is usable after a controlled process exit\n•\t\nVolatile: Persistent memory used for its large capacity, data is not \nneeded after exit.\nMixed data consistency schemes are also implementable. For example, transactional \nwrites for critical data and either persistent or volatile writes for less critical data (e.g., \nstatistics or caches).\nLLPL uses the libpmemobj library from the Persistent Memory Development Kit (PMDK) \nwhich we discussed in Chapter 7. For additional information on PMDK, please visit \nhttps://pmem.io/ and https://github.com/pmem/pmdk.\nAppendix D  Java for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2048.01 sec"
            },
            {
              "page_number": 439,
              "text": "419\n\u0007Using LLPL in Java Applications\nTo use LLPL with your Java application, you need to have PMDK and LLPL installed \non your system. To compile the Java classes, you need to specify the LLPL class path. \nAssuming you have LLPL installed on your home directory, do the following:\n$ javac -cp .:/home/<username>/llpl/target/classes LlplTest.java\nAfter that, you should see the generated ∗.class file. To run the main() method \ninside your class, you need to again pass the LLPL class path. You also need to set the \njava.library.path environment variable to the location of the compiled native library \nused as a bridge between LLPL and PMDK:\n$ java -cp .:/.../llpl/target/classes \\\n-Djava.library.path=/.../llpl/target/cppbuild LlplTest\nPCJ source code examples can be found in the resources listed in the following:\n•\t\nCode Sample: Introducing the Low-Level Persistent Library (LLPL) \nfor Java∗ – https://software.intel.com/en-us/articles/\nintroducing-the-low-level-persistent-library-llpl-for-java\n•\t\nCode Sample: Create a “Hello World” Program Using the Low-Level \nPersistence Library (LLPL) for Java∗ – https://software.intel.\ncom/en-us/articles/code-sample-create-a-hello-world-\nprogram-using-the-low-level-persistence-library-llpl- \nfor-java\n•\t\nEnabling Persistent Memory Use in Java – https://www.snia.\norg/sites/default/files/PM-Summit/2019/presentations/05-\nPMSummit19-Dohrmann.pdf\n\u0007Summary\nAt the time of writing this book, native support for persistent memory in Java is an \nongoing effort. Current features are mostly volatile, meaning the data is not persisted \nonce the app exits. We have described several features that have been integrated and \nshown two libraries – LLPL and PCJ – that provide additional functionality for Java \napplications.\nAppendix D  Java for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2052.72 sec"
            },
            {
              "page_number": 440,
              "text": "420\nThe Low-Level Persistent Library (LLPL) is an open source Java library being \ndeveloped by Intel for persistent memory programming. By providing Java access to \npersistent memory at a memory block level, LLPL gives developers a foundation for \nbuilding custom abstractions or retrofitting existing code.\nThe higher-level Persistent Collections for Java (PCJ) offers developers a range of \nthread-safe persistent collection classes including arrays, lists, and maps. It also offers \npersistent support for things like strings and primitive integer and floating-point types. \nDevelopers can define their own persistent classes as well.\nAppendix D  Java for Persistent Memory",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2055.28 sec"
            },
            {
              "page_number": 441,
              "text": "421\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1\n\u0007APPENDIX E\nThe Future of Remote \nPersistent Memory \nReplication\nAs discussed in Chapter 18, the general purpose and appliance remote persistent \nmemory methods are simple high-level upper-layer-protocol (ULP) changes. These \nmethods add a secondary RDMA Send or RDMA Read after a number of RDMA Writes \nto remote persistent memory. One of the pain points with these implementations is \nthe Intel-specific platform feature, allocating writes, which, by default, pushes inbound \nPCIe Write data from the NIC directly into the lowest-level CPU cache, speeding the \nlocal software access to that newly written data. For persistent memory, it is desirable \nto turn off allocating writes to persistent memory, elevating the need to flush the CPU \ncache to guarantee persistence. However, the platform limitations on the control over \nallocating writes only imprecise control over the behavior of writes for an entire PCIe \nRoot complex. All devices connected to a given root complex will behave the same way. \nThe implications to other software running on the system can be difficult to determine \nif access to the write data is delayed by bypassing caches. These are contradictory \nrequirements since allocating writes should be disabled for writes to persistent memory, \nbut for writes to volatile memory, allocating writes should be enabled.\nTo make this per IO steering possible, the networking hardware and software \nneed to have native support for persistent memory. If the networking stack is aware \nof the persistent memory regions, it can select whether the write is steered toward the \npersistent memory subsystem or the volatile memory subsystem on a per IO basis, \ncompletely removing the need to change global PCIe Root complex allocating-write \nsettings.",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2061.22 sec"
            },
            {
              "page_number": 442,
              "text": "422\nAlso, if the hardware is aware of writes to persistent memory, some significant \nperformance gains can be seen with certain workloads by the reduction in the number \nof round trip completions that software must wait for. This pipeline efficiency gains are \nestimated to yield a 30-50% reduction in round-trip latency for the common database \nSQL Tail-of-Log use case where a large write to persistent memory is followed by an \n8-byte pointer update, to be written only after the first remote write data is considered \nin the persistence domain. The first-generation software remote persistent methods \nrequire two software round-trip completions for the initial SQL data write and again \nfor the small 8-byte pointer update write, as shown in Figure E-1A. In the improved \nnative hardware solution shown in Figure E-1B, software waits for a single round-trip \ncompletion across the network. \nThese performance improvements are coming in a future Intel platform, native \nIntel RDMA-capable NICs, and through industry networking standards. Other vendor’s \nRDMA-capable NICs will also support the improved standard. Broad adoption is \nrequired to allow users of any vendor’s NIC with any vendor’s persistent memory on any \nnumber of platforms. To accomplish this, native persistent memory support is being \ndriven into the standardized iWarp wire protocol by the IETF, Internet Engineering \nTaskforce and the standardized InfiniBand and RoCE wire protocol by the IBTA, \nFigure E-1.  The proposed RDMA protocol changes to efficiently support persistent \nmemory by avoiding Send or Read being called after a Write\nAppendix E  The Future of Remote Persistent Memory Replication",
              "tables": "No table support in fitz.",
              "images": [
                "output\\images\\Programming_Persistent_Memory_medium_457_page442_img1.jpeg"
              ],
              "img_summary_files": [
                "output\\images\\img_summary\\Programming_Persistent_Memory_medium_457_page442_img1_summary.json"
              ],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2065.32 sec"
            },
            {
              "page_number": 443,
              "text": "423\nInfiniBand Trade Association. Both protocols track each other architecturally and have \nessentially added an RDMA Flush and RDMA Atomic Write commands to the existing \nvolatile memory support.\nRDMA Flush – Is a protocol command that flushes a portion of a memory region. \nThe completion of the flush command indicates that all of the RDMA Writes within the \ndomain of the flush have made it to the final placement. Flush placement hints allow \nthe initiator software to request flushing to globally visible memory (could be volatile or \npersistent memory regions) and separately whether the memory is volatile or persistent \nmemory. The scope of the RDMA Write data that is included in the RDMA Flush domain \nis driven by the offset and length for the memory region being flushed. All RDMA Writes \ncovering memory regions contained in the RDMA Flush command shall be included in \nthe RDMA Flush. That means that the RDMA Flush command will not complete on the \ninitiator system until all previous remote writes for those regions have reached the final \nrequested placement location.\nRDMA Atomic Write – Is a protocol command that instructs the NIC to write a \npointer update directly into persistent memory in a pipeline efficient manner. This \nallows the preceding RDMA Write, RDMA Flush, RDMA Atomic Write, and RDMA \nFlush sequence to occur with only one single complete round-trip latency incurred by \nsoftware. It simply needs to wait for the final RDMA Flush completion.\nPlatform hardware changes are required to efficiently make use of the new network \nprotocol additions for persistent memory support. The placement hints provided in the \nRDMA Flush command allows four possible routing combinations:\n•\t\nCache Attribute\n•\t\nNo-cache Attribute\n•\t\nVolatile Destination\n•\t\nPersistent memory destination\nThe chipset, CPU, and PCIe root complexes need to understand these placement \nattributes and steer or route the request to the proper hardware blocks as requested.\nOn upcoming Intel platforms, the CPU will look at the PCIe TLP Processor Hint \nfields to allow the NIC to add the steering information to each PCIe packet generated \nfor the inbound RDMA Writes and RDMA Flush. The optional use of this PCIe steering \nmechanism is defined by the PCIe Firmware Interface in the ACPI specification and \nallows NIC kernel drivers and PCI bus drivers to enable the IO steering and essentially \nAppendix E  The Future of Remote Persistent Memory Replication",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2074.95 sec"
            },
            {
              "page_number": 444,
              "text": "424\nselect cache, no-cache as memory attributes, and persistent memory or DRAM as the \ndestination.\nFrom a software enabling point of view, there will be changes to the verbs definition \nas defined by the IBTA. This will define the specifics of how the NIC will manage and \nimplement the feature. Middleware, including OFA libibverbs and libfabric, will be \nupdated based on these core additions to the networking protocol for native persistent \nmemory support.\nReaders seeking more specific information on the development of these persistent \nmemory extensions to RDMA are encouraged to follow the references in this book \nand the information shared here to begin a more detailed search on native persistent \nmemory support for high-performance remote access. There are many new exciting \ndevelopments occurring on this aspect of persistent memory usage.\nAppendix E  The Future of Remote Persistent Memory Replication",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2075.46 sec"
            },
            {
              "page_number": 445,
              "text": "425\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1\n\u0007Glossary\nTerm\nDefinition\n3D XPoint\n3D Xpoint is a non-volatile memory (NVM) technology developed jointly by Intel and \nMicron Technology.\nACPI\nThe Advanced Configuration and Power Interface is used by BIOS to expose platform \ncapabilities.\nADR\nAsynchronous DRAM Refresh is a feature supported on Intel that triggers a flush of \nwrite pending queues in the memory controller on power failure. Note that ADR does \nnot flush the processor cache.\nAMD\nAdvanced Micro Devices https://www.amd.com\nBIOS\nBasic Input/Output System refers to the firmware used to initialize a server.\nCPU\nCentral processing unit\nDCPM\nIntel Optane DC persistent memory\nDCPMM\nIntel Optane DC persistent memory module(s)\nDDR\nDouble Data Rate is an advanced version of SDRAM, a type of computer memory.\nDDIO\nDirect Data IO.  Intel DDIO makes the processor cache the primary destination and \nsource of I/O data rather than main memory. By avoiding system memory, Intel DDIO \nreduces latency, increases system I/O bandwidth, and reduces power consumption \ndue to memory reads and writes.\nDRAM\nDynamic random-access memory\neADR\nEnhanced Asynchronous DRAM Refresh, a superset of ADR that also flushes the CPU \ncaches on power failure.\n(continued)",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2077.10 sec"
            },
            {
              "page_number": 446,
              "text": "426\nTerm\nDefinition\nECC\nMemory error correction used to provide protection from both transient errors and \ndevice failures.\nHDD\nA hard disk drive is a traditional spinning hard drive.\nInfiniBand\nInfiniBand (IB) is a computer networking communications standard used in high-­\nperformance computing that features very high throughput and very low latency. It is \nused for data interconnect both among and within computers. InfiniBand is also used \nas either a direct or switched interconnect between servers and storage systems, as \nwell as an interconnect between storage systems.\nIntel\nIntel Corporation https://intel.com\niWARP\nInternet Wide Area RDMA Protocol is a computer networking protocol that \nimplements remote direct memory access (RDMA) for efficient data transfer over \nInternet Protocol networks.\nNUMA\nNonuniform memory access, a platform where the time to access memory depends \non its location relative to the processor.\nNVDIMM\nA non-volatile dual inline memory module is a type of random-access memory for \ncomputers. Non-volatile memory is memory that retains its contents even when \nelectrical power is removed, for example, from an unexpected power loss, system \ncrash, or normal shutdown.\nNVMe\nNon-volatile memory express is a specification for directly connecting SSDs on PCIe \nthat provides lower latency and higher performance than SAS and SATA.\nODM\nOriginal Design Manufacturing refers to a producer/reseller relationship in which the \nfull specifications of a project are determined by the reseller rather than based on \nthe specs established by the manufacturer.\nOEM\nAn original equipment manufacturer is a company that produces parts and \nequipment that may be marketed by another manufacturer.\nOS\nOperating system\nPCIe\nPeripheral Component Interconnect Express is a high-speed serial communication bus.\nPersistent \nMemory\nPersistent memory (PM or PMEM) provides persistent storage of data, is byte \naddressable, and has near-memory speeds.\n(continued)\nGLOSSARY",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2084.24 sec"
            },
            {
              "page_number": 447,
              "text": "427\nTerm\nDefinition\nPMoF\nPersistent memory over fabric\nPSU\nPower supply unit\nRDMA\nRemote direct memory access is a direct memory access from the memory of one \ncomputer into that of another without involving the operating system.\nRoCE\nRDMA over Converged Ethernet is a network protocol that allows remote direct \nmemory access (RDMA) over an Ethernet network.\nQPI\nIntel QuickPath Interconnect is used for multi-socket communication between CPUs.\nSCM\nStorage class memory, a synonym for persistent memory.\nSSD\nSolid-state disk drive is a high-performance storage device built using non-volatile \nmemory.\nTDP\nA thermal design point specifies the amount of power that the CPU can consume and \ntherefore the amount of heat that the platform must be able to remove in order to \navoid thermal throttling conditions.\nUMA\nUniform memory access, a platform where the timne to access memory is (roughly) \nthe same, regardless of which processor is doing the access. On Intel patforms, this \nis achieved by interleaving the memory across sockets.\nGlossary",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2086.40 sec"
            },
            {
              "page_number": 448,
              "text": "429\n© The Author(s) 2020 \nS. Scargall, Programming Persistent Memory, https://doi.org/10.1007/978-1-4842-4932-1\nIndex\nA\nACPI specification, 28\nAddress range scrub (ARS), 338\nAddress space layout randomization \n(ASLR), 87, 112, 316\nAppliance remote replication  \nmethod, 355, 357\nApplication binary interface (ABI), 122\nApplication startup and recovery\nACPI specification, 28\nARS, 29\ndirty shutdown, 27\nflow, 27, 28\ninfinite loop, 28\nlibpmem library, 27\nlibpmemobj query, 27\nPMDK, 29\nRAS, 27\nAsynchronous DRAM  \nRefresh (ADR), 17, 207\nAtomicity, consistency, isolation, and \ndurability (ACID), 278\nAtomic operations, 285, 286\nB\nBlock Translation Table (BTT)  \ndriver, 34\nBuffer-based LRU design, 182\nC\nC++ Standard limitations\nobject layout, 122, 123\nobject lifetime, 119, 120\nvs. persistent memory, 125, 126\npointers, 123–125\ntrivial types, 120–122\ntype traits, 125\nCache flush operation (CLWB), 24, 59, 286\nCache hierarchy\nCPU\ncache hit, 15\ncache miss, 16\nlevels, 14, 15\nand memory controller, 14, 15\nnon-volatile storage devices, 16\nCache thrashing, 374\nChunks/buckets, 188\nCLFLUSHOPT, 18, 19, 24, 208, 247, 353\nclose() method, 151\ncloseTable() method, 268\nCLWB flushing instructions, 208\ncmap engine, 4\nConcurrent data structures\ndefinition, 287\nerase operation, 293\nfind operation, 292\nhash map, 291, 292\ninsert operation, 292, 293",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2094.95 sec"
            },
            {
              "page_number": 449,
              "text": "430\nordered map\nerase operation, 291\nfind operation, 288, 289\ninsert operation, 289–291\nCollision, 194\nCompare-and-exchange (CMPXCHG) \noperation, 286\nConcurrent data  \nstructures, 286–287\nconfig_setup() function, 145, 149\nContent delivery networks  \n(CDN), 263\nCopy-on-write (CoW), 192, 193\ncount_all() function, 150\ncreate() abstract method, 266\nD\nData at rest, 17\nData in-flight, 17\nData Loss Count (DLC), 342–346\nData structure\nhash table and transactions, 194\npersistence, 197, 200–202\nsorted array, versioning, 202–206\nData visibility, 23\nDAX-enabled file system, 179, 184\nDB-Engines, 143\ndeleteNodeFromSLL(), 273\ndeleteRowFromAllIndexedColumns() \nfunction, 273\ndelete_row() method, 272\nDirect access (DAX), 19, 66\nDirect Data IO (DDIO), 352\nDirect memory access (DMA), 12, 347\nDirty reads, 233\nDynamic random-access memory \n(DRAM), 11, 155\nE\nEcosystem, persistent containers\nbegin() and end(), 138\nimplementation, 134\niterating, 136, 138\nmemory layout, 134\nsnapshots, 138\nstd::vector, 135, 136\nvector, 135\nEnhanced Asynchronous DRAM  \nRefresh (eADR), 18\nError correcting codes (ECC), 333\nerrormsg() method, 150\nexists() method, 150\nExternal fragmentation, 177, 188\nF\nFence\ncode, 21, 22\nlibpmem library, 23\nPMDK, 22\npseudocode, 21\nSFENCE instructions, 23\nflush() function, 217, 242\nFlushing\nmsync(), 20\nnon-temporal stores, 19\noptimized flush, 19\ntemporal locality, 19\nFragmentation, 187\nfunc() function, 237\nG\nGeneral-purpose remote replication \nmethod (GPRRM)\nperformance implications, 355\npersistent data, 354, 355\nConcurrent data structures (cont.)\nIndex",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2104.29 sec"
            },
            {
              "page_number": 450,
              "text": "431\nRDMA Send request, 353, 354\nsequence of operation, 353, 354\nSFENCE machine instruction, 353\nget_above() function, 151\nget_all() method, 4\nget() function, 150, 197, 201\nget/put interfaces, 7\nGuarantee atomicity/consistency\nCoW/versioning, 192, 193\ntransactions, 189–192\nH\nHeap management API\nallocating memory, 165, 166\nfreeing allocating memory, 166\nHigh bandwidth memory (HBM), 156\nHigh-performance appliance remote \nreplication method, 352\nI\nincrement() function, 278, 281\nindex_init() method, 275\nInfiniBand, 348\nIn-memory databases (IMDB), 177\nIntel Inspector, 212\nIntel Inspector–Persistence  \nInspector, 210\nIntel machine instructions, 24, 25\nIntel Memory Latency Checker  \n(Intel MLC), 304\nIntel Threading Building Blocks  \n(Intel TBB), 168\nInternal fragmentation, 177, 188\nInternet of Things (IoT), 263\nInternet Wide Area RDMA Protocol \n(iWARP), 348\nipmctl show–topology command, 381\nisPrimaryKey()function, 270\nJ\nJava, 411\nheap memory allocation, 412–414\nLLPL, 418–419\nnon-volatile mapped byte buffers, \n415–416\npartial heap allocation, 414–415\nPCJ, 416–418\nJava Development Kit (JDK), 411\nK\nkey-value pairs, 4\nKey-value store, 142\npersistent memory, 5, 6\nstorage, 6\ntraditional storage, 5\nKind configuration management, 167\nkvprint(), 4, 6\nL\nlibmemkind vs. libvmemcache, 180\nlibpmem, 50\nC code examples, 73\ncopying data, 76, 77\nCPU instructions, 73\nflushing, 77, 78\nheader, 74, 75\nmemory mapping files, 75\nlibpmemblk, 69\nlibpmemkv library, 2–4, 69\ncomponents, 8, 9\nsoftware stack, 8\nIndex",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2109.64 sec"
            },
            {
              "page_number": 451,
              "text": "432\nlibpmemlog, 69\nlibpmemobj, 141\nlibpmemobj architecture\nACID transactions, 328, 329\natomic and transactional APIs, 315\ninternal allocator interface, 323, 324\nmemory allocation, 324–328\nMemory OID\nASLR, 316\nconvert pointer, PMEMoid, 318\npointer performance  \nproblems, 316, 317\npointer requirements, 316\nmodules, 313, 314\nOID, 314\nPMEMoids, 315\npower fail atomicity\nredo logging, 320, 321\nUndo logging, 321\nstore runtime (volatile) state, 330\nthread local storage,  \nusing lanes, 318, 319\nlibpmemobj library, 359\nallocate memory, 93\ndata APIs\natomic operations, 94, 95, 97\nbenefits, 104\noptinal flags, 104\nreserve/publish, 97–100\ntransactional, 100–102, 104\ndebugging and error handling, 106–108\ndefining, 81\nmemory pools\npmemobj_create(), 84, 86–88\npmempool utility, 83\nread and display string, 88, 90\nmemory poolset, 90–91\npmemobj_set_funcs() function, 106\npmempool utility features, 92\nTOIDs, 92, 93\nlibpmemobj-cpp, 68\nbindings, 128\ndefinition, 111\ngoal, 111\nlibrpmem architecture\nactive-passive replication  \narchitecture, 359\ncomponents, 360, 361\nerror handling, 364\nHello World, 369, 370\nHello World,  \nreplication, 364, 366, 367\nlatency, 362\nlibpmemobj library, 359\npoolset, 362\npull model, 363\nRDMA architecture, 360\nremote_open routine, 368, 369\nremote persistent memory, 363\nrpmemd program, 359\nsynchronous replication model, 363\nlibsafec, 404\nlibvmem, 67\nlibvmemcache, 66, 178\napplication, 179\ncharacteristics, 178\ndesign\naspects, 180\nextent-based allocator, 180, 181\nnoncontiguous allocations, 181\nfunctions, 183\nworking, 179\nLinux Volume Manager (LVM), 384\nloadIndexTableFromPersistentMemory() \nfunction, 267\nLow-Level Persistence Library (LLPL), 418\nIndex",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2110.23 sec"
            },
            {
              "page_number": 452,
              "text": "433\nM\nMachine check exception (MCE), 334\nmain() function, 213, 234\nMAP_SYNC flag, 385\nMariaDB∗ storage engine\narchitecture, 264\ncreation\ndatabase table, 266, 267\ndatabase table, closing, 268\ndatabase table,  \nopening, 267, 268\ndata structure, 266\nDELETE operation, 272–274\nhandle commits and  \nrollbacks, 265, 266\nINSERT operation, 268–270\nnew handler instance, 265\nSELECT operation, 275, 276\nUPDATE operation, 270, 271\nstorage layer, 264\nmemkind API functions, 159\nfixed-size heap creation, 160, 161\nkind creation, 160\nkind detection, 162, 163\nmemory kind detection API, 163\nvariable size heap creation, 162\nmemkind_config structure, 161\nmemkind_create_pmem()  \nfunction, 160, 169\nmemkind_create_pmem_with_config() \nfunction, 161, 164\nmemkind_destroy_kind()  \nfunction, 164\nmemkind_detect_kind() function, 163\nmemkind_free() function, 166\nmemkind library, 156, 157\nmemkind_realloc() functions, 165\nMemory\ncapacity, 11\ncharacteristics, 12, 13\nkinds of, 158\nleaked object, 213\nleaks, 209\nMemory management unit (MMU), 49\nMetaprogramming\nallocating, 116–118\ndefinition, 112\npersistent pointers, 112, 113\nsnapshots, 115, 116\ntransactions, 113, 114\nmtx object, 282\nMultiple Device Driver (mdadm), 384\nMutexes\nlibpmemobj library, 283\nmain() function, 285\nstd::mutex, 282\nsynchronization primitives, 282–284\nN\nndctl and daxctl, installation\nLinux distribution package repository\nPMDK on Fedora 22, 393\nPMDK on RHEL and  \nCentOS, 393–394\nPMDK on SLES 12 and  \nOpenSUSE, 394\nPMDK on Ubuntu 18.04, 394\nsearching, packages, 391–392\nprerequisites, 389–390\nndctl utility, 376\nNetwork interface controller (NIC), 349\nNon-maskable interrupt (NMI), 18\nNonuniform memory access  \n(NUMA), 65, 156\nautomatic balancing, 382, 383\nIndex",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2114.88 sec"
            },
            {
              "page_number": 453,
              "text": "434\nBIOS options, 382\nCPU, 373\ndefined, 373\nIntel MLC, 378, 379\nipmctl utility, 381\nndctl utility, 376, 378\nnumastat utility, 380\ntwo-socket system, 373, 374\nNonuniform Memory Architecture \n(NUMA), 305\nNon-volatile memory express  \n(NVMe), 156\nnumactl utility, 374\nnumastat utility, 380\nNVDIMM driver, 33\nNVDIMM Firmware Interface Table \n(NFIT), 343\nO\nObject identifier (OID), 314\nopen()method, 267\nOperating systems\nmemory direct access (DAX)\nbenefits, 49\nI/O-accessed storage, 50\nlibpmem, 50\nLinux, locating, 48\nphysical devices, regions, and \nnamespaces,  \ndisplaying, 44–46, 48\npmem_map_file function, 52\npmem_persist function, 53\nprogramming example, 51\nsystem administrator, 43\nWindows, locating, 49\nmemory-mapped files\non Linux, 36, 37\nwith storage, 42\non Windows, 38, 39, 41\npersistent memory, block storage, 33\npersistent memory, file systems, 34\nOptimized flush instructions, 355\nOrdering\nnodes, 20, 21\nsteps, 20\nP\npaintball_init() function, 97\nPatrol scrub, 337\nPerformance difference, 6\nPersistent libraries, 64\nPersistent memory, 173\nadvantages, 262\napplication, 176\ndata structures, 188, 189\nselective, 193\nsnapshotting  \nperformance, 190, 191\nuses, 262, 263\nPersistent Memory Development Kit \n(PMDK), 1, 13, 111, 207, 261, 307\npersistent libraries, 63\nlibpmem, 67, 68\nlibpmemblk, 69\nlibpmemkv, 69\nlibpmemlog, 69\nlibpmemobj, 68\nlibpmemobj-cpp, 68\ntools and command utilities\npmemcheck utility, 70, 71\npmempool utility, 70\npmreorder utility, 71\nNonuniform memory access  \n(NUMA) (cont.)\nIndex",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2120.61 sec"
            },
            {
              "page_number": 454,
              "text": "435\nvolatile libraries\nlibmemkind, 65\nlibvmem, 67\nlibvmemcache, 66\nPersistent memory programming\napplication’s responsibilities, 59\nAMD64 architectures, 247\nanalyzing process, 257\natomic persistent store, 56\nCLFLUSH instruction, 217, 247\ncode modification, 227\ndata collection, 254\ndata() function, 249\ndata structure, 252, 253, 256\ndependency, flush, 244\nflag/data variables, 224\nFlushFileBuffers on Windows, 59\nGUI of Intel Inspector, 225, 226\nhardware configuration, 60\nIntel inspector, 219, 220, 222, 223\nmemory overwriting, 240–242\nmemory variables, 249\nnonpersistent stores, 214\nout-of-order issue, 249–251\noutput_file.log, 258, 259\npersistent memory, 220, 221\npmemcheck code, 215, 217, 219\nreader program, 245, 246\nredundant flushing, 242, 243\nreordering mechanism, 255\nresident data structures, 55\nSFENCE instruction, 247, 248\nstorage-resident data structures, 56\ntransactions\natomicity, 57\nconsistency, 58\ndatabase system, 233\ndurability, 58\nisolation, 58\nfunc() function, 237\nlambda functions, 235\nmultithreaded applications, 235\nmy_root data structure, creation, 229\nobject adding, 231\npersistence inspector, 230, 233\npmemcheck code, 229, 232\nrecovery operation, 235\nreport, 239\nrollback mechanism, 234\nthreads, 236\nValgrind macros, 216, 217\nvalid flag approach, 219\nPersistent queue implementation, 129\nPlatform capabilities, 25, 26\nPlatform support, 13\nPMDK, installation\nLinux distribution package repository\nlibraries, PMDK v1.6, 396\nnaming convention, 396\nPMDK on Fedora 22, 399\nPMDK on Microsoft Windows, 402\nPMDK on RHEL and  \nCentOS 7.5, 400\nPMDK on SLES 12 and  \nOpenSUSE, 400–401\nPMDK on Ubuntu 18.04, 401\nsearching packages,396–398\nprerequisites, 395\npmem::allocator class template, 169–171\npmem_allocator.h header file, 170\npmemcheck utility, 70, 208\n.pmeminspdata directory, 222\nPersistent memory (PMEM_KIND), 158\nPmemkv architecture\ncharacteristics, 144\ncreation, 143\nIndex",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2127.78 sec"
            },
            {
              "page_number": 455,
              "text": "436\ndirect access, 144, 145\nJavaScript bindings, 151, 152\nphonebook example, 147–151\npmemkv_config structure, 145\npmemkv_config.h, 146\nprogramming languages, 144\npmemkv_config structure, 145\npmem::kv::db class, 149, 150\npmemkv_errormsg() function, 145\npmem_map_file function, 52\npmem_memcpy_persist()  \nfunction, 77\npmemobj pool, 197\npmemobj_close() function, 268\npmem::obj:: condition_variable  \nclass, 283\npmemobj_create() function, 84, 266\npmemobj_direct() function, 87\npmemobj_errormsg() function, 106\npmem::obj::mutex class, 283\npmemobj_open() function, 267\npmem::obj::persistent_ptr  \nclass, 113, 125\npmem::obj::pool_base, 130\npmemobj_root() function, 87\npmemobj_set_funcs()  \nfunction, 106\npmem::obj::shared_mutex class, 283\npmem::obj::timed_mutex class, 283\npmem::obj::v<T> class, 284\npmem_persist function, 53\npmempool utility, 70, 83\npmreorder utility, 71\npop() method, 130\nPower-fail protected domains, 16\nADR and eADR, 18\nchallenge, 18\nSFENCE operation, 18\nSNIA NVM programming model, 17\nProfiling and performance optimization \ntechniques\ncomputer bound vs memory  \nbound, 295\ndefault memory page size, 311\nDRAM-only system, 302\nguided data placement, 307\nIntel MLC, 304\nI/O bound, 297\nmemory access optimization, 308\nmemory access pattern, 296\nmemory-capacity constraints, 308\nmemory latency vs memory  \ncapacity, 296\nNUMA\naddressable memory capacity, 306\nbandwidth requirements, 307\nBIOS options, 307\ndata allocation vs access, 309\nQPI/UPI, 305\nthread migration, 310\ntuning hardware, 306\npersistent memory technologies\nuse cases, memory-capacity  \nbound, 298\nuse cases, VTune  \nProfiler, 301, 302\nuse cases, WSS, 300, 301\nread vs. write performance, 296\nread–write ratio, 305\nworking set size and memory  \nfootprint, 305\nworkload characteristics, 303\npull requests, 142\npush() method, 129\nput() method, 150\nPmemkv architecture (cont.)\nIndex",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2134.75 sec"
            },
            {
              "page_number": 456,
              "text": "437\nQ\nQueue implementation, 126, 128\nQuickPath Interconnect (QPI)/Ultra Path \nInterconnect (UPI), 305\nR\nRDMA networking protocols\ncommands, 350\nNIC, 349\nRDMA Read, 350\nRDMA Send (and Receive), 350, 351\nRDMA Write, 350\nRDMA over Converged Ethernet  \n(RoCE), 348\nRedis, 143\nRedo logging, 320\nReliability, Availability, and  \nServiceability (RAS)\ndevice health\nACPI NFIT, 343\nACPI specification, 342\nunsafe/dirty shutdown, 343\nusing ndctl to query, 340, 341\nvendors, 342\nECC\ninifinite loop, 334\nMCE, 334\nusing Linux, 335, 336\nunconsumed uncorrectable error \nhandling\nARS, 338\nclearing errors, 339\nmemory root-device notification, 338\npetrol scrub, 337\nruntime, 337\nunsafe/dirty shutdown,  \nDLC counter, 344, 345\nReliability, availability,  \nserviceability (RAS), 27\nRemote direct memory access  \n(RDMA), 12, 347, 348\nsoftware architecture, 357, 358\nremote_open routine, 368, 369\nremove() method, 150\nResource acquisition is initialization \n(RAII), 113\nrpmem_drain(), 363\nrpmem_flush(), 363\nS\nshow() method, 131\nSingle instruction, multiple data (SIMD) \nprocessing, 190\nSingle linked list (SLL), 266\nSnapshotting optimization, 196\nSNIA NVM programming  \nmodel, 351\nSolid-state disk (SSD), 1, 156\nStack/buffer overflow bug, 208\nStackoverflow app, 211\nStandard Template Library  \n(STL), 168, 282, 287\nstd::is_standard_layout, 122, 123\nStorage and Networking Industry \nAssociation (SNIA), 33, 112\nsymmetric multiprocessing (SMP)  \nsystem, 373\nT\nThread migration, 310\nTransactions and multithreading\ncounter, 281\nillustrative execution, 281\nIndex",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2140.38 sec"
            },
            {
              "page_number": 457,
              "text": "438\nincorrect synchronization, 279, 280\nlibpmemobj, 280\nPMDK transactions,  \n278, 279, 281, 282\nTranslation lookaside buffer  \n(TLB), 187, 311, 325\nTree insert operation, 204\nTyped Object Identifiers  \n(TOIDs), 92\nU\nUndo logging, 321\nUninterruptable power supply  \n(UPS), 18\nupdate_row() method, 270\nV\nValgrind tools, 208, 209\nVector of strings, 171\nVersioning, 193\nvmemcache_add() function, 185\nvmemcache_get() function, 185\nVolatile libraries, 64\nVolume managers\nadvantages, 383\nLinux architecture, 384\nmdadm, 384\nNUMA systems, 383\nW, X, Y, Z\nWrite pending queue (WPQ), 18\nwrite_row() method, 268\nTransactions and multithreading (cont.)\nIndex",
              "tables": "No table support in fitz.",
              "images": [],
              "img_summary_files": [],
              "img_vision_files": [],
              "summary": "Summary not available due to an error.",
              "time_taken": "2143.96 sec"
            }
          ],
          "overall_summary": "PDF extraction complete.",
          "total_time_taken": "2143.97 sec"
        }
      }
    ]
  }
}