{
  "metadata": {
    "file_name": "TDGPT_Documentation.docx",
    "file_type": "word",
    "page_count": 1
  },
  "pages": [
    {
      "page_number": 1,
      "text": "TDGPT - PROJECT DOCUMENTATION\nProject Introduction\nThis AI-powered Document Parser automatically extracts, summarizes, and organizes content from various document types using Large Language Models (LLMs) and traditional document processing libraries.\nMethods Overview\nMethod 1: Custom Parsing (Without Unstructured)\nMethod 2: Using Unstructured Partitioning\nMethod 1: Custom Parsing (Without Unstructured)\nProject Objectives\nExtract text, tables, and images from PDFs, Word docs, and PowerPoint files.\nPerform chunk-wise summarization using LLMs.\nGenerate structured JSON outputs.\nStore extracted images in local folders.\nProcess native and scanned documents with OCR support.\nTechnologies Used\nPython: Core programming language\npdfplumber: Text and table extraction from PDFs\nPyMuPDF (fitz): Image extraction from PDFs\npytesseract: OCR for scanned PDFs\npython-docx: Word document parsing\npython-pptx: PowerPoint content extraction\nPillow (PIL): Image manipulation\nTesseract OCR: Optical character recognition\nGroq API / OpenAI / Llama3: LLMs for summarization\nLLM Comparison\nGPT-3.5/4 - OpenAI: High accuracy and broad support.\nLlama3-70B - Groq: Fast inference and cost-effective.\nSupported File Formats and Tools\nPDF: pdfplumber, fitz, pytesseract\nDOCX: python-docx\nPPTX: python-pptx\nImages: Pillow\nChunking Logic\nType: Word-based chunking\nChunk Size: CHUNK_SIZE = 300 words\nWhy: Optimized for LLM context window & summarization granularity\nPerformance (with LLM):\nSample Output JSON Structure\n{\n  \"metadata\": {\n    \"file_name\": \"example.pdf\",\n    \"file_type\": \"pdf\",\n    \"file_size\": \"1.5 MB\",\n    \"page_count\": 10\n  },\n  \"pages\": [\n    {\n      \"page_number\": 1,\n      \"text\": \"This is the extracted text...\",\n      \"tables\": [\"Table content as text...\"],\n      \"image_paths\": [\"output/images/example_page1_img1.png\"],\n      \"summary\": \"This page discusses the overview of...\"\n    }\n  ],\n  \"overall_summary\": \"Document provides a comprehensive overview of...\",\n\"total_time_taken\": \"25.6 seconds\"\n}\n\nFeatures Recap\nHandles PDF, DOCX, PPTX\nExtracts text, tables, and images\nOCR-enabled for scanned files\nChunk-wise LLM summarization\nStructured JSON output\nOptimized for large-scale document processing\nSample Output\nPros:\nAccurate table extraction via pdfplumber\nReliable image saving using fitz\nFull control of chunking and layout\nWorks consistently across large documents\nEasier debugging and optimization\nCons:\nSlightly longer initial setup\nRequires manual format detection\nBenefits\n\uf0b7   Multi-Format Support: Handles PDFs, Word (DOCX), and PowerPoint (PPTX) files.\n\uf0b7   Smart Summarization: Uses LLMs (like Groq LLaMA 3) to summarize content in chunks.\n\uf0b7   Image & Table Extraction: Automatically detects and saves images and tables.\n\uf0b7   OCR Enabled: Extracts text from scanned PDFs using Tesseract OCR.\n\uf0b7   Structured Output: Delivers clean JSON output for integration and analysis.\n\uf0b7   Offline Ready: Can work locally (except for LLMs), ideal for secure environments.\nLimitations\nLLM Dependency: Summarization needs an API key and internet access.\nOCR Accuracy: Depends on scan quality; poor images may reduce accuracy.\nLayout Loss: Does not preserve original formatting or styles.\nProcessing Time: Large files may take time to process.\nComplex Tables: Might struggle with nested or irregular tables.\nLanguage Support: Primarily optimized for English documents.\nMethod 2: Using Unstructured Partitioning\nTechnologies Used\n\u2022 Python \u2013 Core programming language\n\u2022 os, json, pandas \u2013 File handling and structured output\n\u2022 python-dotenv \u2013 Load environment variables\n\u2022 tqdm \u2013 Show progress during processing\n\u2022 unstructured \u2013 Extract text, tables, images from documents\n\u2022 pytesseract \u2013 OCR via Tesseract\n\u2022 Tesseract OCR \u2013 Recognize text in scanned PDFs\n\u2022 Groq API (LLaMA 3 70B) \u2013 Summarize extracted content using LLM\nLLM Comparison Summary\nSupported File Formats & Partition Modules\nPDF\nModule Used: unstructured.partition.pdf\nExtra Option: ocr=True for scanned PDFs\nOCR Engine: pytesseract\nUse: Extracts narrative text, tables, and embedded images from PDFs (including scanned).\nDOCX\nModule Used: unstructured.partition.docx\nUse: Extracts paragraphs, titles, and lists from Word files.\nXLSX\nModule Used: unstructured.partition.xlsx\nUse: Extracts table content, headers, and sheet metadata.\nPPTX\nModule Used: unstructured.partition.pptx\nUse: Extracts titles, bullet points, speaker notes.\nHTML\nModule Used: unstructured.partition.html\nUse: Parses and extracts main text and structured sections.\nPerformance\nOutput JSON Structure\n{\n\"file\": \"sample.pdf\",\n\"summary\": \"This document covers Big Data Analytics, architectural frameworks, real-time applications...\",\n\"text_elements\": [\n{\n\"type\": \"NarrativeText\",\n\"text\": \"Big data refers to datasets that are too large or complex...\",\n\"metadata\": {...}\n}\n],\n\"tables\": [\n{\n\"type\": \"table\",\n\"text\": \"Framework | Description\\nHadoop | Distributed storage...\",\n\"metadata\": {...}\n}\n],\n\"images\": [\n{\n\"type\": \"image\",\n\"text\": \"\",\n\"metadata\": {\n\"page_number\": 4,\n\"coordinates\": [100, 200, 300, 400]   }   }\n]\n}\nFeatures Summary\nAutomatic file type detection\nAI-based summarization (LLM via Groq)\nStructured output for text, tables, and images\nExport to JSON and CSV\nOCR support for scanned documents\nSample Execution Output\nA progress bar showing :\nProcessing files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:08<00:00, 2.56s/it]\n\u2713 Successfully processed: BigData.pdf\n\u2713 Successfully processed: Report.docx\n\u2713 Successfully processed: Slides.pptx\nOutput saved in /output/\nPros\n\uf0b7   Handles multiple file types and extracts text, tables, and images\n\uf0b7   Uses OCR for scanned PDFs\n\uf0b7   Provides clear summaries with Groq LLaMA 3 70B\n\uf0b7   Outputs structured JSON and CSV files\n\uf0b7   Includes progress tracking for batch processing\nCons\n\uf0b7   OCR depends on scan quality it\u2019s not properly extract images and tables format\n\uf0b7   Large files can take longer to process\n\uf0b7   Limited support for complex tables and media\n\uf0b7   Requires proper Tesseract setup\n\uf0b7   Summarization quality varies with model and input\n\uf0b7   Only supports specific file formats out of the box",
      "tables": [
        "File Size | Pages | Time Taken\nSmall | 30\u201350 | ~32\u201350sec\nMedium |  | \nLarge | 600+ | ~180\u2013240 sec",
        "Model | Speed (on Groq) | Token Limit | Accuracy | Cost Efficiency | Used in Project\nGPT-4 Turbo | Moderate | 128k | High | Costly | No\nClaude 3 Opus | High | 200k | High | Moderate | No\nLLaMA 3 70B | Very High | 8k | High | High | Yes",
        "File Size | Pages | Time Taken\nSmall | 30\u201350 | ~15\u201322sec\nMedium |  | \nLarge | 600+ | ~180\u2013240 sec"
      ],
      "image_paths": [
        "J File Edit Selection View Go Run Terminal Help \u20ac0 P UnstructuredProject By oo = ia) x\n\noO EXPLORER ote @ env \u00ae unstructured_groq_pipeline.py B summaries.csv {} Big Data Analytics by RaJ Kamal, Preethi Saxena,TMH_part9.pdfjson X  \u00ae test.py oO-\n> OPEN EDITORS output > {} Big Data Analytics by RaJ Kamal, Preethi Saxena,IMH_part9.pdf,json > ...\n& > OUTLINE 1 {\nvuns.. REO @ 2 \"file\": \"Big Data Analytics by RaJ Kamal, Preethi Saxena, TMH_part9.pdf\",\n> _pyeache 3 \"summary\": \"Here is a brief summary of the document: \\n\\nThe document appears to be a study guide or textbook chapter on data analytics, big data\np \u2014 4 \u201ctext_elements\": [\n> myenv 5\ngo output 6 \"Title\",\n{} Big Data Analytics b... 7 : \"sample(), union(), crossProduct().\",\nB summaries.csv 8 \"metadata\": {\n= 5 ;\nJ | > temp_workspace 9 genrianeties\n& env 10 \u201cpoints\u201d: i\nA \u00ae dispatch_partition.py \u201c c\nx 12 172.0,\n= requirements.txt 2B 122.0\ng ' sample.png 14 L\n\u00ae summarize_groq.py a5\n\u00ae unstructured_groq_pi... .\n18 1,\n19 [\n20 618.0,\n21 154.0\n22 1,\n23 -\n24\n25\n26\n27\n28 \u201cPixelSpace\u201d,\n29 \u201clayout_width\": 1443,\n30 \u201clayout_height\": 1860\n31\n32 pplication/pdf\",\n33\n(2) 34\n35\n\u00a73 36 \u201clast_modified\u201d: \"2025-05-28118:47:23\",\n> TIMELINE 37 \u201cpage number\u201d: 1, \"\nBH @0A0 Ln1,Col1 Spaces:2 UTF-8 CRLF {3 JSON & @Golive wWPrettier\n\n2s\nMostly cloudy\n\nAP meoCaeePCmmede +466 c0m yt",
        "J File Edit Selection View Go Run Terminal Help \u20ac0 PP TDGPT By oo = ia) x\n\nEXPLORER ote @ utils.ipynb \u00ae config.ipynb \u00ae mainipynb \u20183 50 page sample PDFindd.pdf_page1_img1.png X \u2014 @ extractor.ipynb @ env = requirements.tt O --\n> OPEN EDITORS Data_Extraction > pdf_Extraction > output > images > \u201ci 50 page sample PDF.indd.pdf_page1_img1.png\n> OUTLINE\n\nver RED A\nY Data Collection\n> Large\n> Medium the B E STor\n> small\n~ Data_Extraction\nY pdf_Extraction\n\n> _pycache_\n> Data_Extraction\n\nCe OY GD\n\n}\n\u201cO\n\nY output\nWORST WRITING\n\u2018G50 page sample...\n' 130725essentia...\n\u2018Gi 130725essentia...\n\u2018Gi CYBER SECURIT...\n\u2018Gi CYBER SECURIT...\n\u2018Gi CYBER SECURIT...\n{} 50 page sample ...\n{} 130725essentiaex...\n{} CYBER SECURITY ...\n{} MIPS Assembly L...\n\ni) b\n\n@\n\n@ config.ipynb\n\u00ae extractor.ipynb\n@ mainipynb\n\n@ utils.ipynb\n@ env\n\n> myenv CONOR\nrequirements. &\n@ __F reativementsot 'y  FRUHLINGER\n\nie > TIMELINE\n\nBH @0A0 Whole Image 400x620 \u20ac8 261.50KB @Golive O\n\n2 Peeerreeenn = Q Search a | @ e Ga @ Dy e e ba a & \u201c66 iN eno anisen",
        "J File Edit Selection View Go Run Terminal Help \u20ac7 PP TDGPT By oo = ia) x\n\noO EXPLORER, ore & config.ipynb \u00ae main.ipynb {} CYBER SECURITY (R18A0521) outputjson X \u2014 @ extractor.ipynb @ env = requirements.txt oO:\n> OPEN EDITORS Data_Extraction > pdf_Extraction > output > {} CYBER SECURITY (R18A0521)_outputjson > ...\na > OUTLINE 1\nv TDGPT 2\nje) \u00a9 BERGaatam 3 CYBER SECURITY (R18A0521).pdf\",\n5 4 pdf\",\nLarge 5 ue 1.64 MB\",\n30 > Medium 6 \u201cpage_count\": 38 -\n> small 7 }\n40 \\ Data_Extraction 8 \u201cpages\u201d: [\n] \\ pdf_Extraction 9 {\n> ao 10 \u201cpage_number\u201d: 1, -\nA 11 \"text\": \"DIGITAL NOTES\\nON\\nCYBER SECURITY\\n(R18A0521)\\nB.TECH III YEAR 8 II SEM (R18) \\n(2@20-2021)\\nDEPARTMENT OF INFORMATION TECHNOLOGY\\nMA\n> Data_Extraction x 5 = =\n12 \u2018tables\": \"No tables found.\",\noutput aa \u201cimages\u201d: [\ng \u00a9 images 14 \u201coutput\\\\images\\\\CYBER SECURITY (R18A@521).pdf_page1_img1. jpeg\u201d\n\u2018G50 page sample... 15 Ib\n') 130725essentia... 16 \u201csummary\u201d: \u201cHere is a 1-2 line summary of the content:\\n\\nThis is a digital note on cyber security for B.Tech III Year - II Semester students\n[ east 7 \u2018time_taken\u201d: \"3.36 seconds\u2019\n18 2\n\u2018Gh CYBER SECURIT... to {\n\u2018Gh CYBER SECURIT... 0) Chega mba e 2,\n\u2018& CYBER SECURIT... 21 \u201ctext\u201d: \"MALLA REDDY COLLEGE OF ENGINEERING AND TECHNOLOGY\\nIII Year B.Tech II Sem L T/P/D C\\n3 -/-/- 3\\n(RA18A@521) CYBER SECURITY\\n(Profess\n{} 50 page sample ... 22 \"tables\": \"No tables found.\",\n{} 130725essentiaex... 23 \u201cimages\": [\n{} CYBER SECURITY ... 24 \u201coutput\\\\images\\\\CYBER SECURITY (R18A@521).pdf_page2_img1. jpeg\u201d\n{} MIPS Assembly L... > L MQ 9 i A i i\n. 26 summary\u201d: \u201cHere is a summary of the content in 2 lines:\\n\\nThe course on Cyber Security covers topics such as types of cyber-attacks, cyber-\n\u00ae config.ipynb cna De =\n27 \u2018time_taken\u201d: \u201c6.33 seconds\u2019\n@ extractor.ipynb 28 }\n@ main.ipynb 29 {\n@ utils.ipynb 30 \u201cpage_number\u201d: 3,\n& ew 31 \u201ctext\u201d: \"UNIT - V\\nPrivacy Issues: Basic Data Privacy Concepts: Fundamental Concepts, Data Privacy\\nAttacks, Datalinking and profiling, priva\n> 32 \u201ctables\nmyenv a5\n. 33 \u2018images\n@ = requirements.txt Pe . . . . . . . . . .\n34 \u2018summary\u2019 Here is a 2-line summary of the content:\\n\\nThis unit focuses on privacy issues and cybercrime, covering basic data privacy conce\n35 \u201ctime_taken\": \"9.12 seconds\u201d\n& * ||.\n> TIMELINE 37 { Pe)\nBY @0A0 Ln1,Col1 Spaces:2 UTF-8 CRLF {3 JSON & @Golive wWPrettier O\n\nTrending videos\nFunny Golden R...\n\nMP mMoCaae PCI B -62 8 e098 yi",
        "ba)\n\noe Rep\n\na5)\n\nI)\n\n@\n\ni}\nss\n\nFile Edit Selection View\n\nEXPLORER ate\n\n> OPEN EDITORS\n\n> OUTLINE\n\n\u2018v UNSTRUCTUREDPROJECT\n> _pycache_\n> myenv\nY output\n\n{} Big Data Analytics b...\n\n& summaries.csv\n\n> temp_workspace\n\n& env\n\n\u00ae dispatch_partition.py\nrequirements.txt\n\n{2 sample.png\n summarize_grog.py\n\u00ae test py\n\n\u00ae unstructured_groq_pi...\n\n> TIMELINE\n@o0Ao\n\n85\u00b0F\nMostly cloudy\n\nGo Run = <5\n\n@ env\n\noutput > Ei summaries.csv\n1 file,summary\n\n\u00ae unstructured_groq_pipeline.py\n\nBo - @ x\n@ test. O --\n\nBy\n\nPP UnstructuredProject\n\nE summaries.csv X  {} Big Data Analytics by RaJ Kamal, Preethi Saxena, TMH_part9.pdfjson\n\n\"Big Data Analytics by RaJ Kamal, Preethi Saxena, TMH_part9.pdf\",\"Here is a brief summary of the document:\nThe document appears to be a study guide or textbook chapter on data analytics, big data, and machine learning. It co\n\nSpark stack components and their applications\n\n2. Data analysis, including data mining, pattern mining, clustering, and anomaly detection\n\n3. Data visualization tools such as Tableau, D3.js, and FusionCharts\n\n4. Machine learning algorithms, including regression analysis, K-nearest neighbors, decision trees, and artificial ne\nData preprocessing, including data cleaning, filtering, and transformation\n\nCreating data pipelines using Spark, including data ingestion, processing, and storage\n\nUsing Python libraries such as NumPy, SciPy, and Pandas for analytics\n\nCreating reports and visualizations using Eclipse BIRT and other tools\n\nMachine learning algorithms for big data analytics, including frequent itemset mining, cluster analysis, classific\n\n16 The document also includes multiple-choice questions, practice exercises, and review of previous chapters.\"\n\n2\n\n3\n\n4\n\n5\n\n6 ils\n\n7\n\n8\n\n9\n10 Eo\npicts 6.\n12 To\n115} 8.\n14 2:\n15\n17\n\nPROBLEMS = OUTPUT\n\nDEBUG CONSOLE\n\nTERMINAL PORTS python +v (] fl + ~ x\n\n@ PS C:\\Users\\atcha\\UnstructuredProject> myenv\\scripts\\activate\n%(myenv) PS C:\\Users\\atcha\\UnstructuredProject> python unstructured_grog_pipeline.py\n\nProcessing files: 100%|\n| 16/16 [ee:ee<\n\n?, ?it/s]\n\n[INFO] Extracting from: Big Data Analytics by RaJ Kamal, Preethi Saxena, TMH_part9.pdf\n[INFO] Starting OCR partition on PDF: temp_workspace/split\\Big Data Analytics by RaJ Kamal, Preethi Saxena, TMH_part9.pdf\n\nIQ Search\n\n@ n1,Col1 Spaces:4 UTF-8 LF {} PlainText @ @Golive Q@Prettier (%\n=meoecaeaoaP? Grane ~ 6 BN FYO aosams"
      ]
    }
  ],
  "total_time_taken": "7.60 seconds"
}