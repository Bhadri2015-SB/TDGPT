{
  "metadata": {
    "file_name": "TDGPT_Documentation.docx",
    "file_type": "word",
    "page_count": 1
  },
  "pages": [
    {
      "page_number": 1,
      "text": "TDGPT - PROJECT DOCUMENTATION\nProject Introduction\nThis AI-powered Document Parser automatically extracts, summarizes, and organizes content from various document types using Large Language Models (LLMs) and traditional document processing libraries.\nMethods Overview\nMethod 1: Custom Parsing (Without Unstructured)\nMethod 2: Using Unstructured Partitioning\nMethod 1: Custom Parsing (Without Unstructured)\nProject Objectives\nExtract text, tables, and images from PDFs, Word docs, and PowerPoint files.\nPerform chunk-wise summarization using LLMs.\nGenerate structured JSON outputs.\nStore extracted images in local folders.\nProcess native and scanned documents with OCR support.\nTechnologies Used\nPython: Core programming language\npdfplumber: Text and table extraction from PDFs\nPyMuPDF (fitz): Image extraction from PDFs\npytesseract: OCR for scanned PDFs\npython-docx: Word document parsing\npython-pptx: PowerPoint content extraction\nPillow (PIL): Image manipulation\nTesseract OCR: Optical character recognition\nGroq API / OpenAI / Llama3: LLMs for summarization\nLLM Comparison\nGPT-3.5/4 - OpenAI: High accuracy and broad support.\nLlama3-70B - Groq: Fast inference and cost-effective.\nSupported File Formats and Tools\nPDF: pdfplumber, fitz, pytesseract\nDOCX: python-docx\nPPTX: python-pptx\nImages: Pillow\nChunking Logic\nType: Word-based chunking\nChunk Size: CHUNK_SIZE = 300 words\nWhy: Optimized for LLM context window & summarization granularity\nPerformance (with LLM):\nSample Output JSON Structure\n{\n  \"metadata\": {\n    \"file_name\": \"example.pdf\",\n    \"file_type\": \"pdf\",\n    \"file_size\": \"1.5 MB\",\n    \"page_count\": 10\n  },\n  \"pages\": [\n    {\n      \"page_number\": 1,\n      \"text\": \"This is the extracted text...\",\n      \"tables\": [\"Table content as text...\"],\n      \"image_paths\": [\"output/images/example_page1_img1.png\"],\n      \"summary\": \"This page discusses the overview of...\"\n    }\n  ],\n  \"overall_summary\": \"Document provides a comprehensive overview of...\",\n\"total_time_taken\": \"25.6 seconds\"\n}\n\nFeatures Recap\nHandles PDF, DOCX, PPTX\nExtracts text, tables, and images\nOCR-enabled for scanned files\nChunk-wise LLM summarization\nStructured JSON output\nOptimized for large-scale document processing\nSample Output\nPros:\nAccurate table extraction via pdfplumber\nReliable image saving using fitz\nFull control of chunking and layout\nWorks consistently across large documents\nEasier debugging and optimization\nCons:\nSlightly longer initial setup\nRequires manual format detection\nBenefits\n   Multi-Format Support: Handles PDFs, Word (DOCX), and PowerPoint (PPTX) files.\n   Smart Summarization: Uses LLMs (like Groq LLaMA 3) to summarize content in chunks.\n   Image & Table Extraction: Automatically detects and saves images and tables.\n   OCR Enabled: Extracts text from scanned PDFs using Tesseract OCR.\n   Structured Output: Delivers clean JSON output for integration and analysis.\n   Offline Ready: Can work locally (except for LLMs), ideal for secure environments.\nLimitations\nLLM Dependency: Summarization needs an API key and internet access.\nOCR Accuracy: Depends on scan quality; poor images may reduce accuracy.\nLayout Loss: Does not preserve original formatting or styles.\nProcessing Time: Large files may take time to process.\nComplex Tables: Might struggle with nested or irregular tables.\nLanguage Support: Primarily optimized for English documents.\nMethod 2: Using Unstructured Partitioning\nTechnologies Used\n• Python – Core programming language\n• os, json, pandas – File handling and structured output\n• python-dotenv – Load environment variables\n• tqdm – Show progress during processing\n• unstructured – Extract text, tables, images from documents\n• pytesseract – OCR via Tesseract\n• Tesseract OCR – Recognize text in scanned PDFs\n• Groq API (LLaMA 3 70B) – Summarize extracted content using LLM\nLLM Comparison Summary\nSupported File Formats & Partition Modules\nPDF\nModule Used: unstructured.partition.pdf\nExtra Option: ocr=True for scanned PDFs\nOCR Engine: pytesseract\nUse: Extracts narrative text, tables, and embedded images from PDFs (including scanned).\nDOCX\nModule Used: unstructured.partition.docx\nUse: Extracts paragraphs, titles, and lists from Word files.\nXLSX\nModule Used: unstructured.partition.xlsx\nUse: Extracts table content, headers, and sheet metadata.\nPPTX\nModule Used: unstructured.partition.pptx\nUse: Extracts titles, bullet points, speaker notes.\nHTML\nModule Used: unstructured.partition.html\nUse: Parses and extracts main text and structured sections.\nPerformance\nOutput JSON Structure\n{\n\"file\": \"sample.pdf\",\n\"summary\": \"This document covers Big Data Analytics, architectural frameworks, real-time applications...\",\n\"text_elements\": [\n{\n\"type\": \"NarrativeText\",\n\"text\": \"Big data refers to datasets that are too large or complex...\",\n\"metadata\": {...}\n}\n],\n\"tables\": [\n{\n\"type\": \"table\",\n\"text\": \"Framework | Description\\nHadoop | Distributed storage...\",\n\"metadata\": {...}\n}\n],\n\"images\": [\n{\n\"type\": \"image\",\n\"text\": \"\",\n\"metadata\": {\n\"page_number\": 4,\n\"coordinates\": [100, 200, 300, 400]   }   }\n]\n}\nFeatures Summary\nAutomatic file type detection\nAI-based summarization (LLM via Groq)\nStructured output for text, tables, and images\nExport to JSON and CSV\nOCR support for scanned documents\nSample Execution Output\nA progress bar showing :\nProcessing files: 100%|████████████████████████████| 3/3 [00:08<00:00, 2.56s/it]\n✓ Successfully processed: BigData.pdf\n✓ Successfully processed: Report.docx\n✓ Successfully processed: Slides.pptx\nOutput saved in /output/\nPros\n   Handles multiple file types and extracts text, tables, and images\n   Uses OCR for scanned PDFs\n   Provides clear summaries with Groq LLaMA 3 70B\n   Outputs structured JSON and CSV files\n   Includes progress tracking for batch processing\nCons\n   OCR depends on scan quality it’s not properly extract images and tables format\n   Large files can take longer to process\n   Limited support for complex tables and media\n   Requires proper Tesseract setup\n   Summarization quality varies with model and input\n   Only supports specific file formats out of the box",
      "tables": [
        "File Size | Pages | Time Taken\nSmall | 30–50 | ~32–50sec\nMedium |  | \nLarge | 600+ | ~180–240 sec",
        "Model | Speed (on Groq) | Token Limit | Accuracy | Cost Efficiency | Used in Project\nGPT-4 Turbo | Moderate | 128k | High | Costly | No\nClaude 3 Opus | High | 200k | High | Moderate | No\nLLaMA 3 70B | Very High | 8k | High | High | Yes",
        "File Size | Pages | Time Taken\nSmall | 30–50 | ~15–22sec\nMedium |  | \nLarge | 600+ | ~180–240 sec"
      ],
      "image_paths": [
        "J File Edit Selection View Go Run Terminal Help €0 P UnstructuredProject By oo = ia) x\n\noO EXPLORER ote @ env ® unstructured_groq_pipeline.py B summaries.csv {} Big Data Analytics by RaJ Kamal, Preethi Saxena,TMH_part9.pdfjson X  ® test.py oO-\n> OPEN EDITORS output > {} Big Data Analytics by RaJ Kamal, Preethi Saxena,IMH_part9.pdf,json > ...\n& > OUTLINE 1 {\nvuns.. REO @ 2 \"file\": \"Big Data Analytics by RaJ Kamal, Preethi Saxena, TMH_part9.pdf\",\n> _pyeache 3 \"summary\": \"Here is a brief summary of the document: \\n\\nThe document appears to be a study guide or textbook chapter on data analytics, big data\np — 4 “text_elements\": [\n> myenv 5\ngo output 6 \"Title\",\n{} Big Data Analytics b... 7 : \"sample(), union(), crossProduct().\",\nB summaries.csv 8 \"metadata\": {\n= 5 ;\nJ | > temp_workspace 9 genrianeties\n& env 10 “points”: i\nA ® dispatch_partition.py “ c\nx 12 172.0,\n= requirements.txt 2B 122.0\ng ' sample.png 14 L\n® summarize_groq.py a5\n® unstructured_groq_pi... .\n18 1,\n19 [\n20 618.0,\n21 154.0\n22 1,\n23 -\n24\n25\n26\n27\n28 “PixelSpace”,\n29 “layout_width\": 1443,\n30 “layout_height\": 1860\n31\n32 pplication/pdf\",\n33\n(2) 34\n35\n§3 36 “last_modified”: \"2025-05-28118:47:23\",\n> TIMELINE 37 “page number”: 1, \"\nBH @0A0 Ln1,Col1 Spaces:2 UTF-8 CRLF {3 JSON & @Golive wWPrettier\n\n2s\nMostly cloudy\n\nAP meoCaeePCmmede +466 c0m yt",
        "J File Edit Selection View Go Run Terminal Help €0 PP TDGPT By oo = ia) x\n\nEXPLORER ote @ utils.ipynb ® config.ipynb ® mainipynb ‘3 50 page sample PDFindd.pdf_page1_img1.png X — @ extractor.ipynb @ env = requirements.tt O --\n> OPEN EDITORS Data_Extraction > pdf_Extraction > output > images > “i 50 page sample PDF.indd.pdf_page1_img1.png\n> OUTLINE\n\nver RED A\nY Data Collection\n> Large\n> Medium the B E STor\n> small\n~ Data_Extraction\nY pdf_Extraction\n\n> _pycache_\n> Data_Extraction\n\nCe OY GD\n\n}\n“O\n\nY output\nWORST WRITING\n‘G50 page sample...\n' 130725essentia...\n‘Gi 130725essentia...\n‘Gi CYBER SECURIT...\n‘Gi CYBER SECURIT...\n‘Gi CYBER SECURIT...\n{} 50 page sample ...\n{} 130725essentiaex...\n{} CYBER SECURITY ...\n{} MIPS Assembly L...\n\ni) b\n\n@\n\n@ config.ipynb\n® extractor.ipynb\n@ mainipynb\n\n@ utils.ipynb\n@ env\n\n> myenv CONOR\nrequirements. &\n@ __F reativementsot 'y  FRUHLINGER\n\nie > TIMELINE\n\nBH @0A0 Whole Image 400x620 €8 261.50KB @Golive O\n\n2 Peeerreeenn = Q Search a | @ e Ga @ Dy e e ba a & “66 iN eno anisen",
        "J File Edit Selection View Go Run Terminal Help €7 PP TDGPT By oo = ia) x\n\noO EXPLORER, ore & config.ipynb ® main.ipynb {} CYBER SECURITY (R18A0521) outputjson X — @ extractor.ipynb @ env = requirements.txt oO:\n> OPEN EDITORS Data_Extraction > pdf_Extraction > output > {} CYBER SECURITY (R18A0521)_outputjson > ...\na > OUTLINE 1\nv TDGPT 2\nje) © BERGaatam 3 CYBER SECURITY (R18A0521).pdf\",\n5 4 pdf\",\nLarge 5 ue 1.64 MB\",\n30 > Medium 6 “page_count\": 38 -\n> small 7 }\n40 \\ Data_Extraction 8 “pages”: [\n] \\ pdf_Extraction 9 {\n> ao 10 “page_number”: 1, -\nA 11 \"text\": \"DIGITAL NOTES\\nON\\nCYBER SECURITY\\n(R18A0521)\\nB.TECH III YEAR 8 II SEM (R18) \\n(2@20-2021)\\nDEPARTMENT OF INFORMATION TECHNOLOGY\\nMA\n> Data_Extraction x 5 = =\n12 ‘tables\": \"No tables found.\",\noutput aa “images”: [\ng © images 14 “output\\\\images\\\\CYBER SECURITY (R18A@521).pdf_page1_img1. jpeg”\n‘G50 page sample... 15 Ib\n') 130725essentia... 16 “summary”: “Here is a 1-2 line summary of the content:\\n\\nThis is a digital note on cyber security for B.Tech III Year - II Semester students\n[ east 7 ‘time_taken”: \"3.36 seconds’\n18 2\n‘Gh CYBER SECURIT... to {\n‘Gh CYBER SECURIT... 0) Chega mba e 2,\n‘& CYBER SECURIT... 21 “text”: \"MALLA REDDY COLLEGE OF ENGINEERING AND TECHNOLOGY\\nIII Year B.Tech II Sem L T/P/D C\\n3 -/-/- 3\\n(RA18A@521) CYBER SECURITY\\n(Profess\n{} 50 page sample ... 22 \"tables\": \"No tables found.\",\n{} 130725essentiaex... 23 “images\": [\n{} CYBER SECURITY ... 24 “output\\\\images\\\\CYBER SECURITY (R18A@521).pdf_page2_img1. jpeg”\n{} MIPS Assembly L... > L MQ 9 i A i i\n. 26 summary”: “Here is a summary of the content in 2 lines:\\n\\nThe course on Cyber Security covers topics such as types of cyber-attacks, cyber-\n® config.ipynb cna De =\n27 ‘time_taken”: “6.33 seconds’\n@ extractor.ipynb 28 }\n@ main.ipynb 29 {\n@ utils.ipynb 30 “page_number”: 3,\n& ew 31 “text”: \"UNIT - V\\nPrivacy Issues: Basic Data Privacy Concepts: Fundamental Concepts, Data Privacy\\nAttacks, Datalinking and profiling, priva\n> 32 “tables\nmyenv a5\n. 33 ‘images\n@ = requirements.txt Pe . . . . . . . . . .\n34 ‘summary’ Here is a 2-line summary of the content:\\n\\nThis unit focuses on privacy issues and cybercrime, covering basic data privacy conce\n35 “time_taken\": \"9.12 seconds”\n& * ||.\n> TIMELINE 37 { Pe)\nBY @0A0 Ln1,Col1 Spaces:2 UTF-8 CRLF {3 JSON & @Golive wWPrettier O\n\nTrending videos\nFunny Golden R...\n\nMP mMoCaae PCI B -62 8 e098 yi",
        "ba)\n\noe Rep\n\na5)\n\nI)\n\n@\n\ni}\nss\n\nFile Edit Selection View\n\nEXPLORER ate\n\n> OPEN EDITORS\n\n> OUTLINE\n\n‘v UNSTRUCTUREDPROJECT\n> _pycache_\n> myenv\nY output\n\n{} Big Data Analytics b...\n\n& summaries.csv\n\n> temp_workspace\n\n& env\n\n® dispatch_partition.py\nrequirements.txt\n\n{2 sample.png\n summarize_grog.py\n® test py\n\n® unstructured_groq_pi...\n\n> TIMELINE\n@o0Ao\n\n85°F\nMostly cloudy\n\nGo Run = <5\n\n@ env\n\noutput > Ei summaries.csv\n1 file,summary\n\n® unstructured_groq_pipeline.py\n\nBo - @ x\n@ test. O --\n\nBy\n\nPP UnstructuredProject\n\nE summaries.csv X  {} Big Data Analytics by RaJ Kamal, Preethi Saxena, TMH_part9.pdfjson\n\n\"Big Data Analytics by RaJ Kamal, Preethi Saxena, TMH_part9.pdf\",\"Here is a brief summary of the document:\nThe document appears to be a study guide or textbook chapter on data analytics, big data, and machine learning. It co\n\nSpark stack components and their applications\n\n2. Data analysis, including data mining, pattern mining, clustering, and anomaly detection\n\n3. Data visualization tools such as Tableau, D3.js, and FusionCharts\n\n4. Machine learning algorithms, including regression analysis, K-nearest neighbors, decision trees, and artificial ne\nData preprocessing, including data cleaning, filtering, and transformation\n\nCreating data pipelines using Spark, including data ingestion, processing, and storage\n\nUsing Python libraries such as NumPy, SciPy, and Pandas for analytics\n\nCreating reports and visualizations using Eclipse BIRT and other tools\n\nMachine learning algorithms for big data analytics, including frequent itemset mining, cluster analysis, classific\n\n16 The document also includes multiple-choice questions, practice exercises, and review of previous chapters.\"\n\n2\n\n3\n\n4\n\n5\n\n6 ils\n\n7\n\n8\n\n9\n10 Eo\npicts 6.\n12 To\n115} 8.\n14 2:\n15\n17\n\nPROBLEMS = OUTPUT\n\nDEBUG CONSOLE\n\nTERMINAL PORTS python +v (] fl + ~ x\n\n@ PS C:\\Users\\atcha\\UnstructuredProject> myenv\\scripts\\activate\n%(myenv) PS C:\\Users\\atcha\\UnstructuredProject> python unstructured_grog_pipeline.py\n\nProcessing files: 100%|\n| 16/16 [ee:ee<\n\n?, ?it/s]\n\n[INFO] Extracting from: Big Data Analytics by RaJ Kamal, Preethi Saxena, TMH_part9.pdf\n[INFO] Starting OCR partition on PDF: temp_workspace/split\\Big Data Analytics by RaJ Kamal, Preethi Saxena, TMH_part9.pdf\n\nIQ Search\n\n@ n1,Col1 Spaces:4 UTF-8 LF {} PlainText @ @Golive Q@Prettier (%\n=meoecaeaoaP? Grane ~ 6 BN FYO aosams"
      ]
    }
  ],
  "total_time_taken": "33.45 seconds"
}