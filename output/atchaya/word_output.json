{
  "metadata": {
    "file_name": "TDGPT_github_extraction.docx",
    "file_type": "word",
    "page_count": 1
  },
  "pages": [
    {
      "page_number": 1,
      "text": "TDGPT \u2013 GitHub file extraction\nAim:\nTo build a platform that supports Retrieval-Augmented Generation (RAG)-based application usage by extracting, processing, and embedding documents (e.g., .txt, .md, .py, etc.) from uploaded file, web URL or GitHub repositories link.\nDocument content:\nExtracting files from GitHub repo from the given link and embedding them based on the file format\nInput: GitHub repo URL\nOutput: Saved in \u201coutput/{owner}/{repo}/\u201d with original filenames.\nMethods Overview\nMethod 1: Using GitHub API\nMethod 2: Cloning the Repository via Git\nMethod 3: Downloading and Extracting ZIP Archive\nMethod 4: GitHub GraphQL API\nMethod 5: Raw Content Download via HTTP\nMethod \u2013 1: Using GitHub API\nDescription\nUtilize GitHub's API to programmatically access repository contents, allowing for selective retrieval of files based on specific criteria such as file extensions.\nUse Case\nIdeal for applications requiring targeted access to certain file types without the overhead of downloading the entire repository.\nPackages:\nrequests \u2013 To make HTTP requests to GitHub API.\nbase64 \u2013 To decode base64-encoded file content.\nCore logic:\nasync def fetch_contents(client, owner, repo, path=\"\"):\nurl = f\"{GITHUB_API_BASE}/repos/{owner}/{repo}/contents/{path}\"\nresponse = await client.get(url)\nif response.status_code != 200:\nraise ValueError(f\"GitHub API error: {response.status_code} - {response.text}\")\ndata = response.json()\nfiles = []\nif isinstance(data, dict) and data.get(\"type\") == \"file\":\nfiles.append(data[\"path\"])\nelse:\nfor item in data:\nif item[\"type\"] == \"file\":\nfiles.append(item[\"path\"])\nelif item[\"type\"] == \"dir\":\nnested_files = await fetch_contents(client, owner, repo, item[\"path\"])\nfiles.extend(nested_files)\nreturn files\nasync def download_files(client, owner, repo, file_paths):\nsaved_files = []\nfor path in file_paths:\nif path.endswith(\".txt\"): #required files are copied using its extention\nurl = f\"{GITHUB_API_BASE}/repos/{owner}/{repo}/contents/{path}\"\nresponse = await client.get(url)\nif response.status_code == 200:\ndata = response.json()\nif data.get(\"encoding\") == \"base64\":\nimport base64\ncontent = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\nlocal_path = os.path.join(f\"output/{owner}/{repo}\", os.path.basename(path))\nwith open(local_path, \"w\", encoding=\"utf-8\") as f:\nf.write(content)\nsaved_files.append(local_path)\nreturn saved_files\nCode Explanation\nThe `fetch_contents` function recursively traverses the repository to list all file paths. The `download_files` function filters for `.txt` files, downloads them, decodes base64 content, and saves them locally.\nPros\nEfficient retrieval of specific files without downloading the entire repository.\nReduces bandwidth and storage usage.\nSuitable for both public and private repositories (with authentication).\nCons\nSubject to GitHub API rate limits (60 req/hr).\nRequires handling of API responses and potential errors.\nNot suitable for retrieving large binary files due to size limitations.\nPerformance\nHigh efficiency for selective file access.\nLimitations\nCannot retrieve files managed by Git LFS.\nDoes not provide repository history or metadata beyond file contents.\nMethod \u2013 2: Git Clone\nDescription\nClones the entire Git repository locally, enabling access to all files and full commit history. Use Python to filter for required file types.\nUse Case\nBest when full access to the repository (including history, all file types, and branches) is required.\nPackages:\nsubprocess \u2013 To execute git clone commands from Python.\nCore logic:\nimport subprocess\ndef clone_repo(git_url, clone_dir):\ntry:\nsubprocess.run([\"git\", \"clone\", git_url, clone_dir], check=True)\nprint(\"Repository cloned successfully.\")\nexcept subprocess.CalledProcessError as e:\nprint(\"Error during cloning:\", e)\nPros\nFull access to all branches and history\nNo API rate limits\nWorks offline after cloning\nCons\nClones the entire repository (high disk usage and time for large repos)\nRequires Git to be installed\nSlower compared to selective methods\nPerformance: Medium\nLimitations\nNot selective: entire repository is downloaded\nRequires post-processing to filter files\nMethod \u2013 3: Download ZIP\nDescription\nDownloads the default branch of a repository as a ZIP archive and extracts the needed files locally.\nUse Case\nUseful for a quick, read-only snapshot of a public repo.\nPackages:\nzipfile \u2013 To extract contents from the ZIP file.\nio \u2013 To handle in-memory binary streams.\nCore logic:\nimport requests, zipfile, io, os\ndef download_and_extract_txt(zip_url, output_dir=\"output\"):\nr = requests.get(zip_url)\nz = zipfile.ZipFile(io.BytesIO(r.content))\nz.extractall(output_dir)\ntxt_files = []\nfor file in z.namelist():\nif file.endswith(\".txt\"):\ntxt_files.append(os.path.join(output_dir, file))\nreturn txt_files\nPros\nFast and simple\nNo authentication or Git installation needed\nEasy extraction of .txt, .md, or similar files\nCons\nOnly works for the default branch\nNo access to commit history or private repos\nPerformance: Fast\nLimitations\nRead-only archive\nCannot access other branches or commit history\nMethod \u2013 4: GitHub GraphQL API\nDescription\nUses GitHub\u2019s GraphQL API to query specific files or folders in a single, flexible request.\nUse Case\nIdeal for advanced querying and hierarchical filtering\nPros\nHighly flexible and efficient\nFetches only what\u2019s needed\nReduces the number of requests\nCons\nRequires GraphQL knowledge\nSetup is more complex\nAuthentication is mandatory\nPerformance: Medium\nLimitations\nComplex structure for large repos\nMethod \u2013 5: Raw GitHub URLs\nDescription\nDirectly fetches a known file using the raw content URL from GitHub.\nUse Case\nPerfect for quick access when the file path is known in advance.\nCore logic:\nimport requests\ndef download_raw_txt_file(url, output_path):\nr = requests.get(url)\nif r.status_code == 200:\nwith open(output_path, \"w\", encoding=\"utf-8\") as f:\nf.write(r.text)\nPros\nExtremely fast and easy\nNo setup or API key required\nCons\nRequires exact file path\nCannot discover or list files\nPerformance: Fast\nLimitations\nNo browsing capability\nOnly works when file paths are already known\nComparison:\nSuggested Method:\nMethod 1 (GitHub API)\nIt is optimal for:\nControlled, filtered file access\nGood balance of speed and flexibility\nWorks for public & private repos",
      "tables": [
        "Method | Use Case | Auth Needed | Performance | Private Repos | Filters Available | Best For\nGitHub API | Selective file retrieval | Yes | High | Yes | Yes | Targeted file access\nGit Clone | Full repository access | Yes/No | Medium | Yes | No | Comprehensive repository access\nZIP Archive Download | Quick snapshot of repository contents | No | High | No | Post-download | Simple, public repositories\nGitHub GraphQL API | Complex queries and hierarchical access | Yes | Medium | Yes | Yes | Advanced querying needs\nRaw GitHub File URLs | Direct access to known file paths | No | High | No | No | Quick access to specific files"
      ],
      "image_paths": []
    }
  ],
  "total_time_taken": "0.50 seconds"
}